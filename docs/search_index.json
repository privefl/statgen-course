[["index.html", "Statistical Human Genetics course using R Originally a SMARTbiomed Summer course Chapter 1 Course information 1.1 Prerequisites 1.2 Schedule of SMARTbiomed Summer school 1.3 Main author 1.4 Contact 1.5 License", " Statistical Human Genetics course using R Originally a SMARTbiomed Summer course Florian Privé &amp; Isabelle McGrath 2025-07-04 Chapter 1 Course information REMIND ME TO RECORD 1.1 Prerequisites Have at least a basic knowledge of R, Install R (&gt;= 3.4) and RStudio (&gt;= 1.2), Install recent versions of R packages bigstatsr, bigsnpr, and susieR (e.g. from CRAN), Create an RStudio project for this course and download the data to be used in the tutorials: runonce::download_file( &quot;https://figshare.com/ndownloader/files/38019072&quot;, dir = &quot;tmp-data&quot;, fname = &quot;GWAS_data.zip&quot;) # 109 MB runonce::download_file( &quot;https://figshare.com/ndownloader/files/38019027&quot;, dir = &quot;tmp-data&quot;, fname = &quot;ref_freqs.csv.gz&quot;) # 46 MB runonce::download_file( &quot;https://figshare.com/ndownloader/files/38019024&quot;, dir = &quot;tmp-data&quot;, fname = &quot;projection.csv.gz&quot;) # 44 MB runonce::download_file( &quot;https://figshare.com/ndownloader/files/38077323&quot;, dir = &quot;tmp-data&quot;, fname = &quot;sumstats_CAD_tuto.csv.gz&quot;) # 16 MB runonce::download_file( &quot;https://figshare.com/ndownloader/files/38247288&quot;, dir = &quot;tmp-data&quot;, fname = &quot;gen_pos_tuto.rds&quot;) # 2.5 MB bigsnpr::download_plink(&quot;tmp-data&quot;) # 6.3 MB bigsnpr::download_plink2(&quot;tmp-data&quot;) # 6.6 MB #&gt; [1] &quot;tmp-data/GWAS_data.zip&quot; #&gt; [1] &quot;tmp-data/ref_freqs.csv.gz&quot; #&gt; [1] &quot;tmp-data/projection.csv.gz&quot; #&gt; [1] &quot;tmp-data/sumstats_CAD_tuto.csv.gz&quot; #&gt; [1] &quot;tmp-data/gen_pos_tuto.rds&quot; #&gt; [1] &quot;tmp-data/plink.exe&quot; #&gt; [1] &quot;tmp-data/plink2.exe&quot; 1.2 Schedule of SMARTbiomed Summer school Time Activity Monday 13:00–16:30 Starting part 1 (chapters 2 to 5) Tuesday 09:00–12:00 and 13:00–16:00 Continuing part 1 Wednesday 09:00–10:00 Finishing part 1 + general Q&amp;A Wednesday 10:30–12:00 and 13:00–15:30 Language exchange workshop (with everyone) Wednesday 16:00–17:00 Starting part 2 (chapters 6 to 10) Thursday 09:00–12:00 and 13:00–16:30 Continuing part 2 Friday 09:00–12:00 Finishing part 2 + general Q&amp;A 1.3 Main author Florian Privé is a senior researcher in statistical human genetics, fond of Data Science and an R(cpp) enthusiast. You can find him on Bluesky and GitHub as @privefl. 1.4 Contact If you want me to add or clarify some content in this course, please open an issue on the GitHub repository of this course. If you have bug reports or questions specifically on functions of the packages, please open an issue on the corresponding package’s repository. I will always redirect you to GitHub issues if you email me about the packages, so that others can benefit from our discussion. Search through existing issues; your question might have already been asked and answered. 1.5 License This material is licensed under the Creative Commons Attribution-ShareAlike 3.0 License. "],["inputs-and-formats.html", "Chapter 2 Inputs and formats 2.1 Imputed data 2.2 Formats of genetic data 2.3 Main motivation for developing R packages bigstatsr and bigsnpr 2.4 The bigSNP format from bigsnpr 2.5 Getting a bigSNP object 2.6 The FBM format from bigstatsr 2.7 Working with an FBM 2.8 Exercise 2.9 Matching genetic variants between datasets", " Chapter 2 Inputs and formats Genetic data is represented as sequences of pairs of alleles (one from both parents). In the following figure, the reference alleles are shown in black, and the alternative alleles—arising from mutations—are shown in blue. Here, three DNA positions show alternative alleles; these are called genetic variants. And we refer to them as common variants because the alternative allele is frequent enough in the population. A particularly common type of genetic variant is the single nucleotide polymorphism (SNP), which involves a substitution of a single base at a specific genomic position. We can encode this data into a numeric genotype matrix G by counting the number of alternative alleles per individual at each variant. 2.1 Imputed data Genotype imputation is a widely used statistical technique that infers untyped genetic variants (possibly, dozens of millions) by leveraging known haplotype structures from external reference panels (Marchini &amp; Howie, 2010). It increases variant density, improves power for association testing, and facilitates meta-analysis across studies genotyped on different arrays. Imputation is typically performed after genotype quality control and phasing, using reference panels such as the 1000 Genomes Project, the Haplotype Reference Consortium (HRC), or TOPMed, depending on the ancestral composition and genotyping density of the study cohort. Several online imputation services are available, notably the Michigan Imputation Server (Das et al., 2016) and the Sanger Imputation Service, which provide streamlined pipelines for phasing (e.g., with SHAPEIT or Eagle) and imputation (e.g., using Minimac or IMPUTE). These platforms accept standard VCF inputs, automatically match strand orientation, and return imputed genotypes with associated quality metrics such as the INFO score. It is crucial to select a reference panel that closely matches the ancestry of the study population, as mismatches can reduce imputation accuracy and introduce bias in downstream analyses. 2.2 Formats of genetic data There exist many different data formats for genetic data: bed/bim/fam files (also called PLINK1 files) that respectively store genotype calls only (0, 1, 2, or NA) in a very condensed way (binary using one byte only for 4 genotypes), information on the genetic variants (text), and information on the individuals (text). PLINK 1.9 and 2.0 (Chang et al., 2015) provide many functions to work with this format. The bed format presented here is not the same as the BED format (a text file format used to store genomic regions as coordinates and associated annotations). bgen files (usually one per chromosome) that can store imputed probabilities (P(0), P(1), P(2)) that are often transformed to dosage information (expected values: P(1) + 2 P(2)). Each variant is stored compressed, which is very efficient, especially for low-frequency variants. They are accompanied by bgen.bgi files that store information on the genetic variants and the position of their corresponding data in the bgen files, and by a sample file that stores information on the individual IDs. This is the format used for the original UK Biobank imputed data (Bycroft et al., 2018). pgen/pvar/psam files (also called PLINK2 files) that can store imputed data as well, and pgen files seem more compressed than bgen files. PLINK2 provides many functions to work with this format. Many other formats that you can usually convert from using PLINK. 2.3 Main motivation for developing R packages bigstatsr and bigsnpr At the time, there was a notable lack of user-friendly and efficient R packages for genetic analyses, which posed challenges for researchers. The existing workflows often required the use of disparate software tools with inconsistent input formats, reliance on text files for parameter settings, and limited compatibility with exploratory data analysis and familiar R packages. Additionally, the development of new methods was hindered by the absence of tools supporting a simple matrix-like data structure. To address these challenges, I initiated the development of the R package bigsnpr in 2016, reimplementing the statistical methods commonly used in genetic analyses within a cohesive and accessible R framework. At some point, I realized that many functions (e.g. to perform genome-wide association studies (GWAS), principal component analysis (PCA), summary statistics, etc.) were not specific to genotype data. Indeed, both association studies and PCA are applicable to other omics data, such as transcriptomic or epigenetic datasets. Therefore I decided to move all these functions that could be used on any data stored as a matrix into a new R package, bigstatsr. This is why there are two packages, where bigstatsr can basically be used by any field using data stored as large numeric matrices, while bigsnpr provides some tools more specific to genotype data, largely building on top of bigstatsr. The initial description of the two packages is available in Privé, Aschard, Ziyatdinov, &amp; Blum (2018). 2.4 The bigSNP format from bigsnpr My R package bigsnpr (Privé et al., 2018) uses a class called bigSNP for representing SNP data. A bigSNP object is merely a list with the three following elements: $genotypes: A FBM.code256. Rows are samples and columns are genetic variants. This stores genotype calls or dosages (rounded to 2 decimal places). More about this format below. $fam: A data.frame with some information on the samples. $map: A data.frame with some information on the genetic variants. Package bigsnpr also provides functions for directly working on bed files with a small percentage of missing values (Privé, Luu, Blum, McGrath, &amp; Vilhjálmsson, 2020). 2.5 Getting a bigSNP object To read a bigSNP object from bed/bim/fam files, you can use functions snp_readBed() and snp_readBed2() (the second can read a subset of individuals/variants and use parallelism). To read dosages from BGEN files, you can use function snp_readBGEN(). This function takes around 40 minutes to read 1M variants for 400K individuals using 15 cores. Note that this function currently works only for BGEN v1.2 with probabilities stored as 8 bits (cf. this issue), which is the case for e.g. the UK Biobank files. The previous functions create two new files: &lt;backingfile&gt;.bk (binary file that stores the content of the genetic matrix) and &lt;backingfile&gt;.rds (stores the R bigSNP object, including some information on how to link with the bk file). To read any format used in genetics, you can always convert blocks of the data to text files using PLINK, read these using bigreadr::fread2(), and fill part of the resulting FBM. For example, see the code I used to convert the iPSYCH imputed data from the RICOPILI pipeline to my bigSNP format. Example converting a bed file to bigSNP: library(bigsnpr) #&gt; Loading required package: bigstatsr bedfile &lt;- system.file(&quot;extdata&quot;, &quot;example.bed&quot;, package = &quot;bigsnpr&quot;) Before using some functions you don’t know, have a look at their documentation to see how to use them and some examples (?snp_readBed). Use snp_readBed() to transform this data into a bigSNP object. What do you get? Use snp_attach() to get the data into R and explore it a bit. Click to see solution (rds &lt;- snp_readBed2(bedfile, backingfile = tempfile())) # get path to new file #&gt; [1] &quot;C:\\\\Users\\\\au639593\\\\AppData\\\\Local\\\\Temp\\\\Rtmpc5dgam\\\\file626c1cf44536.rds&quot; bigsnp &lt;- snp_attach(rds) # can then read in the bigSNP object in any R session (G &lt;- bigsnp$genotypes) #&gt; A Filebacked Big Matrix of type &#39;code 256&#39; with 517 rows and 4542 columns. G[1:5, 1:8] # first 8 genotypes for first 5 individuals #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] #&gt; [1,] 0 0 2 0 1 1 0 2 #&gt; [2,] 1 0 1 0 0 1 0 2 #&gt; [3,] 0 1 1 0 2 1 0 2 #&gt; [4,] 0 0 2 0 2 1 0 2 #&gt; [5,] 1 0 0 0 2 2 1 0 str(bigsnp$fam) #&gt; &#39;data.frame&#39;: 517 obs. of 6 variables: #&gt; $ family.ID : chr &quot;POP1&quot; &quot;POP1&quot; &quot;POP1&quot; &quot;POP1&quot; ... #&gt; $ sample.ID : chr &quot;IND0&quot; &quot;IND1&quot; &quot;IND2&quot; &quot;IND3&quot; ... #&gt; $ paternal.ID: int 0 0 0 0 0 0 0 0 0 0 ... #&gt; $ maternal.ID: int 0 0 0 0 0 0 0 0 0 0 ... #&gt; $ sex : int 0 0 0 0 0 0 0 0 0 0 ... #&gt; $ affection : int 1 1 2 1 1 1 1 1 1 1 ... str(bigsnp$map) #&gt; &#39;data.frame&#39;: 4542 obs. of 6 variables: #&gt; $ chromosome : int 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ marker.ID : chr &quot;SNP0&quot; &quot;SNP1&quot; &quot;SNP2&quot; &quot;SNP3&quot; ... #&gt; $ genetic.dist: int 0 0 0 0 0 0 0 0 0 0 ... #&gt; $ physical.pos: int 112 1098 2089 3107 4091 5091 6107 7103 8090 9074 ... #&gt; $ allele1 : chr &quot;A&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; ... #&gt; $ allele2 : chr &quot;T&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... Example directly mapping the bed file: Map the bed file directly using bed(). Can you access the same data as with the bigSNP object? Click to see solution obj.bed &lt;- bed(bedfile) obj.bed[1:5, 1:8] # first 8 genotypes for first 5 individuals #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] #&gt; [1,] 0 0 2 0 1 1 0 2 #&gt; [2,] 1 0 1 0 0 1 0 2 #&gt; [3,] 0 1 1 0 2 1 0 2 #&gt; [4,] 0 0 2 0 2 1 0 2 #&gt; [5,] 1 0 0 0 2 2 1 0 str(obj.bed$fam) #&gt; &#39;data.frame&#39;: 517 obs. of 6 variables: #&gt; $ family.ID : chr &quot;POP1&quot; &quot;POP1&quot; &quot;POP1&quot; &quot;POP1&quot; ... #&gt; $ sample.ID : chr &quot;IND0&quot; &quot;IND1&quot; &quot;IND2&quot; &quot;IND3&quot; ... #&gt; $ paternal.ID: int 0 0 0 0 0 0 0 0 0 0 ... #&gt; $ maternal.ID: int 0 0 0 0 0 0 0 0 0 0 ... #&gt; $ sex : int 0 0 0 0 0 0 0 0 0 0 ... #&gt; $ affection : int 1 1 2 1 1 1 1 1 1 1 ... str(obj.bed$map) #&gt; &#39;data.frame&#39;: 4542 obs. of 6 variables: #&gt; $ chromosome : int 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ marker.ID : chr &quot;SNP0&quot; &quot;SNP1&quot; &quot;SNP2&quot; &quot;SNP3&quot; ... #&gt; $ genetic.dist: int 0 0 0 0 0 0 0 0 0 0 ... #&gt; $ physical.pos: int 112 1098 2089 3107 4091 5091 6107 7103 8090 9074 ... #&gt; $ allele1 : chr &quot;A&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; ... #&gt; $ allele2 : chr &quot;T&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... Example converting a bgen file to bigSNP: bgen &lt;- runonce::download_file( &quot;https://enkre.net/cgi-bin/code/bgen/raw/3ec770a829a753282b5cb45afc3f4eda036b246705b76f9037b6cc98c41a4194?at=example.8bits.bgen&quot;, fname = &quot;example.bgen&quot;) bgi &lt;- runonce::download_file( &quot;https://enkre.net/cgi-bin/code/bgen/raw/dc7276e0f0e2e096f58d2dac645aa5711de2cd64c3b29a07a80575e175344f78?at=example.8bits.bgen.bgi&quot;, fname = &quot;example.bgen.bgi&quot;) First, use snp_readBGI() to get information on the variants. Then read the first 10 variants using snp_readBGEN() (pay attention to the specific format of list_snp_id), and snp_attach(). What extra information about the variants do you get in the bigSNP object? Click to see solution (var_info &lt;- snp_readBGI(bgi)) #&gt; # A tibble: 199 × 8 #&gt; chromosome position rsid number_of_alleles allele1 allele2 #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 01 1001 RSID_101 2 A G #&gt; 2 01 2000 RSID_2 2 A G #&gt; 3 01 2001 RSID_102 2 A G #&gt; 4 01 3000 RSID_3 2 A G #&gt; 5 01 3001 RSID_103 2 A G #&gt; 6 01 4000 RSID_4 2 A G #&gt; 7 01 4001 RSID_104 2 A G #&gt; 8 01 5000 RSID_5 2 A G #&gt; 9 01 5001 RSID_105 2 A G #&gt; 10 01 6000 RSID_6 2 A G #&gt; # ℹ 189 more rows #&gt; # ℹ 2 more variables: file_start_position &lt;dbl&gt;, size_in_bytes &lt;int&gt; (snp_id &lt;- with(var_info[1:10, ], paste(chromosome, position, allele1, allele2, sep = &quot;_&quot;))) #&gt; [1] &quot;01_1001_A_G&quot; &quot;01_2000_A_G&quot; &quot;01_2001_A_G&quot; &quot;01_3000_A_G&quot; &quot;01_3001_A_G&quot; #&gt; [6] &quot;01_4000_A_G&quot; &quot;01_4001_A_G&quot; &quot;01_5000_A_G&quot; &quot;01_5001_A_G&quot; &quot;01_6000_A_G&quot; (rds &lt;- snp_readBGEN(bgen, backingfile = tempfile(), list_snp_id = list(snp_id))) #&gt; [1] &quot;C:\\\\Users\\\\au639593\\\\AppData\\\\Local\\\\Temp\\\\Rtmpc5dgam\\\\file626c390b316d.rds&quot; bigsnp &lt;- snp_attach(rds) (G &lt;- bigsnp$genotypes) #&gt; A Filebacked Big Matrix of type &#39;code 256&#39; with 500 rows and 10 columns. str(bigsnp$map) # `$freq` and `$info` are computed when reading the data #&gt; tibble [10 × 8] (S3: tbl_df/tbl/data.frame) #&gt; $ chromosome : chr [1:10] &quot;01&quot; &quot;01&quot; &quot;01&quot; &quot;01&quot; ... #&gt; $ marker.ID : chr [1:10] &quot;SNPID_101&quot; &quot;SNPID_2&quot; &quot;SNPID_102&quot; &quot;SNPID_3&quot; ... #&gt; $ rsid : chr [1:10] &quot;RSID_101&quot; &quot;RSID_2&quot; &quot;RSID_102&quot; &quot;RSID_3&quot; ... #&gt; $ physical.pos: int [1:10] 1001 2000 2001 3000 3001 4000 4001 5000 5001 6000 #&gt; $ allele1 : chr [1:10] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... #&gt; $ allele2 : chr [1:10] &quot;G&quot; &quot;G&quot; &quot;G&quot; &quot;G&quot; ... #&gt; $ freq : num [1:10] 0.583 0.802 0.198 0.483 0.517 ... #&gt; $ info : num [1:10] 0.975 0.668 0.666 0.952 0.952 ... There is no $fam information when reading BGEN files, which should be read from other data and matched/joined using IDs from the sample file. From the BGEN, you can also use these IDs to read a subset of individuals only, as done in this code. sample &lt;- runonce::download_file( &quot;https://enkre.net/cgi-bin/code/bgen/raw/a3c4d8e4c132048a502dc00a3e51362f98eda5a2889df695ba260dc48c327fd9?at=example.sample&quot;, fname = &quot;example.sample&quot;) readLines(sample, n = 6) #&gt; [1] &quot;ID_1&quot; &quot;0&quot; &quot;sample_001&quot; &quot;sample_002&quot; &quot;sample_003&quot; #&gt; [6] &quot;sample_004&quot; What should you be careful about when reading the sample file? 2.6 The FBM format from bigstatsr The format provided in R package bigstatsr is called a Filebacked Big Matrix (FBM). It is an on-disk matrix format that is accessed through memory-mapping. How memory-mapping works: when you access the 1st element (1st row, 1st col), it accesses a block (say the first column) from disk into memory (RAM) when you access the 2nd element (2nd row, 1st col), it is already in memory so it is accessed very fast when you access the second column, you access from disk again (once) you can access many columns like that, until you do not have enough memory anymore some space is freed automatically so that new columns can be accessed into memory everything is seamlessly managed by the operating system (OS) it is also very convenient for parallelism as data is shared between processes All the elements of an FBM have the same type; supported types are: \"double\" (the default, double precision – 8 bytes per element) \"float\" (single precision – 4 bytes) \"integer\" (signed, so between \\(\\text{-}2^{31}\\) and (\\(2^{31} \\text{ - } 1\\)) – 4 bytes) \"unsigned short\": can store integer values from \\(0\\) to \\(65535\\) (2 bytes) \"raw\" or \"unsigned char\": can store integer values from \\(0\\) to \\(255\\) (1 byte). It is the basis for class FBM.code256 that can access 256 arbitrary different numeric values (decoded using a CODE_*), which is used in bigsnpr. The code used in class FBM.code256 for imputed data is e.g.  bigsnpr::CODE_DOSAGE #&gt; [1] 0.00 1.00 2.00 NA 0.00 1.00 2.00 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 #&gt; [16] 0.08 0.09 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 #&gt; [31] 0.23 0.24 0.25 0.26 0.27 0.28 0.29 0.30 0.31 0.32 0.33 0.34 0.35 0.36 0.37 #&gt; [46] 0.38 0.39 0.40 0.41 0.42 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.50 0.51 0.52 #&gt; [61] 0.53 0.54 0.55 0.56 0.57 0.58 0.59 0.60 0.61 0.62 0.63 0.64 0.65 0.66 0.67 #&gt; [76] 0.68 0.69 0.70 0.71 0.72 0.73 0.74 0.75 0.76 0.77 0.78 0.79 0.80 0.81 0.82 #&gt; [91] 0.83 0.84 0.85 0.86 0.87 0.88 0.89 0.90 0.91 0.92 0.93 0.94 0.95 0.96 0.97 #&gt; [106] 0.98 0.99 1.00 1.01 1.02 1.03 1.04 1.05 1.06 1.07 1.08 1.09 1.10 1.11 1.12 #&gt; [121] 1.13 1.14 1.15 1.16 1.17 1.18 1.19 1.20 1.21 1.22 1.23 1.24 1.25 1.26 1.27 #&gt; [136] 1.28 1.29 1.30 1.31 1.32 1.33 1.34 1.35 1.36 1.37 1.38 1.39 1.40 1.41 1.42 #&gt; [151] 1.43 1.44 1.45 1.46 1.47 1.48 1.49 1.50 1.51 1.52 1.53 1.54 1.55 1.56 1.57 #&gt; [166] 1.58 1.59 1.60 1.61 1.62 1.63 1.64 1.65 1.66 1.67 1.68 1.69 1.70 1.71 1.72 #&gt; [181] 1.73 1.74 1.75 1.76 1.77 1.78 1.79 1.80 1.81 1.82 1.83 1.84 1.85 1.86 1.87 #&gt; [196] 1.88 1.89 1.90 1.91 1.92 1.93 1.94 1.95 1.96 1.97 1.98 1.99 2.00 NA NA #&gt; [211] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA #&gt; [226] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA #&gt; [241] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA #&gt; [256] NA where the first four elements are used to store genotype calls, the next three to store imputed allele counts, and the next 201 values to store dosages rounded to 2 decimal places. This allows for handling many data types (genotype calls and dosages) while storing each element using one byte only (x4 compared to bed files, but /8 compared to double-precision floating-point numbers). 2.7 Working with an FBM 2.7.1 Similar accessor as R matrices library(bigstatsr) X &lt;- FBM(2, 5, init = 1:10, backingfile = &quot;tmp-data/test&quot;)$save() X$backingfile # the file where numeric data is stored #&gt; [1] &quot;C:\\\\Users\\\\au639593\\\\OneDrive - Aarhus universitet\\\\Desktop\\\\statgen-course\\\\tmp-data\\\\test.bk&quot; X$rds # the file where information about the data is stored #&gt; [1] &quot;C:\\\\Users\\\\au639593\\\\OneDrive - Aarhus universitet\\\\Desktop\\\\statgen-course\\\\tmp-data\\\\test.rds&quot; X &lt;- big_attach(&quot;tmp-data/test.rds&quot;) # can get the FBM from any R session You can access the whole FBM as an R matrix in memory using X[]. However, if the matrix is too large to fit in memory, you should always access only a subset of columns. Note that the elements of an FBM are stored column-wise (as for a standard R matrix). Therefore, be careful not to access a subset of rows, since it would read non-contiguous elements from the whole matrix from disk. X[, 1] # ok (must read first column only) #&gt; [1] 1 2 X[1, ] # bad (must read all data from disk) #&gt; [1] 1 3 5 7 9 X[] # super bad (standard R matrix in memory) #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] 1 3 5 7 9 #&gt; [2,] 2 4 6 8 10 2.7.2 Split-(par)Apply-Combine Strategy colSums(X[]) # super bad #&gt; [1] 3 7 11 15 19 Instead, the split-apply-combine strategy works well for applying standard R functions to FBMs (possibly in parallel), as implemented in big_apply(). Learn more with this tutorial on big_apply(). Compute the sum of each column of X &lt;- big_attachExtdata() using big_apply(). 2.7.3 Similar accessor as Rcpp matrices In case you want to develop new R functions for the FBM format while coding in C++ (which is the case for many bigstatsr/bigsnpr functions). Note that it is easy to use only a subset of the data without having to change anything in the code. // [[Rcpp::plugins(cpp11)]] // [[Rcpp::depends(bigstatsr, rmio)]] #include &lt;bigstatsr/BMCodeAcc.h&gt; // [[Rcpp::export]] NumericVector bigcolsums(Environment BM, const IntegerVector&amp; rowInd, const IntegerVector&amp; colInd) { // the external pointer XPtr&lt;FBM&gt; xpBM = BM[&quot;address&quot;]; // accessor to a sub-view of the data + automatically decoded SubBMCode256Acc(xpBM, rowInd, colInd, BM[&quot;code256&quot;], 1) size_t n = macc.nrow(); // similar code as for an Rcpp::NumericMatrix size_t m = macc.ncol(); // similar code as for an Rcpp::NumericMatrix NumericVector res(m); for (size_t j = 0; j &lt; m; j++) for (size_t i = 0; i &lt; n; i++) res[j] += macc(i, j); // similar code as for an Rcpp::NumericMatrix return res; } 2.7.4 Some summary functions are already implemented (X2 &lt;- snp_attachExtdata()$genotypes) #&gt; A Filebacked Big Matrix of type &#39;code 256&#39; with 517 rows and 4542 columns. big_colstats(X2) # sum and var (for each column) #&gt; sum var #&gt; 1 354 0.4604831 #&gt; 2 213 0.3241195 #&gt; 3 245 0.3932122 #&gt; 4 191 0.3109247 #&gt; 5 472 0.5176030 #&gt; 6 368 0.4613528 #&gt; 7 132 0.2292594 #&gt; 8 497 0.4791207 #&gt; 9 498 0.5160886 #&gt; 10 481 0.5416535 #&gt; [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 4532 rows ] big_scale()(X2) # mean and sd (for each column) #&gt; center scale #&gt; 1 0.6847195 0.6785891 #&gt; 2 0.4119923 0.5693149 #&gt; 3 0.4738878 0.6270663 #&gt; 4 0.3694391 0.5576062 #&gt; 5 0.9129594 0.7194463 #&gt; 6 0.7117988 0.6792295 #&gt; 7 0.2553191 0.4788104 #&gt; 8 0.9613153 0.6921855 #&gt; 9 0.9632495 0.7183931 #&gt; 10 0.9303675 0.7359712 #&gt; [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 4532 rows ] snp_scaleBinom()(X2) # 2 * af (mean) and sqrt(2 * af * (1 - af)) (~sd) #&gt; center scale #&gt; 1 0.6847195 0.6710433 #&gt; 2 0.4119923 0.5719471 #&gt; 3 0.4738878 0.6013343 #&gt; 4 0.3694391 0.5488137 #&gt; 5 0.9129594 0.7044231 #&gt; 6 0.7117988 0.6771042 #&gt; 7 0.2553191 0.4719377 #&gt; 8 0.9613153 0.7065775 #&gt; 9 0.9632495 0.7066291 #&gt; 10 0.9303675 0.7053904 #&gt; [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 4532 rows ] Functions starting with big_ are part of bigstatsr and work for any type of FBM, while functions starting with snp_ or bed_ are part of bigsnpr and are more specific to genetic data. There are now many functions implemented in the packages. You can find a comprehensive list of available functions on the package website of bigstatsr and of bigsnpr. Take some time to have a look at the different functions available in both bigstatsr and bigsnpr. What is the difference between functions starting with snp_ vs bed_? To only use a subset of the data stored as an FBM, you should almost never make a copy of the data. Instead, to apply functions to a subset of the data, use their parameters ind.row (or ind.train) and ind.col. 2.8 Exercise zip &lt;- runonce::download_file( &quot;https://figshare.com/ndownloader/files/38019072&quot;, dir = &quot;tmp-data&quot;, fname = &quot;GWAS_data.zip&quot;) unzip(zip, exdir = &quot;tmp-data&quot;, overwrite = FALSE) bedfile &lt;- &quot;tmp-data/GWAS_data.bed&quot; Map this data with bed() and start exploring it a bit, e.g. what summaries you could compute, what functions you could use. 2.9 Matching genetic variants between datasets This is some operation we will have to perform several times in this course. When matching genetic variants across datasets—such as between GWAS summary statistics and a reference panel—it is essential to ensure that alleles are correctly aligned. Two common issues can lead to misalignment: Strand flips occur when alleles are reported on opposite DNA strands. For example, a SNP recorded as A/G in one dataset may appear as T/C in another. These are complements (A↔︎T, G↔︎C), and although they refer to the same variant, their labels differ due to strand orientation. If strand flips are not corrected, allele matching will fail. Allele reversals happen when the effect and non-effect (or reference and alternate) alleles are swapped. For instance, if one dataset reports A/G and another G/A, the alleles are technically the same, but the interpretation of the effect direction (e.g., positive or negative beta) will be reversed. This can introduce sign errors in association results or polygenic scores. These issues are especially problematic for palindromic SNPs such as A/T or C/G, where the alleles are self-complementary. In such cases, even checking for strand flips or reversals is insufficient without additional information, such as allele frequencies, since both datasets may report A/T with no way to determine whether they refer to the same strand or orientation. Look carefully at the following results from snp_match() to understand what is going on. sumstats &lt;- data.frame( chr = 1, pos = c(86303, 86331, 162463, 752566, 755890, 758144), a0 = c(&quot;T&quot;, &quot;G&quot;, &quot;C&quot;, &quot;A&quot;, &quot;T&quot;, &quot;G&quot;), a1 = c(&quot;G&quot;, &quot;A&quot;, &quot;T&quot;, &quot;G&quot;, &quot;A&quot;, &quot;A&quot;), beta = c(-1.868, 0.250, -0.671, 2.112, 0.239, 1.272), p = c(0.860, 0.346, 0.900, 0.456, 0.776, 0.383) ) info_snp &lt;- data.frame( id = c(&quot;rs2949417&quot;, &quot;rs115209712&quot;, &quot;rs143399298&quot;, &quot;rs3094315&quot;, &quot;rs3115858&quot;), chr = 1, pos = c(86303, 86331, 162463, 752566, 755890), a0 = c(&quot;T&quot;, &quot;A&quot;, &quot;G&quot;, &quot;A&quot;, &quot;T&quot;), a1 = c(&quot;G&quot;, &quot;G&quot;, &quot;A&quot;, &quot;G&quot;, &quot;A&quot;) ) snp_match(sumstats, info_snp, return_flip_and_rev = TRUE) #&gt; 6 variants to be matched. #&gt; 1 ambiguous SNPs have been removed. #&gt; 4 variants have been matched; 1 were flipped and 1 were reversed. #&gt; chr pos a0 a1 beta p _NUM_ID_.ss _FLIP_ _REV_ id _NUM_ID_ #&gt; 1 1 86303 T G -1.868 0.860 1 FALSE FALSE rs2949417 1 #&gt; 2 1 86331 A G -0.250 0.346 2 FALSE TRUE rs115209712 2 #&gt; 3 1 162463 G A -0.671 0.900 3 TRUE FALSE rs143399298 3 #&gt; 4 1 752566 A G 2.112 0.456 4 FALSE FALSE rs3094315 4 snp_match(sumstats, info_snp, strand_flip = FALSE) #&gt; 6 variants to be matched. #&gt; 4 variants have been matched; 0 were flipped and 1 were reversed. #&gt; chr pos a0 a1 beta p _NUM_ID_.ss id _NUM_ID_ #&gt; 1 1 86303 T G -1.868 0.860 1 rs2949417 1 #&gt; 2 1 86331 A G -0.250 0.346 2 rs115209712 2 #&gt; 3 1 752566 A G 2.112 0.456 4 rs3094315 4 #&gt; 4 1 755890 T A 0.239 0.776 5 rs3115858 5 References Bycroft, C., Freeman, C., Petkova, D., Band, G., Elliott, L.T., Sharp, K., et al. (2018). The UK Biobank resource with deep phenotyping and genomic data. Nature, 562, 203–209. Chang, C.C., Chow, C.C., Tellier, L.C., Vattikuti, S., Purcell, S.M., &amp; Lee, J.J. (2015). Second-generation PLINK: Rising to the challenge of larger and richer datasets. Gigascience, 4, s13742–015. Das, S., Forer, L., Schönherr, S., Sidore, C., Locke, A.E., Kwong, A., et al. (2016). Next-generation genotype imputation service and methods. Nature Genetics, 48, 1284–1287. Marchini, J., &amp; Howie, B. (2010). Genotype imputation for genome-wide association studies. Nature Reviews Genetics, 11, 499–511. Privé, F., Aschard, H., Ziyatdinov, A., &amp; Blum, M.G.B. (2018). Efficient analysis of large-scale genome-wide data with two R packages: bigstatsr and bigsnpr. Bioinformatics, 34, 2781–2787. Privé, F., Luu, K., Blum, M.G., McGrath, J.J., &amp; Vilhjálmsson, B.J. (2020). Efficient toolkit implementing best practices for principal component analysis of population genetic data. Bioinformatics, 36, 4449–4457. "],["preprocessing.html", "Chapter 3 Preprocessing 3.1 Conversion and quality control of PLINK files 3.2 Imputation 3.3 Example", " Chapter 3 Preprocessing In this chapter, I talk about conversion, quality control and imputation. Conversion is also discussed in 2.5. 3.1 Conversion and quality control of PLINK files PLINK is very efficient and effective at converting and performing quality control of multiple formats, so I provide some wrappers to PLINK in bigsnpr, for ease of use directly from R: download_plink() and download_plink2() for downloading the latest stable versions of PLINK 1.9 and 2.0 (Chang et al., 2015). snp_plinkQC() for quality control (QC) and conversion to bed/bim/fam. snp_plinkKINGQC() for QC on relatedness based on KING-robust kinship estimator (Manichaikul et al., 2010). Using make.bed = FALSE allows for computing related pairs only, i.e. reporting a data frame without producing new bed/bim/fam files. Note that monozygotic twins or identical samples have a KING coefficient of \\(0.5\\), not \\(1\\); \\(0.25\\) = siblings and parents; \\(2^{-3}\\) = second-degree relatives (e.g. grandparents, uncles); \\(2^{-4}\\) = third-degree relatives (e.g. cousins). You can use a threshold of \\(2^{-4.5} \\approx 0.0442\\) to remove related individuals (PLINK removes one member of each pair of samples). snp_plinkIBDQC() for QC based on identity-by-descent (IBD) computed by PLINK using its method-of-moments. The KING one is supposed to be more robust. snp_plinkRmSamples() for producing new PLINK files after having removed some individuals. For any other PLINK function, I prefer calling PLINK directly from R thanks to system calls and R package glue, e.g. plink &lt;- download_plink(&quot;tmp-data&quot;) system(glue::glue( &quot;{plink} --version&quot; )) #&gt; PLINK v1.9.0-b.7.7 64-bit (22 Oct 2024) 3.2 Imputation Note that most functions from bigstatsr and bigsnpr do NOT handle missing values, except for some of the bed_ functions. Simple imputation (e.g. by the mean) of a ‘double’ FBM can be performed by blocks using e.g. the code from this vignette. In bigsnpr, to perform simple imputation of genotyped data, you can use snp_fastImputeSimple(). You can also use the (much) slower snp_fastImpute() that uses XGBoost models to impute genotyped data (Privé et al., 2018). If you have access to imputed data from large external reference panels, it is even better, and you can read this data as dosages in a bigSNP as shown in 2.5. 3.3 Example For the exercises, we will use the data provided in Reed et al. (2015). This can be downloaded using zip &lt;- runonce::download_file( &quot;https://figshare.com/ndownloader/files/38019072&quot;, dir = &quot;tmp-data&quot;, fname = &quot;GWAS_data.zip&quot;) unzip(zip, exdir = &quot;tmp-data&quot;, overwrite = FALSE) For some reason, this data is not ordered by chromosome and position; we can use PLINK to get an ordered version of this using library(bigsnpr) plink &lt;- download_plink(&quot;tmp-data&quot;) system(glue::glue( &quot;{plink} --bfile tmp-data/GWAS_data&quot;, &quot; --make-bed --out tmp-data/GWAS_data_sorted&quot; )) #&gt; PLINK v1.9.0-b.7.7 64-bit (22 Oct 2024) cog-genomics.org/plink/1.9/ #&gt; (C) 2005-2024 Shaun Purcell, Christopher Chang GNU General Public License v3 #&gt; Logging to tmp-data/GWAS_data_sorted.log. #&gt; Options in effect: #&gt; --bfile tmp-data/GWAS_data #&gt; --make-bed #&gt; --out tmp-data/GWAS_data_sorted #&gt; #&gt; 32574 MB RAM detected; reserving 16287 MB for main workspace. #&gt; 500000 variants loaded from .bim file. #&gt; 1401 people (937 males, 464 females) loaded from .fam. #&gt; 933 phenotype values loaded from .fam. #&gt; Using 1 thread (no multithreaded calculations invoked). #&gt; Before main variant filters, 1401 founders and 0 nonfounders present. #&gt; Calculating allele frequencies... 0%\b\b1%\b\b2%\b\b3%\b\b4%\b\b5%\b\b6%\b\b7%\b\b8%\b\b9%\b\b10%\b\b\b11%\b\b\b12%\b\b\b13%\b\b\b14%\b\b\b15%\b\b\b16%\b\b\b17%\b\b\b18%\b\b\b19%\b\b\b20%\b\b\b21%\b\b\b22%\b\b\b23%\b\b\b24%\b\b\b25%\b\b\b26%\b\b\b27%\b\b\b28%\b\b\b29%\b\b\b30%\b\b\b31%\b\b\b32%\b\b\b33%\b\b\b34%\b\b\b35%\b\b\b36%\b\b\b37%\b\b\b38%\b\b\b39%\b\b\b40%\b\b\b41%\b\b\b42%\b\b\b43%\b\b\b44%\b\b\b45%\b\b\b46%\b\b\b47%\b\b\b48%\b\b\b49%\b\b\b50%\b\b\b51%\b\b\b52%\b\b\b53%\b\b\b54%\b\b\b55%\b\b\b56%\b\b\b57%\b\b\b58%\b\b\b59%\b\b\b60%\b\b\b61%\b\b\b62%\b\b\b63%\b\b\b64%\b\b\b65%\b\b\b66%\b\b\b67%\b\b\b68%\b\b\b69%\b\b\b70%\b\b\b71%\b\b\b72%\b\b\b73%\b\b\b74%\b\b\b75%\b\b\b76%\b\b\b77%\b\b\b78%\b\b\b79%\b\b\b80%\b\b\b81%\b\b\b82%\b\b\b83%\b\b\b84%\b\b\b85%\b\b\b86%\b\b\b87%\b\b\b88%\b\b\b89%\b\b\b90%\b\b\b91%\b\b\b92%\b\b\b93%\b\b\b94%\b\b\b95%\b\b\b96%\b\b\b97%\b\b\b98%\b\b\b99%\b\b\b\b done. #&gt; Total genotyping rate is 0.977593. #&gt; 500000 variants and 1401 people pass filters and QC. #&gt; Among remaining phenotypes, 463 are cases and 470 are controls. (468 #&gt; phenotypes are missing.) #&gt; --make-bed to tmp-data/GWAS_data_sorted.bed + tmp-data/GWAS_data_sorted.bim + #&gt; tmp-data/GWAS_data_sorted.fam ... 0%\b\b1%\b\b2%\b\b3%\b\b4%\b\b5%\b\b6%\b\b7%\b\b8%\b\b9%\b\b10%\b\b\b11%\b\b\b12%\b\b\b13%\b\b\b14%\b\b\b15%\b\b\b16%\b\b\b17%\b\b\b18%\b\b\b19%\b\b\b20%\b\b\b21%\b\b\b22%\b\b\b23%\b\b\b24%\b\b\b25%\b\b\b26%\b\b\b27%\b\b\b28%\b\b\b29%\b\b\b30%\b\b\b31%\b\b\b32%\b\b\b33%\b\b\b34%\b\b\b35%\b\b\b36%\b\b\b37%\b\b\b38%\b\b\b39%\b\b\b40%\b\b\b41%\b\b\b42%\b\b\b43%\b\b\b44%\b\b\b45%\b\b\b46%\b\b\b47%\b\b\b48%\b\b\b49%\b\b\b50%\b\b\b51%\b\b\b52%\b\b\b53%\b\b\b54%\b\b\b55%\b\b\b56%\b\b\b57%\b\b\b58%\b\b\b59%\b\b\b60%\b\b\b61%\b\b\b62%\b\b\b63%\b\b\b64%\b\b\b65%\b\b\b66%\b\b\b67%\b\b\b68%\b\b\b69%\b\b\b70%\b\b\b71%\b\b\b72%\b\b\b73%\b\b\b74%\b\b\b75%\b\b\b76%\b\b\b77%\b\b\b78%\b\b\b79%\b\b\b80%\b\b\b81%\b\b\b82%\b\b\b83%\b\b\b84%\b\b\b85%\b\b\b86%\b\b\b87%\b\b\b88%\b\b\b89%\b\b\b90%\b\b\b91%\b\b\b92%\b\b\b93%\b\b\b94%\b\b\b95%\b\b\b96%\b\b\b97%\b\b\b98%\b\b\b99%\b\b\bdone. As you can see from PLINK output, this data contains 1401 individuals and 500,000 variants, with a small percentage of missing values (2.2%). Perform some QC with PLINK then read the QCed data as a bigSNP object in R. Make sure to look at the documentation of functions before using them. Click to see solution We can then perform some quality control using bedfile2 &lt;- snp_plinkQC(plink, &quot;tmp-data/GWAS_data_sorted&quot;) #&gt; PLINK v1.9.0-b.7.7 64-bit (22 Oct 2024) cog-genomics.org/plink/1.9/ #&gt; (C) 2005-2024 Shaun Purcell, Christopher Chang GNU General Public License v3 #&gt; Logging to tmp-data/GWAS_data_sorted_QC.log. #&gt; Options in effect: #&gt; --bfile tmp-data/GWAS_data_sorted #&gt; --geno 0.1 #&gt; --hwe 1e-50 #&gt; --maf 0.01 #&gt; --make-bed #&gt; --mind 0.1 #&gt; --out tmp-data/GWAS_data_sorted_QC #&gt; #&gt; 32574 MB RAM detected; reserving 16287 MB for main workspace. #&gt; 500000 variants loaded from .bim file. #&gt; 1401 people (937 males, 464 females) loaded from .fam. #&gt; 933 phenotype values loaded from .fam. #&gt; 0 people removed due to missing genotype data (--mind). #&gt; Using 1 thread (no multithreaded calculations invoked). #&gt; Before main variant filters, 1401 founders and 0 nonfounders present. #&gt; Calculating allele frequencies... 0%\b\b1%\b\b2%\b\b3%\b\b4%\b\b5%\b\b6%\b\b7%\b\b8%\b\b9%\b\b10%\b\b\b11%\b\b\b12%\b\b\b13%\b\b\b14%\b\b\b15%\b\b\b16%\b\b\b17%\b\b\b18%\b\b\b19%\b\b\b20%\b\b\b21%\b\b\b22%\b\b\b23%\b\b\b24%\b\b\b25%\b\b\b26%\b\b\b27%\b\b\b28%\b\b\b29%\b\b\b30%\b\b\b31%\b\b\b32%\b\b\b33%\b\b\b34%\b\b\b35%\b\b\b36%\b\b\b37%\b\b\b38%\b\b\b39%\b\b\b40%\b\b\b41%\b\b\b42%\b\b\b43%\b\b\b44%\b\b\b45%\b\b\b46%\b\b\b47%\b\b\b48%\b\b\b49%\b\b\b50%\b\b\b51%\b\b\b52%\b\b\b53%\b\b\b54%\b\b\b55%\b\b\b56%\b\b\b57%\b\b\b58%\b\b\b59%\b\b\b60%\b\b\b61%\b\b\b62%\b\b\b63%\b\b\b64%\b\b\b65%\b\b\b66%\b\b\b67%\b\b\b68%\b\b\b69%\b\b\b70%\b\b\b71%\b\b\b72%\b\b\b73%\b\b\b74%\b\b\b75%\b\b\b76%\b\b\b77%\b\b\b78%\b\b\b79%\b\b\b80%\b\b\b81%\b\b\b82%\b\b\b83%\b\b\b84%\b\b\b85%\b\b\b86%\b\b\b87%\b\b\b88%\b\b\b89%\b\b\b90%\b\b\b91%\b\b\b92%\b\b\b93%\b\b\b94%\b\b\b95%\b\b\b96%\b\b\b97%\b\b\b98%\b\b\b99%\b\b\b\b done. #&gt; Total genotyping rate is 0.977593. #&gt; 27390 variants removed due to missing genotype data (--geno). #&gt; --hwe: 91 variants removed due to Hardy-Weinberg exact test. #&gt; 67856 variants removed due to minor allele threshold(s) #&gt; (--maf/--max-maf/--mac/--max-mac). #&gt; 404663 variants and 1401 people pass filters and QC. #&gt; Among remaining phenotypes, 463 are cases and 470 are controls. (468 #&gt; phenotypes are missing.) #&gt; --make-bed to tmp-data/GWAS_data_sorted_QC.bed + #&gt; tmp-data/GWAS_data_sorted_QC.bim + tmp-data/GWAS_data_sorted_QC.fam ... 0%\b\b1%\b\b2%\b\b3%\b\b4%\b\b5%\b\b6%\b\b7%\b\b8%\b\b9%\b\b10%\b\b\b11%\b\b\b12%\b\b\b13%\b\b\b14%\b\b\b15%\b\b\b16%\b\b\b17%\b\b\b18%\b\b\b19%\b\b\b20%\b\b\b21%\b\b\b22%\b\b\b23%\b\b\b24%\b\b\b25%\b\b\b26%\b\b\b27%\b\b\b28%\b\b\b29%\b\b\b30%\b\b\b31%\b\b\b32%\b\b\b33%\b\b\b34%\b\b\b35%\b\b\b36%\b\b\b37%\b\b\b38%\b\b\b39%\b\b\b40%\b\b\b41%\b\b\b42%\b\b\b43%\b\b\b44%\b\b\b45%\b\b\b46%\b\b\b47%\b\b\b48%\b\b\b49%\b\b\b50%\b\b\b51%\b\b\b52%\b\b\b53%\b\b\b54%\b\b\b55%\b\b\b56%\b\b\b57%\b\b\b58%\b\b\b59%\b\b\b60%\b\b\b61%\b\b\b62%\b\b\b63%\b\b\b64%\b\b\b65%\b\b\b66%\b\b\b67%\b\b\b68%\b\b\b69%\b\b\b70%\b\b\b71%\b\b\b72%\b\b\b73%\b\b\b74%\b\b\b75%\b\b\b76%\b\b\b77%\b\b\b78%\b\b\b79%\b\b\b80%\b\b\b81%\b\b\b82%\b\b\b83%\b\b\b84%\b\b\b85%\b\b\b86%\b\b\b87%\b\b\b88%\b\b\b89%\b\b\b90%\b\b\b91%\b\b\b92%\b\b\b93%\b\b\b94%\b\b\b95%\b\b\b96%\b\b\b97%\b\b\b98%\b\b\b99%\b\b\bdone. 404,663 variants are remaining after this quality control. How to control the amount of memory and number of threads used by PLINK? Have a search at https://www.cog-genomics.org/plink/1.9/ using the quick index search at the bottom of the sidebar on the left. We can then read this data into an R object called bigSNP using (rds &lt;- snp_readBed2(bedfile2, ncores = nb_cores())) #&gt; [1] &quot;C:\\\\Users\\\\au639593\\\\OneDrive - Aarhus universitet\\\\Desktop\\\\statgen-course\\\\tmp-data\\\\GWAS_data_sorted_QC.rds&quot; obj.bigsnp &lt;- snp_attach(rds) str(obj.bigsnp, max.level = 2) #&gt; List of 3 #&gt; $ genotypes:Reference class &#39;FBM.code256&#39; [package &quot;bigstatsr&quot;] with 16 fields #&gt; ..and 26 methods, of which 12 are possibly relevant: #&gt; .. add_columns, as.FBM, bm, bm.desc, check_dimensions, #&gt; .. check_write_permissions, copy#envRefClass, initialize, initialize#FBM, #&gt; .. save, show#envRefClass, show#FBM #&gt; $ fam :&#39;data.frame&#39;: 1401 obs. of 6 variables: #&gt; ..$ family.ID : int [1:1401] 10002 10004 10005 10007 10008 10009 10010 10011 10012 10013 ... #&gt; ..$ sample.ID : int [1:1401] 1 1 1 1 1 1 1 1 1 1 ... #&gt; ..$ paternal.ID: int [1:1401] 0 0 0 0 0 0 0 0 0 0 ... #&gt; ..$ maternal.ID: int [1:1401] 0 0 0 0 0 0 0 0 0 0 ... #&gt; ..$ sex : int [1:1401] 1 2 1 1 1 1 1 2 1 2 ... #&gt; ..$ affection : int [1:1401] 1 1 2 1 2 2 2 1 2 -9 ... #&gt; $ map :&#39;data.frame&#39;: 404663 obs. of 6 variables: #&gt; ..$ chromosome : int [1:404663] 1 1 1 1 1 1 1 1 1 1 ... #&gt; ..$ marker.ID : chr [1:404663] &quot;rs12565286&quot; &quot;rs3094315&quot; &quot;rs2980319&quot; &quot;rs2980300&quot; ... #&gt; ..$ genetic.dist: int [1:404663] 0 0 0 0 0 0 0 0 0 0 ... #&gt; ..$ physical.pos: int [1:404663] 721290 752566 777122 785989 798959 947034 949608 1018704 1041700 1129672 ... #&gt; ..$ allele1 : chr [1:404663] &quot;G&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; ... #&gt; ..$ allele2 : chr [1:404663] &quot;C&quot; &quot;T&quot; &quot;T&quot; &quot;G&quot; ... #&gt; - attr(*, &quot;class&quot;)= chr &quot;bigSNP&quot; We can read and store some extra information on the individuals (e.g. some phenotypes): clinical &lt;- bigreadr::fread2(&quot;tmp-data/GWAS_clinical.csv&quot;) str(clinical) #&gt; &#39;data.frame&#39;: 1401 obs. of 7 variables: #&gt; $ FamID: int 10002 10004 10005 10007 10008 10009 10010 10011 10012 10013 ... #&gt; $ CAD : int 1 1 1 1 1 1 1 1 1 0 ... #&gt; $ sex : int 1 2 1 1 1 1 1 2 1 2 ... #&gt; $ age : int 60 50 55 52 58 59 54 66 58 67 ... #&gt; $ tg : int NA 55 105 314 161 171 147 124 60 160 ... #&gt; $ hdl : int NA 23 37 54 40 46 69 47 114 40 ... #&gt; $ ldl : int NA 75 69 108 94 92 109 84 67 112 ... # Get the same order as for the genotypes # (to match over multiple columns, use `vctrs::vec_match()`) ord &lt;- match(obj.bigsnp$fam$family.ID, clinical$FamID) pheno &lt;- clinical[ord, ] # Quick check stopifnot(all.equal(obj.bigsnp$fam$sex, pheno$sex)) # Update the $fam component obj.bigsnp$fam &lt;- cbind(obj.bigsnp$fam, pheno[-c(1, 3)]) Use dplyr::left_join() on obj.bigsnp$fam[1:6] and clinical to get the same information as we have now in obj.bigsnp$fam. Which columns should you use to join from? Click to see solution str(obj.bigsnp$fam) #&gt; &#39;data.frame&#39;: 1401 obs. of 11 variables: #&gt; $ family.ID : int 10002 10004 10005 10007 10008 10009 10010 10011 10012 10013 ... #&gt; $ sample.ID : int 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ paternal.ID: int 0 0 0 0 0 0 0 0 0 0 ... #&gt; $ maternal.ID: int 0 0 0 0 0 0 0 0 0 0 ... #&gt; $ sex : int 1 2 1 1 1 1 1 2 1 2 ... #&gt; $ affection : int 1 1 2 1 2 2 2 1 2 -9 ... #&gt; $ CAD : int 1 1 1 1 1 1 1 1 1 0 ... #&gt; $ age : int 60 50 55 52 58 59 54 66 58 67 ... #&gt; $ tg : int NA 55 105 314 161 171 147 124 60 160 ... #&gt; $ hdl : int NA 23 37 54 40 46 69 47 114 40 ... #&gt; $ ldl : int NA 75 69 108 94 92 109 84 67 112 ... fam2 &lt;- dplyr::left_join(obj.bigsnp$fam[1:6], clinical, by = c(&quot;family.ID&quot; = &quot;FamID&quot;, &quot;sex&quot;)) str(fam2) #&gt; &#39;data.frame&#39;: 1401 obs. of 11 variables: #&gt; $ family.ID : int 10002 10004 10005 10007 10008 10009 10010 10011 10012 10013 ... #&gt; $ sample.ID : int 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ paternal.ID: int 0 0 0 0 0 0 0 0 0 0 ... #&gt; $ maternal.ID: int 0 0 0 0 0 0 0 0 0 0 ... #&gt; $ sex : int 1 2 1 1 1 1 1 2 1 2 ... #&gt; $ affection : int 1 1 2 1 2 2 2 1 2 -9 ... #&gt; $ CAD : int 1 1 1 1 1 1 1 1 1 0 ... #&gt; $ age : int 60 50 55 52 58 59 54 66 58 67 ... #&gt; $ tg : int NA 55 105 314 161 171 147 124 60 160 ... #&gt; $ hdl : int NA 23 37 54 40 46 69 47 114 40 ... #&gt; $ ldl : int NA 75 69 108 94 92 109 84 67 112 ... Normally, dplyr::left_join() doesn’t change the order of individuals from the first data frame (as opposed to merge()), but you can end up with more rows if there are duplicates in the second data frame. Recall that this data contains some missing values. Look at the number of missing values per variant using big_counts() and impute the data using snp_fastImputeSimple(). Click to see solution G &lt;- obj.bigsnp$genotypes counts &lt;- big_counts(G) # counts per variant counts[, 1:8] #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] #&gt; 0 1247 958 1057 988 831 1201 496 386 #&gt; 1 131 362 316 370 502 115 676 730 #&gt; 2 6 66 28 25 67 7 165 216 #&gt; &lt;NA&gt; 17 15 0 18 1 78 64 69 hist(nbNA &lt;- counts[4, ]) We can e.g. perform a quick imputation by the mean using G2 &lt;- snp_fastImputeSimple(G, method = &quot;mean2&quot;, ncores = nb_cores()) big_counts(G2, ind.col = 1:8) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] #&gt; 0 1247 958 1057 988 831 1201 496 386 #&gt; 1 131 362 316 370 502 115 676 730 #&gt; 2 6 66 28 25 67 7 165 216 #&gt; &lt;NA&gt; 0 0 0 0 0 0 0 0 #&gt; 0.01 0 0 0 0 0 0 0 0 #&gt; 0.02 0 0 0 0 0 0 0 0 #&gt; [ reached getOption(&quot;max.print&quot;) -- omitted 196 rows ] big_counts(G, ind.col = 1:8) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] #&gt; 0 1247 958 1057 988 831 1201 496 386 #&gt; 1 131 362 316 370 502 115 676 730 #&gt; 2 6 66 28 25 67 7 165 216 #&gt; &lt;NA&gt; 17 15 0 18 1 78 64 69 G still has missing values, but G2 does not. Although both use the same underlying data (the same binary file .bk on disk), they use a different code to decode the underlying data: G$code256 #&gt; [1] 0 1 2 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA #&gt; [26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA #&gt; [ reached getOption(&quot;max.print&quot;) -- omitted 206 entries ] G2$code256 #&gt; [1] 0.00 1.00 2.00 NA 0.00 1.00 2.00 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 #&gt; [16] 0.08 0.09 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 #&gt; [31] 0.23 0.24 0.25 0.26 0.27 0.28 0.29 0.30 0.31 0.32 0.33 0.34 0.35 0.36 0.37 #&gt; [46] 0.38 0.39 0.40 0.41 0.42 #&gt; [ reached getOption(&quot;max.print&quot;) -- omitted 206 entries ] To always use G2 (with the new code256) and the extended obj.bigsnp$fam, you need to save the new obj.bigsnp using obj.bigsnp$genotypes &lt;- G2 snp_save(obj.bigsnp) # overwrite rds file with new object You can then re-attach this data in another R session later using snp_attach(\"tmp-data/GWAS_data_sorted_QC.rds\"). References Chang, C.C., Chow, C.C., Tellier, L.C., Vattikuti, S., Purcell, S.M., &amp; Lee, J.J. (2015). Second-generation PLINK: Rising to the challenge of larger and richer datasets. Gigascience, 4, s13742–015. Manichaikul, A., Mychaleckyj, J.C., Rich, S.S., Daly, K., Sale, M., &amp; Chen, W.-M. (2010). Robust relationship inference in genome-wide association studies. Bioinformatics, 26, 2867–2873. Privé, F., Aschard, H., Ziyatdinov, A., &amp; Blum, M.G.B. (2018). Efficient analysis of large-scale genome-wide data with two R packages: bigstatsr and bigsnpr. Bioinformatics, 34, 2781–2787. Reed, E., Nunez, S., Kulp, D., Qian, J., Reilly, M.P., &amp; Foulkes, A.S. (2015). A guide to genome-wide association analysis and post-analytic interrogation. Statistics in Medicine, 34, 3769–3792. "],["population-structure.html", "Chapter 4 Population structure 4.1 Principal Component Analysis (PCA) 4.2 The problem with LD 4.3 Best practices for PCA of genetic data 4.4 Exercise 4.5 Ancestry inference", " Chapter 4 Population structure 4.1 Principal Component Analysis (PCA) PCA is a matrix factorization method designed to identify orthogonal components that sequentially capture the maximum possible variance in the data. It is closely related to Singular Value Decomposition (SVD), in which a data matrix \\(G\\) is approximated as \\(G \\approx U D V^T\\), where \\(U\\) and \\(V\\) have orthonormal columns and \\(D\\) is a diagonal matrix of singular values. In this formulation, the principal component (PC) scores are given by \\(U D = G V\\), representing the projection of \\(G\\) onto the PCA space, while the columns of \\(V\\) correspond to the PC loadings. PCA on the genotype matrix can be used to capture population structure. However, PCA can actually capture different kinds of structure (Privé, Luu, Blum, et al., 2020): population structure (what we want), e.g. to use PCs as covariates in GWAS to correct for population structure (Price et al., 2006), Linkage Disequilibrium (LD) structure, when there are too many correlated variants (e.g. within long-range LD regions) and not enough population structure (see this vignette), relatedness structure, when there are related individuals who usually cluster together in later PCs, noise, basically just circles when looking at PC scores. Capturing population structure with PCA is the second main topic of my research work (after polygenic scores). In Privé et al. (2018), I introduced an algorithm to compute PCA for a bigSNP object while accounting for the LD problem by using clumping (not pruning, cf. this vignette) and an automatic detection and removal of long-range LD regions. In Privé, Luu, Vilhjálmsson, &amp; Blum (2020), I improved the implementation of the pcadapt algorithm, which detects variants associated with population structure (i.e. some kind of GWAS for population structure). In Privé, Luu, Blum, et al. (2020), I extended bigsnpr to also be able to run PCA on PLINK bed files directly with a small percentage of missing values, and investigated best practices for PCA in more detail. In Privé, Aschard, et al. (2022) and Privé (2022), I showed how to use PCA for ancestry inference, including grouping individuals in homogeneous ancestry groups, and inferring ancestry proportions from genotype data but also from allele frequencies only (see this vignette). 4.2 The problem with LD Let’s reuse the data prepared in 3.3. library(ggplot2) source(&quot;https://raw.githubusercontent.com/privefl/paper4-bedpca/master/code/plot_grid2.R&quot;) library(bigsnpr) #&gt; Loading required package: bigstatsr bigsnp &lt;- snp_attach(&quot;tmp-data/GWAS_data_sorted_QC.rds&quot;) G &lt;- bigsnp$genotypes NCORES &lt;- nb_cores() Use big_randomSVD() to perform a SVD/PCA on G. Do not forget to use some scaling (at least some centering). Look at PC scores and PC loadings with plot(). What do you see for PC4? Color PC scores using the most associated variant with PC4. Click to see solution svd &lt;- runonce::save_run( big_randomSVD(G, fun.scaling = big_scale(), ncores = NCORES), file = &quot;tmp-data/svd_with_ld.rds&quot;) #&gt; user system elapsed #&gt; 0.73 0.27 59.08 #&gt; Code finished running at 2025-06-10 13:33:51 CEST plot(svd) # scree plot -&gt; singular values plot(svd, type = &quot;scores&quot;) # individuals in the PCA space plot(svd, type = &quot;scores&quot;, scores = 3:4) # effects of variants along the genome # (x-axis: variant index; y-axis: effect of variant in PC) plot(svd, type = &quot;loadings&quot;, loadings = 1:10, coef = 0.4) What is going on with PC4? the_max &lt;- which.max(abs(svd$v[, 4])) plot(svd, type = &quot;scores&quot;, scores = 3:4) + aes(color = as.factor(G[, the_max])) + labs(color = &quot;Genotype at most influential SNP&quot;) + guides(color = guide_legend(override.aes = list(size = 3))) + theme(legend.position = &quot;bottom&quot;, legend.text = element_text(margin = margin(r = 6))) PC4 is basically capturing variation in a block of LD (the peak you see for loadings of PC4) and is then very correlated to genetic variants in this region (especially the_max). PC4 &lt;- predict(svd)[, 4] cor(PC4, G[, the_max]) #&gt; [1] -0.7942538 qplot(PC4, G[, the_max], alpha = I(0.1)) + theme_bw(14) + geom_smooth(method = &quot;lm&quot;) #&gt; Warning: `qplot()` was deprecated in ggplot2 3.4.0. #&gt; This warning is displayed once every 8 hours. #&gt; Call `lifecycle::last_lifecycle_warnings()` to see where this warning was #&gt; generated. #&gt; `geom_smooth()` using formula = &#39;y ~ x&#39; You really want to avoid using PCs that capture LD e.g. as covariates in GWAS because it can cause some collider bias (Grinde, Browning, Reiner, Thornton, &amp; Browning, 2024; Privé, Arbel, Aschard, &amp; Vilhjálmsson, 2022). To avoid capturing LD in PCA, it has been recommended to both perform some pruning (removing variants too correlated with one another) and remove some known list of long-range LD regions (that can still be captured by PCA, even after a stringent pruning step) (Abdellaoui et al., 2013; Price et al., 2008). But this is not enough; this is exactly what the UK Biobank did (Bycroft et al., 2018), and this how the PC loadings of the 40 PCs they provide look like: Therefore I recommend using only the first 16 PCs provided by the UK Biobank (Privé, Luu, Blum, et al., 2020). If you want to compute PCs yourself, I recommend using the autoSVD functions I developed that perform some pruning followed by an automatic detection and removal of long-range LD regions. In case of the UK Biobank, you would get 16 PCs when recomputing PCs yourself on the full dataset. 4.3 Best practices for PCA of genetic data There can be many steps to properly perform a PCA; you can find more about this in Privé, Luu, Blum, et al. (2020). Let’s have a look at the corresponding tutorial from the bigsnpr website. 4.4 Exercise Let’s reuse the data prepared in 3.3. Follow the previous tutorial to perform a PCA for this data, using either snp_* functions on the bigSNP object or bed_* functions on the bed file mapped. Click to see solution First, let’s get an idea of the relatedness in the data using library(bigsnpr) (NCORES &lt;- nb_cores()) plink2 &lt;- download_plink2(&quot;tmp-data&quot;) rel &lt;- snp_plinkKINGQC(plink2, &quot;tmp-data/GWAS_data_sorted_QC.bed&quot;, thr.king = 2^-4.5, make.bed = FALSE, ncores = NCORES) hist(log2(rel$KINSHIP), &quot;FD&quot;); abline(v = c(-4.5, -3.5), col = &quot;red&quot;) When computing relatedness with KING, LD pruning is NOT recommended. However, it may be useful to filter out some variants that are highly associated with population structure, e.g. as performed in the UK Biobank (Bycroft et al., 2018). For example, see this code. Relatedness should not be a major issue here. Let’s now compute PCs. All the code that follows could be run on the bigSNP object we made before. Nevertheless, to showcase the bed_* functions here, we will run the following analyses on the bed file directly. # Memory-map a bed file directly obj.bed &lt;- bed(&quot;tmp-data/GWAS_data_sorted_QC.bed&quot;) obj.svd &lt;- runonce::save_run( bed_autoSVD(obj.bed, k = 12, ncores = NCORES), file = &quot;tmp-data/PCA_GWAS_data.rds&quot;) #&gt; user system elapsed #&gt; 80.42 1.28 392.85 #&gt; Code finished running at 2025-06-10 13:41:05 CEST plot(obj.svd) plot(obj.svd, type = &quot;scores&quot;, scores = 1:12, coeff = 0.5) There is some population structure (maybe up to 6 PCs). You should also check loadings to make sure there is no LD structure (peaks on loadings): plot(obj.svd, type = &quot;loadings&quot;, loadings = 1:6, coeff = 0.5) No peaks, but loadings of PC4 are a bit odd. Why do I use hex bins to plot PC loadings? If you expect the individuals to mostly come from one population (e.g. in a national biobank), you can simply use a robust distance to identify a homogeneous subset of individuals, then look at the histogram of log-distances to choose a threshold based on visual inspection (here I would probably choose 4.5). PC &lt;- predict(obj.svd) ldist &lt;- log(bigutilsr::dist_ogk(PC[, 1:6])) hist(ldist, &quot;FD&quot;); abline(v = 4.5, col = &quot;red&quot;) plot_grid2(plotlist = lapply(1:4, function(k) { k1 &lt;- 2 * k - 1 k2 &lt;- 2 * k qplot(PC[, k1], PC[, k2], color = ldist, size = I(2)) + scale_color_viridis_c() + theme_bigstatsr(0.6) + labs(x = paste0(&quot;PC&quot;, k1), y = paste0(&quot;PC&quot;, k2), color = &quot;log-distance&quot;) + coord_equal() }), nrow = 2, legend_ratio = 0.2, title_ratio = 0) 4.5 Ancestry inference It would be useful to get an idea of the ancestry composition of these individuals. To achieve this, we can project this data onto the PCA space of many known population groups defined in Privé (2022) (based on the UK Biobank and some individuals from the 1000 Genomes). all_freq &lt;- bigreadr::fread2( runonce::download_file( &quot;https://figshare.com/ndownloader/files/38019027&quot;, # subset for the tutorial (46 MB) # &quot;https://figshare.com/ndownloader/files/31620968&quot;, # for real analyses (849 MB) dir = &quot;tmp-data&quot;, fname = &quot;ref_freqs.csv.gz&quot;)) projection &lt;- bigreadr::fread2( runonce::download_file( &quot;https://figshare.com/ndownloader/files/38019024&quot;, # subset for the tutorial (44 MB) # &quot;https://figshare.com/ndownloader/files/31620953&quot;, # for real analyses (847 MB) dir = &quot;tmp-data&quot;, fname = &quot;projection.csv.gz&quot;)) # coefficients to correct for overfitting of PCA correction &lt;- c(1, 1, 1, 1.008, 1.021, 1.034, 1.052, 1.074, 1.099, 1.123, 1.15, 1.195, 1.256, 1.321, 1.382, 1.443) str(all_freq) # reference allele frequencies #&gt; &#39;data.frame&#39;: 301156 obs. of 26 variables: #&gt; $ chr : int 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ pos : int 752566 785989 798959 947034 949608 1018704 1041700 1129672 1130727 1165310 ... #&gt; $ a0 : chr &quot;G&quot; &quot;T&quot; &quot;G&quot; &quot;G&quot; ... #&gt; $ a1 : chr &quot;A&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; ... #&gt; $ rsid : chr &quot;rs3094315&quot; &quot;rs2980300&quot; &quot;rs11240777&quot; &quot;rs2465126&quot; ... #&gt; $ Africa (West) : num 0.307 0.179 0.765 0.493 0.437 ... #&gt; $ Africa (South) : num 0.417 0.233 0.737 0.414 0.352 ... #&gt; $ Africa (East) : num 0.547 0.493 0.608 0.597 0.261 ... #&gt; $ Africa (North) : num 0.662 0.645 0.427 0.762 0.319 ... #&gt; $ Middle East : num 0.794 0.821 0.317 0.886 0.295 ... #&gt; $ Ashkenazi : num 0.775 0.84 0.203 0.899 0.298 ... #&gt; $ Italy : num 0.806 0.864 0.243 0.903 0.357 ... #&gt; $ Europe (South East): num 0.842 0.872 0.201 0.935 0.408 ... #&gt; $ Europe (North East): num 0.783 0.825 0.203 0.967 0.38 ... #&gt; $ Finland : num 0.812 0.831 0.224 0.969 0.483 ... #&gt; $ Scandinavia : num 0.819 0.843 0.228 0.964 0.412 ... #&gt; $ United Kingdom : num 0.839 0.872 0.2 0.963 0.401 ... #&gt; $ Ireland : num 0.86 0.887 0.202 0.973 0.387 ... #&gt; $ Europe (South West): num 0.828 0.867 0.216 0.932 0.347 ... #&gt; $ South America : num 0.775 0.722 0.324 0.928 0.244 ... #&gt; $ Sri Lanka : num 0.749 0.753 0.384 0.967 0.293 ... #&gt; $ Pakistan : num 0.781 0.79 0.347 0.943 0.338 ... #&gt; $ Bangladesh : num 0.799 0.808 0.33 0.975 0.325 ... #&gt; $ Asia (East) : num 0.887 0.758 0.319 0.953 0.191 ... #&gt; $ Japan : num 0.855 0.67 0.397 0.974 0.167 ... #&gt; $ Philippines : num 0.874 0.845 0.224 0.989 0.212 ... str(projection) # PC loadings of the reference data #&gt; &#39;data.frame&#39;: 301156 obs. of 21 variables: #&gt; $ chr : int 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ pos : int 752566 785989 798959 947034 949608 1018704 1041700 1129672 1130727 1165310 ... #&gt; $ a0 : chr &quot;G&quot; &quot;T&quot; &quot;G&quot; &quot;G&quot; ... #&gt; $ a1 : chr &quot;A&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; ... #&gt; $ rsid: chr &quot;rs3094315&quot; &quot;rs2980300&quot; &quot;rs11240777&quot; &quot;rs2465126&quot; ... #&gt; $ PC1 : num 8.73e-05 5.91e-05 3.81e-05 8.08e-05 9.40e-05 ... #&gt; $ PC2 : num -1.25e-04 -1.22e-04 1.54e-04 -1.01e-04 2.02e-05 ... #&gt; $ PC3 : num -5.23e-05 -1.25e-05 1.02e-05 -3.68e-05 1.37e-04 ... #&gt; $ PC4 : num -3.58e-05 1.01e-06 3.73e-05 -1.24e-05 -1.17e-04 ... #&gt; $ PC5 : num 2.69e-06 -7.52e-06 4.28e-05 4.53e-05 1.52e-04 ... #&gt; $ PC6 : num 3.22e-05 2.57e-05 -1.12e-05 1.91e-05 1.76e-04 ... #&gt; $ PC7 : num -1.64e-05 1.86e-05 -1.21e-04 5.86e-05 1.39e-04 ... #&gt; $ PC8 : num 2.03e-05 7.16e-05 -1.05e-04 1.00e-05 3.29e-05 ... #&gt; $ PC9 : num -7.71e-05 -3.57e-05 2.06e-05 -1.10e-05 1.57e-04 ... #&gt; $ PC10: num 7.27e-06 1.28e-05 1.65e-05 1.68e-05 1.78e-05 ... #&gt; $ PC11: num 1.35e-04 4.22e-05 -1.22e-05 -5.77e-05 -1.60e-04 ... #&gt; $ PC12: num -6.57e-05 -4.03e-05 4.44e-05 -2.68e-06 7.79e-05 ... #&gt; $ PC13: num -6.45e-05 -1.94e-05 5.16e-05 3.27e-05 5.64e-05 ... #&gt; $ PC14: num -3.13e-05 9.18e-09 -4.06e-05 2.15e-05 -1.59e-04 ... #&gt; $ PC15: num 1.16e-05 -5.76e-06 1.68e-05 -1.26e-06 -7.81e-05 ... #&gt; $ PC16: num 6.42e-05 6.47e-05 -4.77e-05 2.51e-05 -9.73e-07 ... Match variants between obj.bed and all_freq using snp_match(). Use a1 = allele1, a0 = allele2. You also need to add a column beta with 1s, which will tell you whether the alleles have been reversed for matching. For the variants matched, further remove the variants with more than 5% of missing values. Click to see solution # match variants between the two datasets library(dplyr) matched &lt;- obj.bed$map %&gt;% transmute(chr = chromosome, pos = physical.pos, a1 = allele1, a0 = allele2) %&gt;% mutate(beta = 1) %&gt;% snp_match(all_freq[1:5]) %&gt;% print() #&gt; chr pos a0 a1 beta _NUM_ID_.ss rsid _NUM_ID_ #&gt; 1 1 752566 G A -1 2 rs3094315 1 #&gt; 2 1 785989 T C -1 4 rs2980300 2 #&gt; 3 1 798959 G A 1 5 rs11240777 3 #&gt; 4 1 947034 G A -1 6 rs2465126 4 #&gt; 5 1 949608 G A 1 7 rs1921 5 #&gt; 6 1 1018704 A G -1 8 rs9442372 6 #&gt; [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 301150 rows ] Usually allele1 are the effect alleles. But some datasets may have the convention that a0 = allele1, a1 = allele2, therefore all effects and allele frequencies are reversed (after matching). # further subsetting on missing values counts &lt;- bed_counts(obj.bed, ind.col = matched$`_NUM_ID_.ss`, ncores = NCORES) hist(counts[4, ]) ind &lt;- which((counts[4, ] / nrow(obj.bed)) &lt; 0.05) matched2 &lt;- matched[ind, ] # projection matrix proj_mat &lt;- as.matrix(projection[matched2$`_NUM_ID_`, -(1:5)]) # reference allele frequencies ref_mat &lt;- as.matrix(all_freq [matched2$`_NUM_ID_`, -(1:5)]) # centers of reference populations in the PCA all_centers &lt;- crossprod(ref_mat, proj_mat) # project individuals (divided by 2 -&gt; as AF) onto the PC space all_proj &lt;- apply(sweep(proj_mat, 2, correction / 2, &#39;*&#39;), 2, function(x) { bed_prodVec(obj.bed, x, ind.col = matched2$`_NUM_ID_.ss`, ncores = NCORES, # scaling to get G if beta = 1 and (2 - G) if beta = -1 center = 1 - matched2$beta, scale = matched2$beta) }) The correction factors are needed when projecting new individuals onto a PCA space to correct for the shrinkage that happens due to “overfitting” of the PCA. This happens particularly when the number of individuals used to derive the PCA is smaller than the number of variables. The amount of correction needed increases for later PCs. You can have a look at Privé, Luu, Blum, et al. (2020) to read more about this. Note that the provided correction is specific to the PC loadings I provide, so that you don’t need to recompute it. It is based on the ratio between the projections using a method that corrects for this shrinkage and the simple projections (simply multiplying by the loadings on the right). Why can’t we use these provided PC loadings and correction factors to project UK Biobank individuals? In Privé, Aschard, et al. (2022), I have showed that distances in the PCA space are approximately proportional to \\(F_{ST}\\) (a genetic distance). So I recommend using distances in the PCA space to assign individuals to clusters. So that clusters remain somewhat homogeneous, a maximum distance can be set based on proportionality with a chosen \\(F_{ST}\\) threshold. For all individuals, compute the squared distances from all centers in the PCA space. Then, assign each individual to the closest group, if close enough (using thr_sq_dist). thr_fst &lt;- 0.002 # you can adjust this threshold thr_sq_dist &lt;- max(dist(all_centers)^2) * thr_fst / 0.16 # combine some close groups group &lt;- colnames(all_freq)[-(1:5)] group[group %in% c(&quot;Scandinavia&quot;, &quot;United Kingdom&quot;, &quot;Ireland&quot;)] &lt;- &quot;Europe (North West)&quot; group[group %in% c(&quot;Europe (South East)&quot;, &quot;Europe (North East)&quot;)] &lt;- &quot;Europe (East)&quot; Click to see solution We can then assign each individual to their closest center: # distance of individuals from each reference population (the center) all_sq_dist &lt;- apply(all_centers, 1, function(one_center) { rowSums(sweep(all_proj, 2, one_center, &#39;-&#39;)^2) }) # assign to closest cluster, when close enough cluster &lt;- apply(all_sq_dist, 1, function(sq_dist) { ind &lt;- which.min(sq_dist) if (sq_dist[ind] &lt; thr_sq_dist) group[ind] else NA }) table(cluster, exclude = NULL) # 3 NAs -&gt; almost all assigned #&gt; cluster #&gt; Ashkenazi Europe (East) Europe (North West) Europe (South West) #&gt; 110 148 872 45 #&gt; Italy Middle East &lt;NA&gt; #&gt; 219 4 3 plot_grid2(plotlist = lapply(1:4, function(k) { k1 &lt;- 2 * k - 1 k2 &lt;- 2 * k qplot(PC[, k1], PC[, k2], color = cluster, size = I(2)) + theme_bigstatsr(0.6) + labs(x = paste0(&quot;PC&quot;, k1), y = paste0(&quot;PC&quot;, k2), color = &quot;Assigned group&quot;) + coord_equal() }), nrow = 2, legend_ratio = 0.25, title_ratio = 0) These are mostly European individuals. PC4 is definitively a bit odd. Try to find out what’s going on with PC4. References Abdellaoui, A., Hottenga, J.-J., Knijff, P. de, Nivard, M.G., Xiao, X., Scheet, P., et al. (2013). Population structure, migration, and diversifying selection in the Netherlands. European Journal of Human Genetics, 21, 1277–1285. Bycroft, C., Freeman, C., Petkova, D., Band, G., Elliott, L.T., Sharp, K., et al. (2018). The UK Biobank resource with deep phenotyping and genomic data. Nature, 562, 203–209. Grinde, K.E., Browning, B.L., Reiner, A.P., Thornton, T.A., &amp; Browning, S.R. (2024). Adjusting for principal components can induce collider bias in genome-wide association studies. PLoS Genetics, 20, e1011242. Price, A.L., Patterson, N.J., Plenge, R.M., Weinblatt, M.E., Shadick, N.A., &amp; Reich, D. (2006). Principal components analysis corrects for stratification in genome-wide association studies. Nature Genetics, 38, 904–909. Price, A.L., Weale, M.E., Patterson, N., Myers, S.R., Need, A.C., Shianna, K.V., et al. (2008). Long-range LD can confound genome scans in admixed populations. The American Journal of Human Genetics, 83, 132–135. Privé, F. (2022). Using the UK Biobank as a global reference of worldwide populations: application to measuring ancestry diversity from GWAS summary statistics. Bioinformatics, 38, 3477–3480. Privé, F., Arbel, J., Aschard, H., &amp; Vilhjálmsson, B.J. (2022). Identifying and correcting for misspecifications in GWAS summary statistics and polygenic scores. Human Genetics and Genomics Advances, 3, 100136. Privé, F., Aschard, H., Carmi, S., Folkersen, L., Hoggart, C., O’Reilly, P.F., &amp; Vilhjálmsson, B.J. (2022). Portability of 245 polygenic scores when derived from the UK Biobank and applied to 9 ancestry groups from the same cohort. The American Journal of Human Genetics, 109, 12–23. Privé, F., Aschard, H., Ziyatdinov, A., &amp; Blum, M.G.B. (2018). Efficient analysis of large-scale genome-wide data with two R packages: bigstatsr and bigsnpr. Bioinformatics, 34, 2781–2787. Privé, F., Luu, K., Blum, M.G., McGrath, J.J., &amp; Vilhjálmsson, B.J. (2020). Efficient toolkit implementing best practices for principal component analysis of population genetic data. Bioinformatics, 36, 4449–4457. Privé, F., Luu, K., Vilhjálmsson, B.J., &amp; Blum, M.G. (2020). Performing highly efficient genome scans for local adaptation with R package pcadapt version 4. Molecular Biology and Evolution, 37, 2153–2154. "],["genome-wide-association-study-gwas.html", "Chapter 5 Genome-Wide Association Study (GWAS) 5.1 Example 5.2 REGENIE 5.3 Genomic control and LD score regression 5.4 Some post GWAS analyses", " Chapter 5 Genome-Wide Association Study (GWAS) In bigstatsr, you can perform both standard linear and logistic regressions GWAS, using either big_univLinReg() or big_univLogReg(). Function big_univLinReg() should be very fast, while big_univLogReg() is slower. This type of association, where each variable is considered independently, can be performed for any type of FBM (i.e. it does not have to be a genotype matrix). This is why these two functions are in package bigstatsr, and not bigsnpr. 5.1 Example Let’s reuse the data prepared in 3.3 and the PCs in 4.4. library(bigsnpr) #&gt; Loading required package: bigstatsr obj.bigsnp &lt;- snp_attach(&quot;tmp-data/GWAS_data_sorted_QC.rds&quot;) NCORES &lt;- nb_cores() obj.svd &lt;- readRDS(&quot;tmp-data/PCA_GWAS_data.rds&quot;) PC &lt;- predict(obj.svd) The clinical data includes age, sex, high-density lipoprotein (HDL)-cholesterol (hdl), low-density lipoprotein (LDL)-cholesterol (ldl), triglycerides (tg) and coronary artery disease status (CAD). For the set of covariates, we will use sex, age, and the first 6 PCs: covar &lt;- cbind(as.matrix(obj.bigsnp$fam[c(&quot;sex&quot;, &quot;age&quot;)]), PC[, 1:6]) PCs are used as covariates in GWAS to correct for population structure (Price et al., 2006). You probably should not account for other information such as cholesterol as they are heritable covariates, which can lead to collider bias (Aschard, Vilhjálmsson, Joshi, Price, &amp; Kraft, 2015; Day, Loh, Scott, Ong, &amp; Perry, 2016). G &lt;- obj.bigsnp$genotypes y &lt;- obj.bigsnp$fam$CAD ind.gwas &lt;- which(!is.na(y) &amp; complete.cases(covar)) To only use a subset of the data stored as an FBM (G here), you should almost never make a copy of the data; instead, use parameters ind.row (or ind.train) and ind.col to apply functions to a subset of the data. Let’s perform a case-control GWAS for CAD: gwas &lt;- runonce::save_run( big_univLogReg(G, y[ind.gwas], ind.train = ind.gwas, covar.train = covar[ind.gwas, ], ncores = NCORES), file = &quot;tmp-data/GWAS_CAD.rds&quot;) #&gt; user system elapsed #&gt; 0.17 0.14 157.05 #&gt; Code finished running at 2025-06-10 13:44:27 CEST This takes about two minutes with 4 cores on my laptop. Note that big_univLinReg() takes two seconds only, and should give very similar p-values, if you just need something quick. plot(gwas) CHR &lt;- obj.bigsnp$map$chromosome POS &lt;- obj.bigsnp$map$physical.pos snp_manhattan(gwas, CHR, POS, npoints = 50e3) + ggplot2::geom_hline(yintercept = -log10(5e-8), linetype = 2, color = &quot;red&quot;) Here, nothing is genome-wide significant because of the small sample size. Normally, a p-value threshold of \\(5 \\times 10^{-8}\\) is used when reporting significant findings, which corresponds to correcting for one million independent tests. Now, with larger GWAS datasets that include tens of millions of variants, researchers have started using an even more stringent threshold of \\(5 \\times 10^{-9}\\). Perform a GWAS for the other phenotypes, and look at the histogram, Q-Q plot and Manhattan plot. Click to see solution y2 &lt;- obj.bigsnp$fam$hdl ind.gwas2 &lt;- which(!is.na(y2) &amp; complete.cases(covar)) gwas2 &lt;- big_univLinReg(G, y2[ind.gwas2], ind.train = ind.gwas2, covar.train = covar[ind.gwas2, ], ncores = NCORES) snp_manhattan(gwas2, CHR, POS, npoints = 50e3) + ggplot2::geom_hline(yintercept = -log10(5e-8), linetype = 2, color = &quot;red&quot;) To report independent findings (instead of all variants in one peak), researchers have used LD clumping, which iteratively keeps the most associated variant and remove those that are too correlated with this one. Another strategy is to use GCTA-COJO (Yang et al., 2012). Perform some LD clumping for all variants with a p-value lower than \\(10^{-5}\\), using a stringent \\(r^2\\) threshold of 0.05 over a 5 Mb (= 5000 kb) window. How many variants are left? Click to see solution lpval &lt;- -predict(gwas) # -log10(pval) ind_keep &lt;- snp_clumping(G, CHR, S = lpval, thr.r2 = 0.05, size = 5000, infos.pos = POS, exclude = which(lpval &lt; -log10(1e-5)), ncores = NCORES) obj.bigsnp$map[ind_keep, ] #&gt; chromosome marker.ID genetic.dist physical.pos allele1 allele2 #&gt; 121561 4 rs4863155 0 190300221 G A #&gt; 136435 5 rs4957723 0 107157011 C T #&gt; 224210 9 rs9632884 0 22072301 G C snp_manhattan(gwas, CHR, POS, npoints = 50e3, ind.highlight = ind_keep) + ggplot2::geom_hline(yintercept = -log10(5e-8), linetype = 2, color = &quot;red&quot;) + ggplot2::geom_hline(yintercept = -log10(1e-5), linetype = 2, color = &quot;blue&quot;) For some example code to run a GWAS on multiple nodes on an HPC cluster: GWAS in iPSYCH; you can perform the GWAS on multiple nodes in parallel that would each process a chunk of the variants only GWAS for very large data and multiple phenotypes; you should perform the GWAS for all phenotypes for a “small” chunk of columns to avoid repeated access from disk, and can process these chunks on multiple nodes in parallel some template for {future.batchtools} when using Slurm 5.2 REGENIE regenie (Mbatchou et al., 2021) is a C++ program for whole genome regression modeling of large genome-wide association studies. It is developed and supported by a team of scientists at the Regeneron Genetics Center. It is fancier than simple linear/logistic regressions and is currently (one of if not) the state-of-the-art GWAS method. REGENIE has the following properties: It supports the BGEN, PLINK bed/bim/fam and PLINK2 pgen/pvar/psam genetic data formats It works on both quantitative and binary traits, including binary traits with unbalanced case-control ratios It can handle population structure and relatedness It can provide some power boost It is fast and memory efficient  It can process multiple phenotypes at once efficiently For binary traits, it supports Firth logistic regression and an SPA test (useful in case of very unbalanced phenotypes) It can perform gene/region-based tests (burden, SBAT, SKAT/SKATO, ACATV/ACATO) It can perform interaction tests (GxE, GxG) as well as conditional analyses Meta-analysis of REGENIE summary statistics can be performed using REMETA It can be installed with Conda 5.3 Genomic control and LD score regression \\(\\lambda_\\text{GC}\\), the genomic inflation factor (GIF), was used a lot to assess whether a GWAS is confounded by population structure or relatedness (when e.g. larger than 1.05). It corresponds to the ratio between the median of test statistics over the median of expected test statistics (under the null hypothesis). However, it is no longer used because, with sufficient GWAS power and trait polygenicity, a GIF larger than 1 is actually expected (Yang et al., 2011). snp_qq(gwas) + ggplot2::ylim(1, NA) # to plot less points #&gt; Warning: Removed 363713 rows containing missing values or values outside the scale range #&gt; (`geom_point()`). Then, the intercept of LD score regression was introduced to test for confounding (B. K. Bulik-Sullivan et al., 2015). However, note that it was observed that LD score regression intercepts tend to increase with SNP-heritability and GWAS sample size (Loh, Kichaev, Gazal, Schoech, &amp; Price, 2018). 5.4 Some post GWAS analyses It is challenging to interpret a GWAS on its own as there are often many associations, many of which are in non-coding regions. There is a large collection of downstream analyses to aid in gaining biological understanding from your GWAS, some of which are based on GWAS summary statistics which will cover later in this course. There are several online tools/resources that allow you to quickly get a sense of the meaning of a variant or GWAS results. If you want to understand what is known about a particular variant, SNPedia, dbSNP, and OpenTargets are good places to start. Here you can find information on the peer-reviewed papers that mention this particular variant, its allele frequencies across populations, its predicted consequences, and associations with other traits. GTEx is a database of the genetic regulation of gene expression in humans, and is also useful to get a sense of whether a variant of interest may influence the expression of a particular gene. Variants in non-coding regions are generally hypothesized to affect the trait through regulatory mechanisms. This database is not an exhaustive list of genetic regulation of gene expression – a variant may influence gene expression in certain cell or tissue types or contexts not captured in the data. FUMA is a website where you can upload GWAS summary statistics and it will run a range of analyses to link genetic variants to genes. You can then further investigate these genes with pathways analyses on the same platform. To my knowledge, there is no good tool that can run a range of analyses locally in one go – this would certainly save me a lot of time and be more efficient! Resources for helping linking GWAS hits to eQTLs: eQTL Catalogue GTEx CellxGene OneK1K Read more in this PowerPoint presentation. References Aschard, H., Vilhjálmsson, B.J., Joshi, A.D., Price, A.L., &amp; Kraft, P. (2015). Adjusting for heritable covariates can bias effect estimates in genome-wide association studies. The American Journal of Human Genetics, 96, 329–339. Bulik-Sullivan, B.K., Loh, P.-R., Finucane, H.K., Ripke, S., Yang, J., Psychiatric Genomics Consortium, S.W.G. of the, et al. (2015). LD score regression distinguishes confounding from polygenicity in genome-wide association studies. Nature Genetics, 47, 291–295. Day, F.R., Loh, P.-R., Scott, R.A., Ong, K.K., &amp; Perry, J.R. (2016). A robust example of collider bias in a genetic association study. The American Journal of Human Genetics, 98, 392–393. Loh, P.-R., Kichaev, G., Gazal, S., Schoech, A.P., &amp; Price, A.L. (2018). Mixed-model association for biobank-scale datasets. Nature Genetics, 50, 906–908. Mbatchou, J., Barnard, L., Backman, J., Marcketta, A., Kosmicki, J.A., Ziyatdinov, A., et al. (2021). Computationally efficient whole-genome regression for quantitative and binary traits. Nature Genetics, 53, 1097–1103. Price, A.L., Patterson, N.J., Plenge, R.M., Weinblatt, M.E., Shadick, N.A., &amp; Reich, D. (2006). Principal components analysis corrects for stratification in genome-wide association studies. Nature Genetics, 38, 904–909. Yang, J., Ferreira, T., Morris, A.P., Medland, S.E., ANthropometric Traits (GIANT) Consortium, G.I. of, Consortium, D.G.R.A.M. (DIAGRAM), et al. (2012). Conditional and joint multiple-SNP analysis of GWAS summary statistics identifies additional variants influencing complex traits. Nature Genetics, 44, 369–375. Yang, J., Weedon, M.N., Purcell, S., Lettre, G., Estrada, K., Willer, C.J., et al. (2011). Genomic inflation factors under polygenic inheritance. European Journal of Human Genetics, 19, 807–812. "],["gwas-summary-statistics.html", "Chapter 6 GWAS summary statistics 6.1 Format 6.2 Quality control 6.3 Ancestry inference", " Chapter 6 GWAS summary statistics When genome-wide association studies (GWAS) were first introduced to explore genetic factors associated with diseases, researchers may not have realized how valuable the resulting GWAS summary statistics would be. Over the past decade, GWAS summary statistics have become essential for many genetic analyses. The summary statistics generated by GWAS do not include personal information, making GWAS summary statistics easy to share and use for further analysis. Collaboration among researchers in large consortia has facilitated meta-analyses of GWAS summary statistics from numerous study sites, resulting in unprecedentedly large GWAS sample sizes (Levey et al., 2021; Okbay et al., 2022; Suzuki et al., 2023; Yengo et al., 2022; Zhou et al., 2022). 6.1 Format This an example of GWAS summary statistics (a subset of it at least): tgz &lt;- runonce::download_file( &quot;https://figshare.com/ndownloader/files/55262768&quot;, dir = &quot;tmp-data&quot;, fname = &quot;FT_sumstats_small.tsv.gz&quot;) writeLines(readLines(tgz, n = 6)) #&gt; variant_id chromosome base_pair_location effect_allele other_allele effect_allele_frequency imputation_quality beta standard_error p_value #&gt; rs72858371 1 6409383 T C 0.986224 0.977552 -0.00615364 0.0108886 0.64 #&gt; rs72633437 1 6409494 C T 0.906138 0.988959 0.00289667 0.00434703 0.39 #&gt; rs151043271 1 6409495 G A 0.998537 0.732147 -0.0244749 0.0386808 0.4 #&gt; rs58484986 1 6409653 C G 0.986228 0.976894 -0.00623617 0.0108958 0.63 #&gt; rs60772726 1 6409662 A T 0.986228 0.976844 -0.00625052 0.0108957 0.63 Fields included in GWAS summary statistics and their specific names in the header of the file can change very much from one file to the other; this is why tools like MungeSumstats have been developed (Murphy, Schilder, &amp; Skene, 2021). They often include Information to match variants (chromosome, physical position, alleles, rsid). \\(\\hat{\\beta}_j\\) or \\(\\hat{\\gamma}_j\\) — the GWAS effect size of variant \\(j\\) (marginal effect), \\(\\text{se}(\\hat{\\gamma}_j)\\) — its standard error, \\(z_j = \\frac{\\hat{\\gamma}_j}{\\text{se}(\\hat{\\gamma}_j)}\\) — the Z-score of variant \\(j\\), p-values derived from Z-scores but they more rarely include \\(n_j\\) — the GWAS sample size associated with variant \\(j\\), \\(f_j\\) — the allele frequency of variant \\(j\\), \\(\\text{INFO}_j\\) — the imputation INFO score (imputation quality) of variant \\(j\\) Read-in these GWAS summary statistics and derive the p-values again, using the other columns. Recall that the Z-score squared is \\(\\chi^2\\)-distributed with one degree of freedom. Click to see solution (sumstats &lt;- bigreadr::fread2(tgz)) #&gt; variant_id chromosome base_pair_location effect_allele other_allele #&gt; 1 rs72858371 1 6409383 T C #&gt; 2 rs72633437 1 6409494 C T #&gt; 3 rs151043271 1 6409495 G A #&gt; 4 rs58484986 1 6409653 C G #&gt; 5 rs60772726 1 6409662 A T #&gt; effect_allele_frequency imputation_quality beta standard_error p_value #&gt; 1 0.986224 0.977552 -0.00615364 0.01088860 0.64 #&gt; 2 0.906138 0.988959 0.00289667 0.00434703 0.39 #&gt; 3 0.998537 0.732147 -0.02447490 0.03868080 0.40 #&gt; 4 0.986228 0.976894 -0.00623617 0.01089580 0.63 #&gt; 5 0.986228 0.976844 -0.00625052 0.01089570 0.63 #&gt; [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 41805 rows ] chi2 &lt;- with(sumstats, (beta / standard_error)^2) pval &lt;- pchisq(chi2, df = 1, lower.tail = FALSE) plot(pval, sumstats$p_value, log = &quot;xy&quot;); abline(0, 1, col = &quot;red&quot;) 6.2 Quality control However, pooling many GWAS summary statistics in large meta-analyses has been a double-edged sword. On the one hand, this has given much-needed power to many genetic analyses. On the other hand, the quality and standardization of GWAS summary statistics varies widely, and the more studies that are pooled together, the more issues arise when using these pooled summary data. An overview of known issues in using GWAS summary statistics is presented in Figure 6.1. This includes weakening the construction of polygenic scores (Privé, Arbel, et al., 2022), biasing heritability estimates (Gazal et al., 2018; Grotzinger, Fuente, Privé, Nivard, &amp; Tucker-Drob, 2023), identifying spurious causal genes (Kanai et al., 2022; Zou, Carbonetto, Wang, &amp; Stephens, 2022), as well as undermining other genetic analyses (Chen et al., 2021; Julienne et al., 2021). Figure 6.1: Overview of possible errors and misspecifications in GWAS summary statistics, with possible harmful consequences, as well as possible remedies (Privé, Arbel, et al., 2022). \\(\\hat{\\gamma}\\): GWAS effect sizes; SE: standard errors; \\(n^{eff}\\): effective GWAS sample sizes; SD: standard deviations of genotypes; INFO: measure of imputation quality. The QC I recommend to perform consist in comparing standard deviations of genotypes estimated in 2 ways (Privé, Arbel, et al., 2022; Privé, Arbel, &amp; Vilhjálmsson, 2020): When linear regression was used \\[\\begin{equation} \\text{sd}(G_j) \\approx \\dfrac{\\text{sd}(y)}{\\sqrt{n_j \\cdot \\text{se}(\\hat{\\gamma}_j)^2 + \\hat{\\gamma}_j^2}} \\tag{6.1} \\end{equation}\\] When logistic regression was used (case-control phenotype) \\[\\begin{equation}\\label{eq:approx-sd-log} \\text{sd}(G_j) \\approx \\dfrac{2}{\\sqrt{n_j^\\text{eff} \\cdot \\text{se}(\\hat{\\gamma}_j)^2 + \\hat{\\gamma}_j^2}}~, \\tag{6.2} \\end{equation}\\] where \\(n_\\text{eff} = \\frac{4}{1 / n_\\text{ca} + 1 / n_\\text{co}}\\) \\[\\begin{equation} \\text{sd}(G_j) \\approx \\sqrt{2 \\cdot f_j \\cdot (1 - f_j) \\cdot \\text{INFO}_j} \\tag{6.3} \\end{equation}\\] Why would different variants have different GWAS sample sizes? With this QC, you can detect differences in per-variant GWAS sample sizes Figure 6.2: This is based on simulations. When \\(n_j\\) are missing, you can use this to perform some QC (e.g. at 70% of max n) or to impute \\(n_j\\). You can detect bias in total effective GWAS sample size Figure 6.3: This is based on CAD (coronary artery disease) GWAS summary statistics. Here I used \\(N_\\text{eff} = \\frac{4}{1 / N_\\text{ca} + 1 / N_\\text{co}}\\) in (6.2), where \\(N_\\text{ca}\\) is the total number of cases and \\(N_\\text{co}\\) is the total number of controls. What is the problem with this approach when you have a meta-analysis? (have a look at the following table with sample sizes for all studies comprising the meta-analysis) For example, overestimating the sample size leads to underestimating the SNP-heritability in many methods such as LD score regression (Grotzinger et al., 2023). You can detect and QC low imputation INFO scores, &amp; other issues Figure 6.4: This is based on breast cancer GWAS summary statistics. Any idea what’s going on for outliers in the second figure? Beware that multi-ancestry INFO scores are overestimated (e.g. in the UK Biobank), because the formula used to derive INFO scores assumes an homogeneous population. Figure 6.5: This is based on the UK Biobank. Use the following GWAS results for HDL and allele frequencies to estimate standard deviations and compare both estimates. How can you estimate sd(y)? library(bigsnpr) obj.bigsnp &lt;- snp_attach(&quot;tmp-data/GWAS_data_sorted_QC.rds&quot;) NCORES &lt;- nb_cores() obj.svd &lt;- readRDS(&quot;tmp-data/PCA_GWAS_data.rds&quot;) PC &lt;- predict(obj.svd) covar &lt;- cbind(as.matrix(obj.bigsnp$fam[c(&quot;sex&quot;, &quot;age&quot;)]), PC[, 1:6]) G &lt;- obj.bigsnp$genotypes y &lt;- obj.bigsnp$fam$hdl ind.gwas &lt;- which(!is.na(y) &amp; complete.cases(covar)) gwas &lt;- big_univLinReg(G, y[ind.gwas], ind.train = ind.gwas, covar.train = covar[ind.gwas, ], ncores = NCORES) N &lt;- length(ind.gwas) af &lt;- big_colstats(G, ind.row = ind.gwas, ncores = NCORES)$sum / (2 * N) Click to see solution sd_y &lt;- with(gwas, quantile(sqrt(0.5 * (N * std.err^2 + estim^2)), 0.01)) c(sd(y[ind.gwas]), sd_y) #&gt; 1% #&gt; 12.96917 11.84793 sd_ss &lt;- sd_y / with(gwas, sqrt(N * std.err^2 + estim^2)) sd_af &lt;- sqrt(2 * af * (1 - af)) library(ggplot2) ggplot(dplyr::slice_sample(data.frame(sd_af, sd_ss), n = 100e3)) + geom_point(aes(sd_af, sd_ss), alpha = 0.5) + theme_bigstatsr(0.9) + geom_abline(linetype = 2, color = &quot;red&quot;) + labs(x = &quot;Standard deviations derived from the allele frequencies&quot;, y = &quot;Standard deviations derived from the summary statistics&quot;) nb_na &lt;- big_counts(G$copy(CODE_012), ind.row = ind.gwas)[4, ] ggplot(dplyr::slice_sample(data.frame(sd_af, sd_ss, nb_na), n = 100e3)) + geom_point(aes(sd_af, sd_ss, color = nb_na), alpha = 0.5) + theme_bigstatsr(0.9) + scale_color_viridis_c(direction = -1, trans = &quot;sqrt&quot;) + geom_abline(linetype = 2, color = &quot;red&quot;) + labs(x = &quot;Standard deviations derived from the allele frequencies&quot;, y = &quot;Standard deviations derived from the summary statistics&quot;) We will practice more QC on GWAS summary statistics in 8.6. I provide some example R script in the LDpred2 tutorial that implements this QC. You can also find some other scripts with examples how to prepare several GWAS summary statistics here. I am currently working on implementing a method that performs a complementary QC, as well as the imputation of GWAS summary statistics (i.e. GWAS results for more variants). 6.3 Ancestry inference In 4.5, we’ve seen how to infer ancestry for individual-level data using reference data provided in Privé (2022); we can also use this to infer the ancestry composition of a GWAS dataset using only allele frequencies from the GWAS summary statistics. The tutorial accompanying Privé (2022) is here. Reuse some of the code from 4.5 (and/or from the tutorial) and allele frequencies (that you should compute) to get the ancestry composition of this dataset using function snp_ancestry_summary(). What do you need to do for allele frequencies after matching variants? Click to see solution library(bigsnpr) obj.bed &lt;- bed(&quot;tmp-data/GWAS_data_sorted_QC.bed&quot;) NCORES &lt;- nb_cores() all_freq &lt;- bigreadr::fread2( runonce::download_file( &quot;https://figshare.com/ndownloader/files/38019027&quot;, # subset for the tutorial (46 MB) # &quot;https://figshare.com/ndownloader/files/31620968&quot;, # for real analyses (849 MB) dir = &quot;tmp-data&quot;, fname = &quot;ref_freqs.csv.gz&quot;)) projection &lt;- bigreadr::fread2( runonce::download_file( &quot;https://figshare.com/ndownloader/files/38019024&quot;, # subset for the tutorial (44 MB) # &quot;https://figshare.com/ndownloader/files/31620953&quot;, # for real analyses (847 MB) dir = &quot;tmp-data&quot;, fname = &quot;projection.csv.gz&quot;)) # coefficients to correct for overfitting of PCA correction &lt;- c(1, 1, 1, 1.008, 1.021, 1.034, 1.052, 1.074, 1.099, 1.123, 1.15, 1.195, 1.256, 1.321, 1.382, 1.443) # match variants between the two datasets library(dplyr) matched &lt;- obj.bed$map %&gt;% transmute(chr = chromosome, pos = physical.pos, a1 = allele1, a0 = allele2) %&gt;% mutate(beta = 1) %&gt;% snp_match(all_freq[1:5]) %&gt;% print() #&gt; chr pos a0 a1 beta _NUM_ID_.ss rsid _NUM_ID_ #&gt; 1 1 752566 G A -1 2 rs3094315 1 #&gt; 2 1 785989 T C -1 4 rs2980300 2 #&gt; 3 1 798959 G A 1 5 rs11240777 3 #&gt; 4 1 947034 G A -1 6 rs2465126 4 #&gt; 5 1 949608 G A 1 7 rs1921 5 #&gt; 6 1 1018704 A G -1 8 rs9442372 6 #&gt; [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 301150 rows ] When matching between variants from two datasets, sometimes physical positions are in two different genome builds, usually it can be hg19 and hg38 (for newer datasets). You will need to either convert one of the two sets of positions with snp_modifyBuild() (that uses liftOver, not available for Windows) or by matching using rsIDs instead of positions (by using join_by_pos = FALSE). # further subsetting on missing values counts &lt;- bed_counts(obj.bed, ind.col = matched$`_NUM_ID_.ss`, ncores = NCORES) ind &lt;- which((counts[4, ] / nrow(obj.bed)) &lt; 0.05) matched2 &lt;- matched[ind, ] # compute allele frequencies af &lt;- bed_MAF(obj.bed, ind.col = matched2$`_NUM_ID_.ss`)$af af2 &lt;- ifelse(matched2$beta &gt; 0, af, 1 - af) # get ancestry composition res &lt;- snp_ancestry_summary( freq = af2, info_freq_ref = all_freq[matched2$`_NUM_ID_`, -(1:5)], projection = projection[matched2$`_NUM_ID_`, -(1:5)], correction = correction ) res #&gt; Africa (West) Africa (South) Africa (East) Africa (North) #&gt; 0.0015918 0.0000000 0.0000000 0.0000000 #&gt; Middle East Ashkenazi Italy Europe (South East) #&gt; 0.0000000 0.0824672 0.2020482 0.1529422 #&gt; Europe (North East) Finland Scandinavia United Kingdom #&gt; 0.0000000 0.0051918 0.1891938 0.1510849 #&gt; Ireland Europe (South West) South America Sri Lanka #&gt; 0.2154802 0.0000000 0.0000000 0.0000000 #&gt; Pakistan Bangladesh Asia (East) Japan #&gt; 0.0000000 0.0000000 0.0000000 0.0000000 #&gt; Philippines #&gt; 0.0000000 #&gt; attr(,&quot;cor_each&quot;) #&gt; Africa (West) Africa (South) Africa (East) Africa (North) #&gt; 0.6109985 0.6137032 0.8576549 0.9560451 #&gt; Middle East Ashkenazi Italy Europe (South East) #&gt; 0.9775514 0.9809802 0.9924371 0.9953612 #&gt; Europe (North East) Finland Scandinavia United Kingdom #&gt; 0.9902400 0.9782142 0.9946803 0.9975349 #&gt; Ireland Europe (South West) South America Sri Lanka #&gt; 0.9956773 0.9949990 0.9002633 0.8909370 #&gt; Pakistan Bangladesh Asia (East) Japan #&gt; 0.9428874 0.8967786 0.7286470 0.7290332 #&gt; Philippines #&gt; 0.7343768 #&gt; attr(,&quot;cor_pred&quot;) #&gt; [1] 0.9990652 # combine some close groups group &lt;- colnames(all_freq)[-(1:5)] group[group %in% c(&quot;Scandinavia&quot;, &quot;United Kingdom&quot;, &quot;Ireland&quot;)] &lt;- &quot;Europe (North West)&quot; group[group %in% c(&quot;Europe (South East)&quot;, &quot;Europe (North East)&quot;)] &lt;- &quot;Europe (East)&quot; grp_fct &lt;- factor(group, unique(group)) # use factors to keep order final_res &lt;- tapply(res, grp_fct, sum) round(100 * final_res, 1) #&gt; Africa (West) Africa (South) Africa (East) Africa (North) #&gt; 0.2 0.0 0.0 0.0 #&gt; Middle East Ashkenazi Italy Europe (East) #&gt; 0.0 8.2 20.2 15.3 #&gt; Finland Europe (North West) Europe (South West) South America #&gt; 0.5 55.6 0.0 0.0 #&gt; Sri Lanka Pakistan Bangladesh Asia (East) #&gt; 0.0 0.0 0.0 0.0 #&gt; Japan Philippines #&gt; 0.0 0.0 References Chen, W., Wu, Y., Zheng, Z., Qi, T., Visscher, P.M., Zhu, Z., &amp; Yang, J. (2021). Improved analyses of GWAS summary statistics by reducing data heterogeneity and errors. Nature Communications, 12, 1–10. Gazal, S., Loh, P.-R., Finucane, H.K., Ganna, A., Schoech, A., Sunyaev, S., &amp; Price, A.L. (2018). Functional architecture of low-frequency variants highlights strength of negative selection across coding and non-coding annotations. Nature Genetics, 50, 1600–1607. Grotzinger, A.D., Fuente, J. de la, Privé, F., Nivard, M.G., &amp; Tucker-Drob, E.M. (2023). Pervasive downward bias in estimates of liability-scale heritability in genome-wide association study meta-analysis: A simple solution. Biological Psychiatry, 93, 29–36. Julienne, H., Laville, V., McCaw, Z.R., He, Z., Guillemot, V., Lasry, C., et al. (2021). Multitrait GWAS to connect disease variants and biological mechanisms. PLoS Genetics, 17, e1009713. Kanai, M., Elzur, R., Zhou, W., Wu, K.-H.H., Rasheed, H., Tsuo, K., et al. (2022). Meta-analysis fine-mapping is often miscalibrated at single-variant resolution. Cell Genomics, 2, 100210. Levey, D.F., Stein, M.B., Wendt, F.R., Pathak, G.A., Zhou, H., Aslan, M., et al. (2021). Bi-ancestral depression GWAS in the Million Veteran Program and meta-analysis in&gt; 1.2 million individuals highlight new therapeutic directions. Nature Neuroscience, 24, 954–963. Murphy, A.E., Schilder, B.M., &amp; Skene, N.G. (2021). MungeSumstats: A Bioconductor package for the standardization and quality control of many GWAS summary statistics. Bioinformatics, 37, 4593–4596. Okbay, A., Wu, Y., Wang, N., Jayashankar, H., Bennett, M., Nehzati, S.M., et al. (2022). Polygenic prediction of educational attainment within and between families from genome-wide association analyses in 3 million individuals. Nature Genetics, 54, 437–449. Privé, F. (2022). Using the UK Biobank as a global reference of worldwide populations: application to measuring ancestry diversity from GWAS summary statistics. Bioinformatics, 38, 3477–3480. Privé, F., Arbel, J., Aschard, H., &amp; Vilhjálmsson, B.J. (2022). Identifying and correcting for misspecifications in GWAS summary statistics and polygenic scores. Human Genetics and Genomics Advances, 3, 100136. Privé, F., Arbel, J., &amp; Vilhjálmsson, B.J. (2020). LDpred2: better, faster, stronger. Bioinformatics, 36, 5424–5431. Suzuki, K., Hatzikotoulas, K., Southam, L., Taylor, H.J., Yin, X., Lorenz, K.M., et al. (2023). Multi-ancestry genome-wide study in &gt; 2.5 million individuals reveals heterogeneity in mechanistic pathways of type 2 diabetes and complications. medRxiv. Retrieved from https://doi.org/10.1101/2023.03.31.23287839 Yengo, L., Vedantam, S., Marouli, E., Sidorenko, J., Bartell, E., Sakaue, S., et al. (2022). A saturated map of common genetic variants associated with human height. Nature, 610, 704–712. Zhou, W., Kanai, M., Wu, K.-H.H., Rasheed, H., Tsuo, K., Hirbo, J.B., et al. (2022). Global Biobank Meta-analysis Initiative: Powering genetic discovery across human disease. Cell Genomics, 2, 100192. Zou, Y., Carbonetto, P., Wang, G., &amp; Stephens, M. (2022). Fine-mapping from summary data with the \"Sum of Single Effects\" model. PLoS Genetics, 18, e1010299. "],["linkage-disequilibrium-ld.html", "Chapter 7 Linkage Disequilibrium (LD) 7.1 Derivation of an LD matrix 7.2 Subtleties about LD matrices", " Chapter 7 Linkage Disequilibrium (LD) Most methods that rely on GWAS summary statistics also rely on an LD matrix (correlation matrix between genetic variants). It is used in algorithms to transform GWAS marginal (i.e. independent) effects into joint effects. Indeed, if we take the simple example of a multiple linear regression \\(y = X \\beta + \\epsilon\\), the joint effects are estimated by \\(\\hat\\beta = (X^T X)^{-1} X^T y\\), where \\(X^T y\\) are the marginal effects and \\(X^T X\\) is the covariance matrix (up to some scaling). Both \\(X^T y\\) and \\(X^T X\\) are summary statistics. The GWAS data from which the GWAS summary statistics were derived is often not accessible, so that the LD matrix is usually derived from another dataset. This reference dataset needs to be as close as possible as the GWAS data (in terms of genetic ancestry) to provide a close estimation of LD. This is why I got interested in estimating the ancestry composition of a GWAS based on allele frequencies reported in GWAS summary statistics (as we’ve seen in 6.3), at least to make sure that the reference data used to compute the LD is similar (in terms of genetic ancestry) to the original GWAS data. 7.1 Derivation of an LD matrix Functions snp_cor() and bed_cor() from bigsnpr can be used to derive a sparse LD matrix. Both functions allow for some missing values in the input genotypes (although we will see that it might not be such a good idea). If we use e.g. one million variants, the LD matrix is a 1M x 1M matrix, which is too large to compute and also to store. In practice, we know that LD between two genetic variants tend to decay with an increasing distance between them (Pritchard &amp; Przeworski, 2001). In LDpred2 (Privé, Arbel, et al., 2020), I assume that variants distant from more than 3 cM are uncorrelated (same for variants on different chromosomes). Genetic positions in centimorgans (cM) are preferred over using physical positions in base pairs (bp) in this case. From this, it results in a windowed (sparse) LD matrix (i.e. only values close to the diagonal are computed and stored). In 9.3.3, we will see how to alternatively use PLINK to derive an LD matrix. Reuse the same data as before to compute the LD matrix for variants on chromosome 22. Use snp_asGeneticPos() to convert positions to cM (do it for chromosome 22 only, to avoid downloading large files). And use size = 3 / 1000 (because positions are divided by 1000 internally in bigsnpr). Do you get any different result using snp_cor() and bed_cor(); why? Click to see solution library(bigsnpr) obj.bed &lt;- bed(&quot;tmp-data/GWAS_data_sorted_QC.bed&quot;) NCORES &lt;- nb_cores() library(dplyr) ind_chr22 &lt;- which(obj.bed$map$chromosome == 22) POS_chr22 &lt;- obj.bed$map$physical.pos[ind_chr22] # in bp # downloads 5.9 MB POS2_chr22 &lt;- snp_asGeneticPos(infos.chr = rep(22, length(POS_chr22)), infos.pos = POS_chr22, dir = &quot;tmp-data&quot;) cor1 &lt;- bed_cor(obj.bed, ind.col = ind_chr22, ncores = NCORES, infos.pos = POS2_chr22, size = 3 / 1000) # in cM bigsnp &lt;- snp_attach(&quot;tmp-data/GWAS_data_sorted_QC.rds&quot;) G &lt;- bigsnp$genotypes stopifnot(all(dim(G) == dim(obj.bed))) # quick check cor2 &lt;- snp_cor(G, ind.col = ind_chr22, ncores = NCORES, infos.pos = POS2_chr22, size = 3 / 1000) cbind(cor2[1:5, 1:5], cor1[1:5, 1:5]) # not exactly the same; why? #&gt; 5 x 10 sparse Matrix of class &quot;dgCMatrix&quot; #&gt; #&gt; [1,] 1.00000000 -0.05154381 -0.03629097 -0.02959808 -0.04231449 1.00000000 #&gt; [2,] -0.05154381 1.00000000 -0.15134204 -0.15277138 0.15337533 -0.05251683 #&gt; [3,] -0.03629097 -0.15134204 1.00000000 0.92384555 -0.50516808 -0.03645858 #&gt; [4,] -0.02959808 -0.15277138 0.92384555 1.00000000 -0.45964191 -0.02979486 #&gt; [5,] -0.04231449 0.15337533 -0.50516808 -0.45964191 1.00000000 -0.04304076 #&gt; #&gt; [1,] -0.05251683 -0.03645858 -0.02979486 -0.04304076 #&gt; [2,] 1.00000000 -0.15427297 -0.15639606 0.15833843 #&gt; [3,] -0.15427297 1.00000000 0.92937159 -0.51078705 #&gt; [4,] -0.15639606 0.92937159 1.00000000 -0.46606808 #&gt; [5,] 0.15833843 -0.51078705 -0.46606808 1.00000000 G2 &lt;- G$copy(CODE_012) # go back to having missing values cor3 &lt;- snp_cor(G2, ind.col = ind_chr22, ncores = NCORES, infos.pos = POS2_chr22, size = 3 / 1000) all.equal(cor3, cor1) #&gt; [1] TRUE 7.2 Subtleties about LD matrices LD matrices are often singular (non invertible), often with negative eigenvalues even, which causes stability and divergence issues in algorithms using them (Zabad, Haryan, Gravel, Misra, &amp; Li, 2025) if the LD is computed using less individuals than the number of variants, this leads to some eigenvalues being 0 (singular) computing LD from data with missing values often leads to the LD matrix having negative eigenvalues (because not the same individuals are used for all correlations, when using pairwise complete observations); imputing before computing LD can solve this issue (Zabad et al., 2025) windowing often results in having negative eigenvalues; defining independent LD blocks (Privé, 2021) in the LD matrix helps a lot (Privé, Arbel, et al., 2022) thresholding (discarding small absolute correlations) results in negative eigenvalues, so best to be avoided one can regularize the LD matrix (e.g. by adding some positive value to the diagonal, or by scaling down off-diagonal elements, as done in many methods); but too much regularization will bias results (Privé, Albiñana, Arbel, Pasaniuc, &amp; Vilhjálmsson, 2023) computing LD from imputed data seems to provide an unbiased estimation (Privé, Arbel, et al., 2022) should partial correlations be used instead? (e.g. when there is some population structure in the data); this is still an open question I want to investigate References Pritchard, J.K., &amp; Przeworski, M. (2001). Linkage disequilibrium in humans: Models and data. The American Journal of Human Genetics, 69, 1–14. Privé, F. (2021). Optimal linkage disequilibrium splitting. Bioinformatics, 38, 255–256. Privé, F., Albiñana, C., Arbel, J., Pasaniuc, B., &amp; Vilhjálmsson, B.J. (2023). Inferring disease architecture and predictive ability with LDpred2-auto. The American Journal of Human Genetics, 110, 2042–2055. Privé, F., Arbel, J., Aschard, H., &amp; Vilhjálmsson, B.J. (2022). Identifying and correcting for misspecifications in GWAS summary statistics and polygenic scores. Human Genetics and Genomics Advances, 3, 100136. Privé, F., Arbel, J., &amp; Vilhjálmsson, B.J. (2020). LDpred2: better, faster, stronger. Bioinformatics, 36, 5424–5431. Zabad, S., Haryan, C.A., Gravel, S., Misra, S., &amp; Li, Y. (2025). Towards whole-genome inference of polygenic scores with fast and memory-efficient algorithms. The American Journal of Human Genetics. Retrieved from https://doi.org/10.1016/j.ajhg.2025.05.002 "],["polygenic-risk-scores-pgs-or-prs.html", "Chapter 8 Polygenic (risk) scores (PGS, or PRS) 8.1 One use of PRS 8.2 PGS from individual-level data 8.3 PGS from GWAS summary statistics 8.4 Predictive ability of PGS 8.5 Future of PGS 8.6 Exercise with LDpred2 and lassosum2", " Chapter 8 Polygenic (risk) scores (PGS, or PRS) Improving PGS methods is the main topic of my research work. These are the main methods currently available in my packages: efficient penalized regressions, with individual-level data (Privé, Aschard, &amp; Blum (2019) + tutorial) Clumping and Thresholding (C+T) and Stacked C+T (SCT), with GWAS summary statistics and individual level data (Privé, Vilhjálmsson, Aschard, &amp; Blum (2019) + tutorial) LDpred2, with summary statistics (Privé, Arbel, et al. (2020) + tutorial) lassosum2, with the same input data as LDpred2 (Privé, Arbel, et al. (2022) + tutorial) You can now use LDpred2-auto for inference of e.g. the SNP-heritability and polygenicity, i.e. the proportion of phenotypic variance that can be explained by the set of genetic variants considered and the proportion of causal variants in this set (Privé et al. (2023) + tutorial). 8.1 One use of PRS For common complex diseases such as heart diseases, cancers, diabetes, many common genetic variants are causal, but with a small effect (they do have an impact on the probability of developing the disease, but often small) \\(\\Rightarrow\\) a common causal variant is not useful as a standalone risk factor in contrast to rare mutations causing rare monogenic diseases With polygenic risk scores (PRS), many genetic variants are aggregated in a joint predictive model by aggregating many small effects, the PRS can have a large effect \\(\\Rightarrow\\) the PRS is useful as a risk factor Then PRS can be used in public health to improve disease risk stratification beyond traditional risk factors (age, smoking, pollution, low SES, diet, physical inactivity, family history, low-frequency large-effect genetic mutations, etc). PRS are complementary to these risk factors and improve prediction accuracy—for example, by identifying more individuals at high risk for a given disease. The clinical validity of PRS has been shown in many studies, but their clinical utility has been demonstrated in a clinical trial conducted in the UK for cardiovascular disease (Fuat et al., 2024). 8.2 PGS from individual-level data If you have individual-level data (i.e. genotypes and phenotypes), you can basically use any supervised learning (machine learning) method to train a PGS. However, because of the size of the genetic data, you will quickly have scalability issues with these models. Moreover, it has been shown that effects for most diseases and traits are small and essentially additive, and that fancy methods such as deep learning are not much effective at constructing PGS (Kelemen et al., 2025). Therefore, using penalized linear/logistic regression (PLR) can be a very efficient and effective method to train PGS. In my R package bigstatsr, I have developed a very fast implementation with automatic choice of the two hyper-parameters (Privé, Aschard, et al., 2019). This is an example of using PLR for predicting height from genotypes in the UK Biobank training on 350K individuals x 656K variants in less than 24H within both males and females, PGS achieved a correlation of 65.5% (\\(r^2\\) of 42.9%) between genetically predicted and true height 8.3 PGS from GWAS summary statistics Why is it suboptimal to derive a PGS like this: \\(PGS_i = \\sum_j \\hat\\beta_j \\cdot G_{i,j}\\)? (\\(\\hat\\beta\\) are the GWAS effects sizes here) Figure 6.2: Local correlation between variants causes redundant GWAS signals. 8.3.1 Clumping + Thresholding (C+T, or P+T): restricting predictors C+T is the simplest and has long been the most widely used method for constructing PGS based on GWAS summary statistics. C+T is simple because it directly uses the GWAS effects as the predictive (PGS) effects. To reduce noise in the predictor, C+T only uses genetic variants that pass some chosen p-value threshold (the thresholding part). To avoid redundancy, C+T retains only one variant per group of correlated variants (the clumping step), as GWAS effects were learned independently rather than jointly. \\[PGS_i = \\sum_{\\substack{j \\in S_\\text{clumping} \\\\ p_j~&lt;~p_T}} \\hat\\beta_j \\cdot G_{i,j}\\] In GWAS, a p-value threshold of \\(5 \\times 10^{-8}\\) is used when reporting significant findings, which corresponds to correcting for one million independent tests. Yet for prediction purposes, including less significant variants can substantially improve predictive performance of C+T. Therefore, the p-value threshold must be chosen carefully in C+T. A stringent threshold risks excluding informative variants, while a lenient threshold may introduce noise by including too many non-informative variants. For the clumping step, one must choose the threshold above which to discard correlated variants. Hyper-parameters in C+T (and usual values): threshold on squared correlation in clumping (e.g. 0.2) window size for LD computation in clumping (e.g. 500 kb) p-value threshold (e.g. test \\(p_T\\) between \\(1\\) and \\(10^{-8}\\) and choose the best one in a tuning/validation set) other parameters such as the threshold of imputation quality score (e.g. \\(INFO &gt; 0.5\\)) or minor allele frequency (e.g. \\(MAF &gt; 0.01\\)) A popular software to derive C+T scores is PRSice-2 (Choi &amp; O’Reilly, 2019). My contributions in Privé, Vilhjálmsson, et al. (2019): an implementation to efficiently compute thousands of C+T scores corresponding to different sets of hyper-parameters (to explore more values for more hyper-parameters, rather than simply a few values for \\(p_T\\) only) going further by stacking with a (penalized) linear combination of all C+T models (instead of just choosing the best model) \\(\\Rightarrow\\) SCT (Stacked C+T) This can lead to achieving much better predictive performance compared to some standard use of C+T (tuning \\(p_T\\) only). Over the past years, stacking has become a very popular methodology to improve PGS predictive performance, sometimes by combining many models from multiple methods. 8.3.2 Other methods There are methods that better model LD than C+T (which uses simple heuristics): lassosum (Mak, Porsch, Choi, Zhou, &amp; Sham, 2017) and lassosum2 (Privé, Arbel, et al., 2022), which approximate a penalized linear regression based on GWAS summary statistics and some LD reference Bayesian methods such as LDpred2 (Privé, Arbel, et al., 2020), SBayesR/RC (Lloyd-Jones et al., 2019; Zheng et al., 2024) and PRS-CS (Ge, Chen, Ni, Feng, &amp; Smoller, 2019), also approximating a penalized regression, with some different priors and details in the implementation many others My opinion on these methods: C+T is known to be suboptimal but works okay for traits with low polygenicity (the proportion of causal variants) and can be improved with proper tuning/stacking (cf. above) lassosum is very good at getting very sparse solutions (lots of variants have a zero effect; they are not used in the PGS); C+T can be good for that as well (when a low p-value threshold is used; e.g. with \\(p_T &lt; 0.01\\), you can achieve a 95–99% sparsity) LDpred2 is the best in my very biased opinion ; there is a “grid” option where you need to tune two hyper-parameters and an “auto” option to directly infer hyper-parameters of the method from the data, and many developments have been made regarding improving its robustness SBayesR is very good with in-sample LD (i.e. from the GWAS data), otherwise it lacks robustness SBayesRC uses external functional annotations to help prioritize causal variants, which can help improve prediction, especially when applying the resulting PGS to other populations PRS-CS uses a lot of regularization internally, which makes it very robust, but sometimes at the expense of lower predictive ability; it is also usually slower than the other methods mentioned before Figure 6.4: Comparison of PGS methods from Privé, Arbel, et al. (2020). External comparisons have also been performed (Kulm, Marderstein, Mezey, &amp; Elemento, 2020; Pain et al., 2021). Since then, developments have been made to ensure even better predictive performance of LDpred2 models. 8.4 Predictive ability of PGS What influences predictive power of PGS? Predictive power \\(r^2\\) is bounded by the heritability \\(h^2\\) captured by the set of variants used (called SNP-heritability), except for some particular cases (X. Wang et al., 2023). \\(r^2\\) should increase with GWAS sample size \\(N\\), even though it probably increases slower than expected (Henches et al., 2025), possibly due to meta-analyzing heterogeneous GWAS summary statistics \\(r^2\\) decreases with polygenicity (the proportion of causal variants), because there are many smaller effects, harder to detect and estimate. Let’s denote \\(M_c\\) the number of causal variants. The theoretical upper bound for the phenotypic variance that can be explained by PGS (Daetwyler, Villanueva, &amp; Woolliams, 2008): \\[r^2_\\text{max} = \\dfrac{h^2}{1 + (1 - r^2_\\text{max}) \\dfrac{M_c}{N h^2}}\\] In R: uniroot(function(r2) r2 - h2 / (1 + (1 - r2) * M_c / (N * h2)), interval = c(0, h2))$root A major limitation of current PGS is their poor portability across ancestries. Here is an example across 245 phenotypes and 9 ancestry groups (Privé, Aschard, et al., 2022): Figure 6.5: Partial correlations and 95% CIs in the UK test set versus in a test set from another ancestry group. Each point represents a phenotype and training has been performed with penalized regression on UK individuals and HapMap3 variants. The slope (in blue) is computed using Deming regression accounting for standard errors in both x and y, fixing the intercept at 0. The square of this slope is provided above each plot, which we report as the relative predictive performance compared to testing in the ‘United Kingdom’ ancestry group (see next figure). Basically, predictive performance drops with genetic distance (captured by distance in the PCA space here) from the training population (Ding et al., 2023; Privé, Aschard, et al., 2022): One possible explanation: different tagging, because correlations between genetic variants are different across populations (Y. Wang et al., 2020): effect of a causal variant should be similar for all populations (Hou et al., 2023; Hu et al., 2025) all 3 variants have same effect in N.W. Europeans (perfect correlation) however, in E. Asians, variant 1 retains only 60% of the causal effect, and 40% in W. Africans 8.5 Future of PGS Use ever-increasing GWAS summary statistics Use more variants to include more causal variants (currently, most methods use around 1M variants) Integrate functional annotations and multi-ancestry data to prioritize causal variants and get better PGS for all populations 8.6 Exercise with LDpred2 and lassosum2 First, let’s carefully look at the LDpred2 tutorial. 8.6.1 Preparing the data Let’s first read the data produced in 3.3: library(bigsnpr) obj.bigsnp &lt;- snp_attach(&quot;tmp-data/GWAS_data_sorted_QC.rds&quot;) G &lt;- obj.bigsnp$genotypes NCORES &lt;- nb_cores() Let’s use some GWAS summary statistics for CAD that I derived from the UK Biobank (Bycroft et al., 2018): gz &lt;- runonce::download_file( &quot;https://figshare.com/ndownloader/files/38077323&quot;, dir = &quot;tmp-data&quot;, fname = &quot;sumstats_CAD_tuto.csv.gz&quot;) readLines(gz, n = 5) #&gt; [1] &quot;chr,pos,rsid,allele1,allele2,freq,info,beta,se&quot; #&gt; [2] &quot;1,721290,rs12565286,C,G,0.035889027911808,0.941918079726998,0.0361758959140647,0.0290865883937757&quot; #&gt; [3] &quot;1,752566,rs3094315,A,G,0.840799909379283,0.997586884856296,-0.0340838522604864,0.0144572980122262&quot; #&gt; [4] &quot;1,777122,rs2980319,T,A,0.871069627596275,0.997302103009046,-0.018725387068563,0.0155088941489917&quot; #&gt; [5] &quot;1,785989,rs2980300,C,T,0.869926014169995,0.991323313568096,-0.0182890096248982,0.0154915306067785&quot; Read and prepare them in the format required by LDpred2 (columns “chr”, “pos”, “a0”, “a1”, “beta”, “beta_se”, and “n_eff”, as well as additional columns “freq” and “info” for QC) Note that there were 20791 cases and 323124 controls in the GWAS. Can you estimate the total effective sample size without this information? Click to see solution sumstats &lt;- bigreadr::fread2( gz, select = c(&quot;chr&quot;, &quot;pos&quot;, &quot;allele2&quot;, &quot;allele1&quot;, &quot;beta&quot;, &quot;se&quot;, &quot;freq&quot;, &quot;info&quot;), col.names = c(&quot;chr&quot;, &quot;pos&quot;, &quot;a0&quot;, &quot;a1&quot;, &quot;beta&quot;, &quot;beta_se&quot;, &quot;freq&quot;, &quot;info&quot;)) # GWAS effective sample size for binary traits (4 / (1 / n_case + 1 / n_control)) # For quantitative traits, just use the total sample size for `n_eff`. (Neff &lt;- 4 / (1 / 20791 + 1 / 323124)) #&gt; [1] 78136.41 quantile(8 / sumstats$beta_se^2, 0.999) # one estimation #&gt; 99.9% #&gt; 73780.49 sumstats$n_eff &lt;- Neff Note that we recommend to use imputed HapMap3(+) variants when available, for which you can download some precomputed LD reference for European individuals based on the UK Biobank. Here, we will use the genotyped variants for this tutorial. Try to use an LD reference with at least 2000 individuals (we have only 1401 in this example). The LDpred2 tutorial provides more information. Match the variants in the GWAS summary statistics with the internal data we have here. What should you do for the allele frequencies? Click to see solution library(dplyr) map &lt;- transmute(obj.bigsnp$map, chr = chromosome, pos = physical.pos, a0 = allele2, a1 = allele1) info_snp &lt;- snp_match(sumstats, map, return_flip_and_rev = TRUE) %&gt;% mutate(freq = ifelse(`_REV_`, 1 - freq, freq), `_REV_` = NULL, `_FLIP_`= NULL) %&gt;% print() #&gt; chr pos a0 a1 beta beta_se freq info n_eff #&gt; 1 1 752566 T C 0.034083852 0.01445730 0.15920009 0.9975869 78136.41 #&gt; 2 1 785989 G A 0.018289010 0.01549153 0.13007399 0.9913233 78136.41 #&gt; 3 1 798959 G A 0.003331013 0.01307707 0.20524280 0.9734898 78136.41 #&gt; 4 1 947034 T C -0.021202725 0.02838148 0.03595249 0.9924989 78136.41 #&gt; _NUM_ID_.ss _NUM_ID_ #&gt; 1 2 2 #&gt; 2 4 4 #&gt; 3 5 5 #&gt; 4 6 6 #&gt; [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 340206 rows ] What QC could you perform on the GWAS summary statistics? Apply it here. Click to see solution Check the summary statistics; some quality control may be needed: hist(info_snp$n_eff) # all the same values, otherwise filter at 70% of max hist(info_snp$info) # very good imputation; filter e.g. at 0.7 or 0.8 summary(info_snp$freq) # can filter for MAF &gt; 0.01 (i.e. AF &gt; 0.01 &amp; AF &lt; 0.99) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 0.0000027 0.1100250 0.2261357 0.2365588 0.3580946 0.9998703 Then we can perform some quality control on the summary statistics by checking whether standard deviations (of genotypes) inferred from the external GWAS summary statistics are consistent with the ones in the internal data we have: af_ref &lt;- big_colstats(G, ind.col = info_snp$`_NUM_ID_`, ncores = NCORES)$sum / (2 * nrow(G)) sd_ref &lt;- sqrt(2 * af_ref * (1 - af_ref)) sd_ss &lt;- with(info_snp, 2 / sqrt(n_eff * beta_se^2 + beta^2)) is_bad &lt;- sd_ss &lt; (0.5 * sd_ref) | sd_ss &gt; (sd_ref + 0.1) | sd_ss &lt; 0.05 | sd_ref &lt; 0.05 # basically filtering small MAF library(ggplot2) ggplot(slice_sample(data.frame(sd_ref, sd_ss, is_bad), n = 100e3)) + geom_point(aes(sd_ref, sd_ss, color = is_bad), alpha = 0.5) + theme_bigstatsr(0.9) + scale_color_viridis_d(direction = -1) + geom_abline(linetype = 2) + labs(x = &quot;Standard deviations in the reference set&quot;, y = &quot;Standard deviations derived from the summary statistics&quot;, color = &quot;To remove?&quot;) When using quantitative traits (linear regression instead of logistic regression for the GWAS), you need to replace 2 by sd(y) when computing sd_ss (equations (6.2) and (6.1)). When allele frequencies are available in the GWAS summary statistics, you can use them (along with INFO scores) to get an even better match (equation (6.3)): sd_af &lt;- with(info_snp, sqrt(2 * freq * (1 - freq) * info)) ggplot(slice_sample(data.frame(sd_af, sd_ss), n = 100e3)) + geom_point(aes(sd_af, sd_ss), alpha = 0.5) + theme_bigstatsr(0.9) + geom_abline(linetype = 2, color = &quot;red&quot;) + labs(x = &quot;Standard deviations derived from allele frequencies&quot;, y = &quot;Standard deviations derived from the summary statistics&quot;) You can still use the reference panel to do some quality control by comparing allele frequencies, which is useful for detecting allelic errors (inversion of alleles so that the effect size should have the opposite sign) and the other issues detected before: af_diff &lt;- af_ref - info_snp$freq hist(af_diff, &quot;FD&quot;, xlim = c(-0.1, 0.1)) Then you can filter is_bad2 &lt;- sd_ss &lt; (0.7 * sd_af) | sd_ss &gt; (sd_af + 0.1) | sd_ss &lt; 0.05 | sd_af &lt; 0.05 | info_snp$info &lt; 0.7 | abs(af_diff) &gt; 0.07 # based on visual inspection mean(is_bad2) #&gt; [1] 0.002410276 df_beta &lt;- info_snp[!is_bad2, ] Then, we compute the correlation for each chromosome (note that we are using only 4 chromosomes here, for faster running of this tutorial): # Precomputed genetic positions (in cM) to avoid downloading large files in this tuto gen_pos &lt;- readRDS(runonce::download_file( &quot;https://figshare.com/ndownloader/files/38247288&quot;, dir = &quot;tmp-data&quot;, fname = &quot;gen_pos_tuto.rds&quot;)) df_beta &lt;- dplyr::filter(df_beta, chr %in% 1:4) # TO REMOVE (for speed here) for (chr in 1:4) { # REPLACE BY 1:22 print(chr) corr0 &lt;- runonce::save_run({ # indices in &#39;sumstats&#39; ind.chr &lt;- which(df_beta$chr == chr) # indices in &#39;G&#39; ind.chr2 &lt;- df_beta$`_NUM_ID_`[ind.chr] # genetic positions (in cM) # POS2 &lt;- snp_asGeneticPos(map$chr[ind.chr2], map$pos[ind.chr2], dir = &quot;tmp-data&quot;) POS2 &lt;- gen_pos[ind.chr2] # PRECOMPUTED HERE; USE snp_asGeneticPos() IN REAL CODE # compute the banded correlation matrix in sparse matrix format snp_cor(G, ind.col = ind.chr2, size = 3 / 1000, infos.pos = POS2, ncores = NCORES) }, file = paste0(&quot;tmp-data/corr_chr&quot;, chr, &quot;.rds&quot;)) # transform to SFBM (on-disk format) on the fly if (chr == 1) { ld &lt;- Matrix::colSums(corr0^2) corr &lt;- as_SFBM(corr0, &quot;tmp-data/corr&quot;, compact = TRUE) } else { ld &lt;- c(ld, Matrix::colSums(corr0^2)) corr$add_columns(corr0, nrow(corr)) } } #&gt; [1] 1 #&gt; user system elapsed #&gt; 110.20 0.71 28.93 #&gt; Code finished running at 2025-06-13 14:37:03 CEST #&gt; [1] 2 #&gt; user system elapsed #&gt; 141.60 0.64 37.57 #&gt; Code finished running at 2025-06-13 14:37:47 CEST #&gt; [1] 3 #&gt; user system elapsed #&gt; 130.27 0.75 34.44 #&gt; Code finished running at 2025-06-13 14:38:27 CEST #&gt; [1] 4 #&gt; user system elapsed #&gt; 112.91 0.95 31.21 #&gt; Code finished running at 2025-06-13 14:39:04 CEST file.size(corr$sbk) / 1024^3 # file size in GB #&gt; [1] 0.5756225 Note that you will need at least the same memory as this file size (to keep it cached for faster processing) + some other memory for all the results returned by LDpred2. If you do not have enough memory, processing will be very slow (because you would read the data from disk all the time). If using HapMap3 variants, requesting 60 GB should be enough. For this small example, 8 GB of RAM on a laptop should be enough. 8.6.2 LDpred2 Following the LDpred2 tutorial, run the three versions of LDpred2 and test their predictive performance for obj.bigsnp$fam$CAD. For speed here, you can use a smaller grid of hyper-parameters for LDpred2-grid, and a smaller number of burn-in/iterations for LDpred2-auto. Click to see solution We can now run LD score regression: (ldsc &lt;- with(df_beta, snp_ldsc(ld, length(ld), chi2 = (beta / beta_se)^2, sample_size = n_eff, blocks = NULL))) #&gt; int h2 #&gt; 0.9795999 0.0528327 ldsc_h2_est &lt;- ldsc[[&quot;h2&quot;]] We can now run LDpred2-inf very easily: # LDpred2-inf beta_inf &lt;- snp_ldpred2_inf(corr, df_beta, ldsc_h2_est) pred_inf &lt;- big_prodVec(G, beta_inf, ind.col = df_beta$`_NUM_ID_`) AUCBoot(pred_inf, obj.bigsnp$fam$CAD) #&gt; Mean 2.5% 97.5% Sd #&gt; 0.55123396 0.51920429 0.58365525 0.01631818 For LDpred2(-grid), this is the grid we recommend to use: # LDpred2-grid (h2_seq &lt;- round(ldsc_h2_est * c(0.3, 0.7, 1, 1.4), 4)) #&gt; [1] 0.0158 0.0370 0.0528 0.0740 (p_seq &lt;- signif(seq_log(1e-5, 1, length.out = 21), 2)) #&gt; [1] 1.0e-05 1.8e-05 3.2e-05 5.6e-05 1.0e-04 1.8e-04 3.2e-04 5.6e-04 1.0e-03 #&gt; [10] 1.8e-03 3.2e-03 5.6e-03 1.0e-02 1.8e-02 3.2e-02 5.6e-02 1.0e-01 1.8e-01 #&gt; [19] 3.2e-01 5.6e-01 1.0e+00 params &lt;- expand.grid(p = p_seq, h2 = h2_seq, sparse = c(FALSE, TRUE)) dim(params) #&gt; [1] 168 3 Here, we will be using this smaller grid instead (for speed in this tutorial): # smaller grid for tutorial only (USE PREVIOUS ONE IN REAL CODE) (params &lt;- expand.grid(p = signif(seq_log(1e-4, 0.5, length.out = 16), 2), h2 = round(ldsc_h2_est, 4), sparse = TRUE)) #&gt; p h2 sparse #&gt; 1 0.00010 0.0528 TRUE #&gt; 2 0.00018 0.0528 TRUE #&gt; 3 0.00031 0.0528 TRUE #&gt; 4 0.00055 0.0528 TRUE #&gt; 5 0.00097 0.0528 TRUE #&gt; 6 0.00170 0.0528 TRUE #&gt; 7 0.00300 0.0528 TRUE #&gt; 8 0.00530 0.0528 TRUE #&gt; 9 0.00940 0.0528 TRUE #&gt; 10 0.01700 0.0528 TRUE #&gt; 11 0.02900 0.0528 TRUE #&gt; 12 0.05200 0.0528 TRUE #&gt; 13 0.09100 0.0528 TRUE #&gt; 14 0.16000 0.0528 TRUE #&gt; 15 0.28000 0.0528 TRUE #&gt; 16 0.50000 0.0528 TRUE beta_grid &lt;- snp_ldpred2_grid(corr, df_beta, params, ncores = NCORES) params$sparsity &lt;- colMeans(beta_grid == 0) Then, we can compute the corresponding PGS for all these models, and visualize their performance: pred_grid &lt;- big_prodMat(G, beta_grid, ind.col = df_beta[[&quot;_NUM_ID_&quot;]], ncores = NCORES) params$score &lt;- apply(pred_grid, 2, function(.x) { if (all(is.na(.x))) return(NA) # models that diverged substantially summary(glm( # simply use `lm()` for quantitative traits CAD ~ .x + sex + age, data = obj.bigsnp$fam, family = &quot;binomial&quot; ))$coef[&quot;.x&quot;, 3] }) ggplot(params, aes(x = p, y = score, color = as.factor(h2))) + theme_bigstatsr() + geom_point() + geom_line() + scale_x_log10(breaks = 10^(-5:0), minor_breaks = params$p) + facet_wrap(~ sparse, labeller = label_both) + labs(y = &quot;GLM Z-Score&quot;, color = &quot;h2&quot;) + theme(legend.position = &quot;top&quot;, panel.spacing = unit(1, &quot;lines&quot;)) Then you can use the best-performing model here. Note that, in practice, you should use only individuals from the validation set to compute the $score and then evaluate the best model for the individuals in the test set (unrelated to those in the validation set). library(dplyr) best_beta_grid &lt;- params %&gt;% mutate(id = row_number()) %&gt;% arrange(desc(score)) %&gt;% print() %&gt;% slice(1) %&gt;% pull(id) %&gt;% beta_grid[, .] #&gt; p h2 sparse sparsity score id #&gt; 1 0.00300 0.0528 TRUE 0.7668965 4.141109 7 #&gt; 2 0.00530 0.0528 TRUE 0.6853274 3.994907 8 #&gt; 3 0.00940 0.0528 TRUE 0.5946515 3.978679 9 #&gt; 4 0.00170 0.0528 TRUE 0.8318286 3.956292 6 #&gt; 5 0.00097 0.0528 TRUE 0.8793796 3.929743 5 #&gt; 6 0.01700 0.0528 TRUE 0.5077261 3.842456 10 #&gt; 7 0.00031 0.0528 TRUE 0.9383287 3.836808 3 #&gt; 8 0.00010 0.0528 TRUE 0.9666869 3.818947 1 #&gt; [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 8 rows ] What is the best model with less than 5% of variants used? To run LDpred2-auto, you can use: # LDpred2-auto multi_auto &lt;- snp_ldpred2_auto( corr, df_beta, h2_init = ldsc_h2_est, vec_p_init = seq_log(1e-4, 0.2, 30), burn_in = 100, num_iter = 100, # TO REMOVE, for speed here allow_jump_sign = FALSE, use_MLE = FALSE, # USE `TRUE` ONLY FOR GWAS WITH LARGE N AND M shrink_corr = 0.95, ncores = NCORES) Perform some quality control on the chains: # `range` should be between 0 and 2 (range &lt;- sapply(multi_auto, function(auto) diff(range(auto$corr_est)))) #&gt; [1] 0.05448907 0.05442862 0.05261028 0.05480762 0.05368479 0.05576853 #&gt; [7] 0.05444713 0.05452793 0.05538609 0.05197896 0.05241466 0.05446092 #&gt; [13] 0.05406081 0.05331133 0.05599592 0.05493762 0.05328035 0.05303112 #&gt; [19] 0.05475263 0.05279201 0.05363907 0.05490972 0.05541536 0.05407016 #&gt; [25] 0.05410121 0.05563394 0.05560854 0.05553329 0.05448200 0.05344541 (keep &lt;- which(range &gt; (0.95 * quantile(range, 0.95, na.rm = TRUE)))) #&gt; [1] 1 2 4 5 6 7 8 9 12 13 14 15 16 17 18 19 21 22 23 24 25 26 27 28 29 #&gt; [26] 30 To get the final effects / predictions, you should only use chains that pass this filtering: final_beta_auto &lt;- rowMeans(sapply(multi_auto[keep], function(auto) auto$beta_est)) We can finally test the final prediction final_pred_auto &lt;- big_prodVec(G, final_beta_auto, ind.col = df_beta[[&quot;_NUM_ID_&quot;]], ncores = NCORES) AUCBoot(final_pred_auto, obj.bigsnp$fam$CAD) #&gt; Mean 2.5% 97.5% Sd #&gt; 0.56399026 0.53174361 0.59614571 0.01638768 8.6.3 lassosum2: grid of models lassosum2 is a re-implementation of the lassosum model (Mak et al., 2017) that now uses the exact same input parameters as LDpred2 (corr and df_beta). It can therefore be run next to LDpred2 and the best model can be chosen using the validation set. Run lassosum2 and test its predictive performance for obj.bigsnp$fam$CAD. For speed here, you can use parameters nlambda = 10, maxiter = 50. Click to see solution beta_lassosum2 &lt;- snp_lassosum2( corr, df_beta, ncores = NCORES, nlambda = 10, maxiter = 50) # TO REMOVE, for speed here As with LDpred2-grid, we can compute the corresponding PGS for all these models, and visualize their performance: pred_grid2 &lt;- big_prodMat(G, beta_lassosum2, ind.col = df_beta[[&quot;_NUM_ID_&quot;]], ncores = NCORES) params2 &lt;- attr(beta_lassosum2, &quot;grid_param&quot;) params2$score &lt;- apply(pred_grid2, 2, function(.x) { if (all(is.na(.x))) return(NA) # models that diverged substantially summary(glm( # simply use `lm()` for quantitative traits CAD ~ .x + sex + age, data = obj.bigsnp$fam, family = &quot;binomial&quot; ))$coef[&quot;.x&quot;, 3] }) ggplot(params2, aes(x = lambda, y = score, color = as.factor(delta))) + theme_bigstatsr() + geom_point() + geom_line() + scale_x_log10(breaks = 10^(-5:0)) + labs(y = &quot;GLM Z-Score&quot;, color = &quot;delta&quot;) #&gt; Warning: Removed 4 rows containing missing values or values outside the scale range #&gt; (`geom_point()`). #&gt; Warning: Removed 4 rows containing missing values or values outside the scale range #&gt; (`geom_line()`). When are large delta values useful? best_grid_lassosum2 &lt;- params2 %&gt;% mutate(id = row_number()) %&gt;% arrange(desc(score)) %&gt;% print() %&gt;% slice(1) %&gt;% pull(id) %&gt;% beta_lassosum2[, .] #&gt; lambda delta num_iter time sparsity score id #&gt; 1 0.00996541 0.001 51 0.15 0.9937624 4.112203 3 #&gt; 2 0.00996541 0.010 51 0.15 0.9936938 4.106545 13 #&gt; 3 0.00996541 0.100 45 0.14 0.9930475 4.053764 23 #&gt; 4 0.01579411 0.001 51 0.10 0.9997062 3.977849 2 #&gt; 5 0.00996541 1.000 14 0.05 0.9902959 3.976200 33 #&gt; 6 0.01579411 0.010 51 0.08 0.9997062 3.973049 12 #&gt; 7 0.01579411 0.100 35 0.08 0.9996181 3.900467 22 #&gt; [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 33 rows ] We can choose the best-overall model from both LDpred2-grid and lassosum2: best_grid_overall &lt;- `if`(max(params2$score, na.rm = TRUE) &gt; max(params$score, na.rm = TRUE), best_grid_lassosum2, best_beta_grid) References Bycroft, C., Freeman, C., Petkova, D., Band, G., Elliott, L.T., Sharp, K., et al. (2018). The UK Biobank resource with deep phenotyping and genomic data. Nature, 562, 203–209. Choi, S.W., &amp; O’Reilly, P.F. (2019). PRSice-2: Polygenic risk score software for biobank-scale data. Gigascience, 8, giz082. Daetwyler, H.D., Villanueva, B., &amp; Woolliams, J.A. (2008). Accuracy of predicting the genetic risk of disease using a genome-wide approach. PloS One, 3, e3395. Ding, Y., Hou, K., Xu, Z., Pimplaskar, A., Petter, E., Boulier, K., et al. (2023). Polygenic scoring accuracy varies across the genetic ancestry continuum. Nature, 618, 774–781. Fuat, A., Adlen, E., Monane, M., Coll, R., Groves, S., Little, E., et al. (2024). A polygenic risk score added to a QRISK 2 cardiovascular disease risk calculator demonstrated robust clinical acceptance and clinical utility in the primary care setting. European Journal of Preventive Cardiology, 31, 716–722. Ge, T., Chen, C.-Y., Ni, Y., Feng, Y.-C.A., &amp; Smoller, J.W. (2019). Polygenic prediction via Bayesian regression and continuous shrinkage priors. Nature Communications, 10, 1776. Henches, L., Kim, J., Yang, Z., Rubinacci, S., Pires, G., Albiñana, C., et al. (2025). Polygenic risk score prediction accuracy convergence. Human Genetics and Genomics Advances, 6. Retrieved from https://doi.org/10.1016/j.xhgg.2025.100457 Hou, K., Ding, Y., Xu, Z., Wu, Y., Bhattacharya, A., Mester, R., et al. (2023). Causal effects on complex traits are similar for common variants across segments of different continental ancestries within admixed individuals. Nature Genetics, 55, 549–558. Hu, S., Ferreira, L.A., Shi, S., Hellenthal, G., Marchini, J., Lawson, D.J., &amp; Myers, S.R. (2025). Fine-scale population structure and widespread conservation of genetic effect sizes between human groups across traits. Nature Genetics, 1–11. Kelemen, M., Xu, Y., Jiang, T., Zhao, J.H., Anderson, C.A., Wallace, C., et al. (2025). Performance of deep-learning-based approaches to improve polygenic scores. Nature Communications, 16, 1–9. Kulm, S., Marderstein, A., Mezey, J., &amp; Elemento, O. (2020). A systematic framework for assessing the clinical impact of polygenic risk scores. medRxiv, 2020–2004. Lloyd-Jones, L.R., Zeng, J., Sidorenko, J., Yengo, L., Moser, G., Kemper, K.E., et al. (2019). Improved polygenic prediction by Bayesian multiple regression on summary statistics. Nature Communications, 10, 5086. Mak, T.S.H., Porsch, R.M., Choi, S.W., Zhou, X., &amp; Sham, P.C. (2017). Polygenic scores via penalized regression on summary statistics. Genetic Epidemiology, 41, 469–480. Pain, O., Glanville, K.P., Hagenaars, S.P., Selzam, S., Fürtjes, A.E., Gaspar, H.A., et al. (2021). Evaluation of polygenic prediction methodology within a reference-standardized framework. PLoS Genetics, 17, e1009021. Privé, F., Albiñana, C., Arbel, J., Pasaniuc, B., &amp; Vilhjálmsson, B.J. (2023). Inferring disease architecture and predictive ability with LDpred2-auto. The American Journal of Human Genetics, 110, 2042–2055. Privé, F., Arbel, J., Aschard, H., &amp; Vilhjálmsson, B.J. (2022). Identifying and correcting for misspecifications in GWAS summary statistics and polygenic scores. Human Genetics and Genomics Advances, 3, 100136. Privé, F., Arbel, J., &amp; Vilhjálmsson, B.J. (2020). LDpred2: better, faster, stronger. Bioinformatics, 36, 5424–5431. Privé, F., Aschard, H., &amp; Blum, M.G. (2019). Efficient implementation of penalized regression for genetic risk prediction. Genetics, 212, 65–74. Privé, F., Aschard, H., Carmi, S., Folkersen, L., Hoggart, C., O’Reilly, P.F., &amp; Vilhjálmsson, B.J. (2022). Portability of 245 polygenic scores when derived from the UK Biobank and applied to 9 ancestry groups from the same cohort. The American Journal of Human Genetics, 109, 12–23. Privé, F., Vilhjálmsson, B.J., Aschard, H., &amp; Blum, M.G.B. (2019). Making the most of clumping and thresholding for polygenic scores. The American Journal of Human Genetics, 105, 1213–1221. Wang, X., Walker, A., Revez, J.A., Ni, G., Adams, M.J., McIntosh, A.M., et al. (2023). Polygenic risk prediction: Why and when out-of-sample prediction R2 can exceed SNP-based heritability. The American Journal of Human Genetics, 110, 1207–1215. Wang, Y., Guo, J., Ni, G., Yang, J., Visscher, P.M., &amp; Yengo, L. (2020). Theoretical and empirical quantification of the accuracy of polygenic scores in ancestry divergent populations. Nature Communications, 11, 3865. Zheng, Z., Liu, S., Sidorenko, J., Wang, Y., Lin, T., Yengo, L., et al. (2024). Leveraging functional genomic annotations and genome coverage to improve polygenic prediction of complex traits within and between ancestries. Nature Genetics, 56, 767–777. "],["fine-mapping-with-susie.html", "Chapter 9 Fine-mapping with SuSiE 9.1 Objective 9.2 Simulated Example 9.3 Real data", " Chapter 9 Fine-mapping with SuSiE Read more in this PowerPoint presentation. 9.1 Objective SuSiE (G. Wang, Sarkar, Carbonetto, &amp; Stephens, 2020; Zou et al., 2022) is a fine-mapping method used to determine which variants are most likely to be causal for a trait. It uses a Bayesian iterative approach to create “credible sets” of SNPs that are likely to each contain a causal variant. In this tutorial we will first use SuSiE on simulated data where we know the true causal effects. We will then apply it to real data from (Ruth et al., 2020). This tutorial was created largely by modifying pre-existing tutorials written by Alesha Hatton (https://cnsgenomics.com/data/teaching/GNGWS23/module1/11_fine-mapping.html) and Gao Wang (https://stephenslab.github.io/susie-paper/manuscript_results/pedagogical_example.html). Thank you Alesha and Gao for your excellent tutorials! 9.1.1 Requirements: 1000 Genomes European reference panel in PLINK1 format (bed/bim/fam) GWAS summary statistics from (Ruth et al., 2020) (GWAS catalog accession: GCST90012102) PLINK 1.9 R packages susieR, dplyr, ggplot2, ggrepel, bigsnpr, and bigreadr. 9.1.2 Load libraries library(susieR) library(dplyr) library(ggplot2) library(ggrepel) library(bigsnpr) library(bigreadr) set.seed(7236) 9.2 Simulated Example This example uses simulated data to illustrate the use of SuSiE for fine-mapping over stage-wise selection. 9.2.1 Load data # This data is included in the susieR package dat &lt;- N2finemapping str(dat) #&gt; List of 8 #&gt; $ X : num [1:574, 1:1002] 0.703 0.703 -0.297 -0.297 0.703 ... #&gt; $ chrom : chr &quot;chr8&quot; #&gt; $ pos : int [1:1002(1d)] 38854423 38854585 38854614 38854829 38855770 38856373 38856591 38856612 38856903 38857229 ... #&gt; $ true_coef : num [1:1002, 1:2] 0 0 0 0 0 0 0 0 0 0 ... #&gt; $ residual_variance: num [1:2(1d)] 1.1 1.01 #&gt; $ Y : num [1:574, 1:2] 1.0446 -1.3656 -0.0235 -0.4344 0.4681 ... #&gt; $ allele_freq : num [1:1002, 1] 0.1483 0.0209 0.1368 0.2134 0.5627 ... #&gt; $ V : num [1:2, 1:2] 1.344 -0.0336 -0.0336 1.2541 # This simulated data-set has two replicates. Let&#39;s focus on the first replicate: y &lt;- dat$Y[, 1] 9.2.2 Fit SuSiE to the data Also perform univariate regression so that PIPs can be compared with p-values # Fit SuSiE with L=5 (maximum number of causal variants per replicate) fitted &lt;- susie(dat$X, y, L = 5, estimate_residual_variance = TRUE, scaled_prior_variance = 0.2, tol = 1e-3, track_fit = TRUE, compute_univariate_zscore = TRUE, coverage = 0.95, min_abs_corr = 0.1) str(fitted, max.level = 1) #&gt; List of 21 #&gt; $ alpha : num [1:5, 1:1002] 9.98e-04 8.96e-32 9.32e-19 9.98e-04 9.98e-04 ... #&gt; $ mu : num [1:5, 1:1002] 0 -0.0221 -0.0514 0 0 ... #&gt; $ mu2 : num [1:5, 1:1002] 0 0.00232 0.00447 0 0 ... #&gt; $ Xr : num [1:574] -0.842 -0.274 -0.842 -0.842 1.209 ... #&gt; $ KL : num [1:5] -1.49e-05 5.87 8.68 -1.49e-05 -1.49e-05 #&gt; $ lbf : num [1:5] 1.49e-05 6.23e+01 3.31e+01 1.49e-05 1.49e-05 #&gt; $ lbf_variable : num [1:5, 1:1002] 0 -2.33 -1.49 0 0 ... #&gt; $ sigma2 : num 1.06 #&gt; $ V : num [1:5] 0 0.251 0.154 0 0 #&gt; $ pi : num [1:1002] 0.000998 0.000998 0.000998 0.000998 0.000998 ... #&gt; $ null_index : num 0 #&gt; $ converged : logi TRUE #&gt; $ elbo : num [1:10] -867 -861 -858 -854 -850 ... #&gt; $ niter : int 10 #&gt; $ intercept : num -7.82e-17 #&gt; $ fitted : num [1:574] -0.842 -0.274 -0.842 -0.842 1.209 ... #&gt; $ trace :List of 10 #&gt; $ sets :List of 5 #&gt; $ pip : num [1:1002] 0 0 0 0 0 0 0 0 0 0 ... #&gt; $ z : num [1:1002] -0.8091 1.1147 -0.5836 -0.0842 -2.1866 ... #&gt; $ X_column_scale_factors: num [1:1002] 0.486 0.2 0.494 0.603 0.708 ... #&gt; - attr(*, &quot;class&quot;)= chr &quot;susie&quot; # Let&#39;s have a look at the first iteration (by setting max_iter=1 this will give us the first iteration of fitted) fitted.one.iter &lt;- susie(dat$X, y, L = 5, max_iter = 1, estimate_residual_variance = TRUE, scaled_prior_variance = 0.2, tol = 1e-3, coverage = 0.95, min_abs_corr = 0.1) #&gt; Warning in susie(dat$X, y, L = 5, max_iter = 1, estimate_residual_variance = #&gt; TRUE, : IBSS algorithm did not converge in 1 iterations! str(fitted.one.iter) #&gt; List of 19 #&gt; $ alpha : num [1:5, 1:1002] 5.22e-18 1.61e-04 6.10e-04 7.44e-04 8.03e-04 ... #&gt; $ mu : num [1:5, 1:1002] -0.03868 -0.02164 -0.01568 -0.01138 -0.00922 ... #&gt; $ mu2 : num [1:5, 1:1002] 0.00381 0.002501 0.001695 0.001228 0.000992 ... #&gt; $ Xr : num [1:574] -0.814 -0.187 -0.802 -0.748 0.675 ... #&gt; $ KL : num [1:5] 6.021 2.322 0.713 0.404 0.295 #&gt; $ lbf : num [1:5] 31.0473 0.9323 0.0954 0.0364 0.0199 #&gt; $ lbf_variable : num [1:5, 1:1002] -1.836 -0.893 -0.396 -0.257 -0.198 ... #&gt; $ sigma2 : num 1.15 #&gt; $ V : num [1:5] 0.17389 0.01527 0.00379 0.00207 0.00148 #&gt; $ pi : num [1:1002] 0.000998 0.000998 0.000998 0.000998 0.000998 ... #&gt; $ null_index : num 0 #&gt; $ elbo : num -867 #&gt; $ niter : int 1 #&gt; $ converged : logi FALSE #&gt; $ intercept : num -3.47e-17 #&gt; $ fitted : num [1:574] -0.814 -0.187 -0.802 -0.748 0.675 ... #&gt; $ sets :List of 5 #&gt; ..$ cs :List of 1 #&gt; .. ..$ L1: int [1:24] 765 767 770 774 776 778 780 782 783 788 ... #&gt; ..$ purity :&#39;data.frame&#39;: 1 obs. of 3 variables: #&gt; .. ..$ min.abs.corr : num 0.78 #&gt; .. ..$ mean.abs.corr : num 0.922 #&gt; .. ..$ median.abs.corr: num 0.92 #&gt; ..$ cs_index : int 1 #&gt; ..$ coverage : num 0.955 #&gt; ..$ requested_coverage: num 0.95 #&gt; $ pip : num [1:1002] 0.00232 0.00219 0.0022 0.0022 0.00315 ... #&gt; $ X_column_scale_factors: num [1:1002] 0.486 0.2 0.494 0.603 0.708 ... #&gt; - attr(*, &quot;class&quot;)= chr &quot;susie&quot; 9.2.3 Plot the SuSiE results Plot both PIPs from SuSiE as well as the p-values from the regressions. b &lt;- dat$true_coef[, 1] b[which(b != 0)] &lt;- 1 # Run this code all at once to get side-by-side plots par_saved &lt;- par(mfrow = c(1, 3), cex.axis = 0.9) # Plot the marginal associations susie_plot(fitted, y = &quot;z&quot;, b = b, max_cs = 1, main = &quot;Marginal associations&quot;, xlab = &quot;variable (SNP)&quot;, col = &quot;#767676&quot;) # Plot PIPs after the first iteration susie_plot(fitted.one.iter, y = &quot;PIP&quot;, b = b, max_cs = 0.4, main = &quot;IBSS after 1 iteration&quot;, add_legend = FALSE, ylim = c(0, 1), xlab = &quot;variable (SNP)&quot;, col = &quot;#767676&quot;) # Plot PIPs after convergence susie_plot(fitted, y = &quot;PIP&quot;, b = b, max_cs = 0.4, main = &quot;IBSS after 10 iterations&quot;, add_legend = FALSE, ylim = c(0, 1), xlab = &quot;variable (SNP)&quot;, col = &quot;#767676&quot;) par(par_saved) # back to as before The “true” effects are highlighted in red. The strongest signal by p-value does not contain the causal variant, but is being tagged by two causal variants. The first iteration of SuSiE identifies the strongest signal by p-value, but by the 10th iteration the true causal variants are identified within two credible sets. 9.2.4 Let’s take a closer look at which variants are in the credible sets fitted.one.iter$sets$cs #&gt; $L1 #&gt; [1] 765 767 770 774 776 778 780 782 783 788 790 791 792 814 817 824 827 834 837 #&gt; [20] 838 847 849 868 869 fitted$sets$cs #&gt; $L2 #&gt; [1] 850 913 914 915 916 920 924 925 926 927 930 931 933 934 935 #&gt; [16] 942 946 948 951 952 962 967 968 979 980 982 983 985 988 989 #&gt; [31] 993 994 996 999 1000 1001 1002 #&gt; #&gt; $L3 #&gt; [1] 337 379 440 9.2.5 How correlated are the variants in the credible sets? fitted$sets$purity #&gt; min.abs.corr mean.abs.corr median.abs.corr #&gt; L2 0.9722386 0.9938064 0.9947184 #&gt; L3 0.8534981 0.8775989 0.8848378 9.3 Real data In the simulation we used individual level data (genotypes and phenotypes). Often this is not available due to data access restrictions. Here we will use GWAS summary statistics and an LD reference panel. Let’s look at bioavailable testosterone in females from (Ruth et al., 2020). 9.3.1 Import GWAS data Here we use (a subset of) the GWAS summary statistics originally accessible from the GWAS Catalog (accession number: GCST90012102). # original data # tgz &lt;- runonce::download_file( # &quot;https://ftp.ebi.ac.uk/pub/databases/gwas/summary_statistics/GCST90012001-GCST90013000/GCST90012102/GCST90012102_buildGRCh37.tsv.gz&quot;, # dir = &quot;tmp-data&quot;) # 419 MB # small subset, for the tutorial tgz &lt;- runonce::download_file( &quot;https://figshare.com/ndownloader/files/55262768&quot;, dir = &quot;tmp-data&quot;, fname = &quot;FT_sumstats_small.tsv.gz&quot;) readLines(tgz, n = 5) #&gt; [1] &quot;variant_id\\tchromosome\\tbase_pair_location\\teffect_allele\\tother_allele\\teffect_allele_frequency\\timputation_quality\\tbeta\\tstandard_error\\tp_value&quot; #&gt; [2] &quot;rs72858371\\t1\\t6409383\\tT\\tC\\t0.986224\\t0.977552\\t-0.00615364\\t0.0108886\\t0.64&quot; #&gt; [3] &quot;rs72633437\\t1\\t6409494\\tC\\tT\\t0.906138\\t0.988959\\t0.00289667\\t0.00434703\\t0.39&quot; #&gt; [4] &quot;rs151043271\\t1\\t6409495\\tG\\tA\\t0.998537\\t0.732147\\t-0.0244749\\t0.0386808\\t0.4&quot; #&gt; [5] &quot;rs58484986\\t1\\t6409653\\tC\\tG\\t0.986228\\t0.976894\\t-0.00623617\\t0.0108958\\t0.63&quot; And some reference panel (also subsetted for this tutorial) to compute LD from: # original data # gzip &lt;- runonce::download_file( # &quot;https://vu.data.surfsara.nl/index.php/s/VZNByNwpD8qqINe/download&quot;, # fname = &quot;g1000_eur.zip&quot;, dir = &quot;tmp-data&quot;) # 488 MB # small subset, for the tutorial gzip &lt;- runonce::download_file( &quot;https://figshare.com/ndownloader/files/55263098&quot;, fname = &quot;g1000_eur_small.zip&quot;, dir = &quot;tmp-data&quot;) unzip(gzip, exdir = &quot;tmp-data&quot;, overwrite = FALSE) g1000_map &lt;- fread2(&quot;tmp-data/g1000_eur_small.bim&quot;, select = c(1:2, 4:6), col.names = c(&quot;chr&quot;, &quot;rsid&quot;, &quot;pos&quot;, &quot;a1&quot;, &quot;a0&quot;)) # import GWAS summary statistics of bioavailable testosterone in females FT &lt;- fread2(tgz) head(FT) #&gt; variant_id chromosome base_pair_location effect_allele other_allele #&gt; 1 rs72858371 1 6409383 T C #&gt; 2 rs72633437 1 6409494 C T #&gt; 3 rs151043271 1 6409495 G A #&gt; 4 rs58484986 1 6409653 C G #&gt; 5 rs60772726 1 6409662 A T #&gt; effect_allele_frequency imputation_quality beta standard_error p_value #&gt; 1 0.986224 0.977552 -0.00615364 0.01088860 0.64 #&gt; 2 0.906138 0.988959 0.00289667 0.00434703 0.39 #&gt; 3 0.998537 0.732147 -0.02447490 0.03868080 0.40 #&gt; 4 0.986228 0.976894 -0.00623617 0.01089580 0.63 #&gt; 5 0.986228 0.976844 -0.00625052 0.01089570 0.63 #&gt; [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 1 rows ] FT_cleaned &lt;- FT %&gt;% # QC on MAF &gt;= 0.01 filter(effect_allele_frequency &gt;= 0.01 &amp; effect_allele_frequency &lt;= 0.99) %&gt;% # rename some columns for compatibility with bigsnpr::snp_match() rename(chr = chromosome, pos = base_pair_location, a1 = effect_allele, a0 = other_allele) %&gt;% # align alleles to reference panel snp_match(g1000_map) # match on chr/pos but can also match on chr/rsid #&gt; 25,328 variants to be matched. #&gt; 3,099 ambiguous SNPs have been removed. #&gt; 18,293 variants have been matched; 0 were flipped and 14,274 were reversed. 9.3.2 Find window around loci of interest Let’s look at two loci: Locus 1: rs1989147. Locus 2: rs34954997, rs11879227, rs34255979. These variants in locus 2 are nearby so will be considered as one locus for the purpose of running SuSiE. # find coordinates filter(FT_cleaned, rsid == &quot;rs1989147&quot;) # on chr1 #&gt; chr pos a0 a1 variant_id effect_allele_frequency imputation_quality #&gt; 1 1 7909373 C T rs1989147 0.806791 0.97999 #&gt; beta standard_error p_value _NUM_ID_.ss rsid _NUM_ID_ #&gt; 1 -0.0235512 0.00322554 7e-14 5329 rs1989147 12274 # Extract 1 Mb locus surrounding rs1989147 locus1 &lt;- filter(FT_cleaned, chr == 1, pos &gt; 7909373 - 5e5, pos &lt; 7909373 + 5e5) # Plot locus ggplot(locus1, aes(x = pos, y = -log10(p_value))) + theme_bw(14) + geom_point(alpha = 0.8, size = 1.3) + geom_point(aes(x = pos, y = -log10(p_value)), color = &quot;red&quot;, size = 2, data = filter(locus1, rsid == &quot;rs1989147&quot;)) + geom_label_repel(aes(label = ifelse(rsid == &quot;rs1989147&quot;, rsid, NA)), size = 4, min.segment.length = 0) #&gt; Warning: Removed 2815 rows containing missing values or values outside the scale range #&gt; (`geom_label_repel()`). 9.3.3 Create an LD matrix As we are using GWAS summary statistics, we need information of the correlation between variants. Here we are using 1000 Genomes European for convenience. If you are performing this analysis yourself, it is advised to use a larger sample. For fine-mapping, it is often recommended to use in-sample LD (LD computed from the original GWAS data). Here we use PLINK, but remember that you can also use functions from bigsnpr for computing LD and actually also from bigstatsr when you need all correlation values for a set of variants (see big_cor()). # Export the SNPs in this locus locus1_snps_txt &lt;- tempfile(fileext = &quot;.txt&quot;) write(locus1$rsid, file = locus1_snps_txt) # Compute correlation matrix (LD) with PLINK plink &lt;- download_plink(&quot;tmp-data&quot;) system(glue::glue( &quot;{plink} --bfile tmp-data/g1000_eur_small&quot;, &quot; --extract {locus1_snps_txt}&quot;, &quot; --keep-allele-order&quot;, &quot; --r square&quot;, # correlations as a square matrix, not squared correlations &quot; --out tmp-data/ld_locus1&quot; )) #&gt; PLINK v1.9.0-b.7.7 64-bit (22 Oct 2024) cog-genomics.org/plink/1.9/ #&gt; (C) 2005-2024 Shaun Purcell, Christopher Chang GNU General Public License v3 #&gt; Logging to tmp-data/ld_locus1.log. #&gt; Options in effect: #&gt; --bfile tmp-data/g1000_eur_small #&gt; --extract C:\\Users\\au639593\\AppData\\Local\\Temp\\Rtmp6Th2Hp\\file6ebc60487075.txt #&gt; --keep-allele-order #&gt; --out tmp-data/ld_locus1 #&gt; --r square #&gt; #&gt; 32574 MB RAM detected; reserving 16287 MB for main workspace. #&gt; 56148 variants loaded from .bim file. #&gt; 503 people (240 males, 263 females) loaded from .fam. #&gt; --extract: 2816 variants remaining. #&gt; Using up to 8 threads (change this with --threads). #&gt; Before main variant filters, 503 founders and 0 nonfounders present. #&gt; Calculating allele frequencies... 0%\b\b1%\b\b2%\b\b3%\b\b4%\b\b5%\b\b6%\b\b7%\b\b8%\b\b9%\b\b10%\b\b\b11%\b\b\b12%\b\b\b13%\b\b\b14%\b\b\b15%\b\b\b16%\b\b\b17%\b\b\b18%\b\b\b19%\b\b\b20%\b\b\b21%\b\b\b22%\b\b\b23%\b\b\b24%\b\b\b25%\b\b\b26%\b\b\b27%\b\b\b28%\b\b\b29%\b\b\b30%\b\b\b31%\b\b\b32%\b\b\b33%\b\b\b34%\b\b\b35%\b\b\b36%\b\b\b37%\b\b\b38%\b\b\b39%\b\b\b40%\b\b\b41%\b\b\b42%\b\b\b43%\b\b\b44%\b\b\b45%\b\b\b46%\b\b\b47%\b\b\b48%\b\b\b49%\b\b\b50%\b\b\b51%\b\b\b52%\b\b\b53%\b\b\b54%\b\b\b55%\b\b\b56%\b\b\b57%\b\b\b58%\b\b\b59%\b\b\b60%\b\b\b61%\b\b\b62%\b\b\b63%\b\b\b64%\b\b\b65%\b\b\b66%\b\b\b67%\b\b\b68%\b\b\b69%\b\b\b70%\b\b\b71%\b\b\b72%\b\b\b73%\b\b\b74%\b\b\b75%\b\b\b76%\b\b\b77%\b\b\b78%\b\b\b79%\b\b\b80%\b\b\b81%\b\b\b82%\b\b\b83%\b\b\b84%\b\b\b85%\b\b\b86%\b\b\b87%\b\b\b88%\b\b\b89%\b\b\b90%\b\b\b91%\b\b\b92%\b\b\b93%\b\b\b94%\b\b\b95%\b\b\b96%\b\b\b97%\b\b\b98%\b\b\b99%\b\b\b\b done. #&gt; Total genotyping rate is in [0.9999995, 1). #&gt; 2816 variants and 503 people pass filters and QC. #&gt; Note: No phenotypes present. #&gt; --r square to tmp-data/ld_locus1.ld ... 0% [processing]\b\b\b\b\b\b\b\b\b\b\bwriting] \b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\bdone. # Read in LD matrix ld_mat &lt;- as.matrix(fread2(&quot;tmp-data/ld_locus1.ld&quot;)) 9.3.4 Read in data and fit SuSiE susie_rss() is the function to fit SuSiE using summary statistics. Earlier we used susie() function which requires individual level data. # Fit SuSiE using sample size from publication (n=188507) fitted_rss_1 = susie_rss(bhat = locus1$beta, shat = locus1$standard_error, n = 188507, R = ld_mat) 9.3.5 Examine results # Examine results summary(fitted_rss_1) #&gt; #&gt; Variables in credible sets: #&gt; #&gt; variable variable_prob cs #&gt; 1456 0.09199869 1 #&gt; 1420 0.07451778 1 #&gt; 1417 0.06848847 1 #&gt; 1401 0.06379124 1 #&gt; 1334 0.05783427 1 #&gt; 1364 0.05781606 1 #&gt; 1367 0.05778666 1 #&gt; 1331 0.05777183 1 #&gt; 1347 0.05429258 1 #&gt; 1288 0.05367458 1 #&gt; 1339 0.04928034 1 #&gt; 1280 0.04659030 1 #&gt; 1391 0.04472727 1 #&gt; 1378 0.04098725 1 #&gt; 1256 0.03098191 1 #&gt; 1257 0.03085088 1 #&gt; [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 5 rows ] #&gt; #&gt; Credible sets summary: #&gt; #&gt; cs cs_log10bf cs_avg_r2 cs_min_r2 #&gt; 1 7.455457 0.9186663 0.8268138 #&gt; variable #&gt; 1256,1257,1267,1270,1272,1280,1288,1314,1316,1331,1334,1339,1347,1364,1367,1378,1391,1401,1417,1420,1456 locus1_susie &lt;- arrange(summary(fitted_rss_1)$vars, variable) locus1 &lt;- cbind(locus1, locus1_susie[-1]) # Plot ggplot(locus1, aes(x = pos, y = -log10(p_value))) + theme_bw(14) + geom_point(alpha = 0.8, size = 1.3) + geom_point(color = &quot;red&quot;, size = 2, data = filter(locus1, cs == 1)) + geom_label_repel(aes(label = rsid), size = 2, max.overlaps = 45, data = filter(locus1, cs == 1)) How do the PIPs compare to the p-values? Generate a plot to show this. 9.3.6 Locus 2 locus2 &lt;- filter(FT_cleaned, chr == 19, pos &gt; 45417638 - 5e5, pos &lt; 46384830 + 5e5) Now perform similar analyses on locus2. How do the PIPs compare to the p-values? Generate a plot to show this. What if we changed to a 75% credible set for locus 2? Write some code to do this and plot. Can we trust this result? Why might there be no result in fitted_rss_2 for the second credible set? 9.3.7 Reflections What can you do next with these results? What is the advantages of using SuSiE over conditional approaches (such as LD clumping or COJO)? What assumptions are we making when using SuSiE? References Ruth, K.S., Day, F.R., Tyrrell, J., Thompson, D.J., Wood, A.R., Mahajan, A., et al. (2020). Using human genetics to understand the disease impacts of testosterone in men and women. Nature Medicine, 26, 252–258. Wang, G., Sarkar, A., Carbonetto, P., &amp; Stephens, M. (2020). A simple new approach to variable selection in regression, with application to genetic fine mapping. Journal of the Royal Statistical Society Series B: Statistical Methodology, 82, 1273–1300. Zou, Y., Carbonetto, P., Wang, G., &amp; Stephens, M. (2022). Fine-mapping from summary data with the \"Sum of Single Effects\" model. PLoS Genetics, 18, e1010299. "],["some-other-analyses.html", "Chapter 10 Some other analyses", " Chapter 10 Some other analyses Read more in this PowerPoint presentation. Here we just mention some other types of analyses that haven’t really been covered in the course, but that are useful to know that they exist. We simply mention a few tools; you can find a list with more here. Estimation of SNP-based heritability: The proportion of phenotypic variance in a trait that can be explained by genetic variation captured by measured SNPs. A simple way to estimate it is with LD Score regression (LDSC) using GWAS summary statistics (B. K. Bulik-Sullivan et al., 2015). R packages bigsnpr and GenomicSEM also provide some implementation of LDSC. There exist many other tools to estimate SNP-heritability, such as LDpred2-auto (mentioned before). Genetic Correlation: LDSC can be also used to estimate the genome-wide genetic correlation between two traits using GWAS summary statistics (B. Bulik-Sullivan et al., 2015), and is also implemented in R package GenomicSEM. Local genetic correlations (e.g. within LD blocks) can also be estimated (Darlay et al., 2025; Zhang, Zhang, Zhang, &amp; Zhao, 2023). Partitioned heritability: Heritability partitioned by functional annotation (which may be cell-type or state-specific), which can be estimated using S-LDSC (Finucane et al., 2015), SumHer (Speed &amp; Balding, 2019) and SBayesRC (Zheng et al., 2024). Gene-based tests: These determine the genes associated with a trait from a GWAS by using proximity, such as mBAT-combo as an R package or a command-line software (Li et al., 2023). Summary statistic imputation: imputation of GWAS summary statistics help increase the SNP overlap between GWAS summary statistics and LD reference panels. Meta-analysis of GWAS: same trait, different cohorts: METAL (Willer, Li, &amp; Abecasis, 2010) multiple traits: MTAG (Turley et al., 2018) multiple ancestries: MR-MEGA (Mägi et al., 2017) Colocalisation: The same variant may be significant for two traits, but not necessarily be causal for both. Tools like COLOC + SuSiE directly assess this, yielding probabilities the signal is driven by the same variant, or two different variants (Wallace, 2021). SMR is a complementary approach designed for testing if a molQTL is also a causal variant for a trait, yielding an association statistic (Zhu et al., 2016). Gene expression signature of a trait: Use GWAS summary stats with eQTL/gene expression data. Early tools include FUSION (Gusev et al., 2016) and PrediXcan (Gamazon et al., 2015), although many more have been developed since. Structural Equation Modelling: GenomicSEM is a tool for structural equation modelling of GWAS summary statistics, allowing you to model the relationships between traits using their genetic architecture (Grotzinger et al., 2019). References Bulik-Sullivan, B., Finucane, H.K., Anttila, V., Gusev, A., Day, F.R., Loh, P.-R., et al. (2015). An atlas of genetic correlations across human diseases and traits. Nature Genetics, 47, 1236–1241. Bulik-Sullivan, B.K., Loh, P.-R., Finucane, H.K., Ripke, S., Yang, J., Psychiatric Genomics Consortium, S.W.G. of the, et al. (2015). LD score regression distinguishes confounding from polygenicity in genome-wide association studies. Nature Genetics, 47, 291–295. Darlay, R., Shah, R.L., Dodds, R.M., Nair, A.T., Pearson, E.R., Witham, M.D., et al. (2025). Exploring similarities and differences between methods that exploit patterns of local genetic correlation to identify shared causal loci through application to genome-wide association studies of multiple long term conditions. Genetic Epidemiology, 49, e70012. Finucane, H.K., Bulik-Sullivan, B., Gusev, A., Trynka, G., Reshef, Y., Loh, P.-R., et al. (2015). Partitioning heritability by functional annotation using genome-wide association summary statistics. Nature Genetics, 47, 1228–1235. Gamazon, E.R., Wheeler, H.E., Shah, K.P., Mozaffari, S.V., Aquino-Michaels, K., Carroll, R.J., et al. (2015). A gene-based association method for mapping traits using reference transcriptome data. Nature Genetics, 47, 1091–1098. Grotzinger, A.D., Rhemtulla, M., Vlaming, R. de, Ritchie, S.J., Mallard, T.T., Hill, W.D., et al. (2019). Genomic structural equation modelling provides insights into the multivariate genetic architecture of complex traits. Nature Human Behaviour, 3, 513–525. Gusev, A., Ko, A., Shi, H., Bhatia, G., Chung, W., Penninx, B.W., et al. (2016). Integrative approaches for large-scale transcriptome-wide association studies. Nature Genetics, 48, 245–252. Li, A., Liu, S., Bakshi, A., Jiang, L., Chen, W., Zheng, Z., et al. (2023). mBAT-combo: A more powerful test to detect gene-trait associations from GWAS data. The American Journal of Human Genetics, 110, 30–43. Mägi, R., Horikoshi, M., Sofer, T., Mahajan, A., Kitajima, H., Franceschini, N., et al. (2017). Trans-ethnic meta-regression of genome-wide association studies accounting for ancestry increases power for discovery and improves fine-mapping resolution. Human Molecular Genetics, 26, 3639–3650. Speed, D., &amp; Balding, D.J. (2019). SumHer better estimates the SNP heritability of complex traits from summary statistics. Nature Genetics, 51, 277–284. Turley, P., Walters, R.K., Maghzian, O., Okbay, A., Lee, J.J., Fontana, M.A., et al. (2018). Multi-trait analysis of genome-wide association summary statistics using MTAG. Nature Genetics, 50, 229–237. Wallace, C. (2021). A more accurate method for colocalisation analysis allowing for multiple causal variants. PLoS Genetics, 17, e1009440. Willer, C.J., Li, Y., &amp; Abecasis, G.R. (2010). METAL: Fast and efficient meta-analysis of genomewide association scans. Bioinformatics, 26, 2190–2191. Zhang, C., Zhang, Y., Zhang, Y., &amp; Zhao, H. (2023). Benchmarking of local genetic correlation estimation methods using summary statistics from genome-wide association studies. Briefings in Bioinformatics, 24, bbad407. Zheng, Z., Liu, S., Sidorenko, J., Wang, Y., Lin, T., Yengo, L., et al. (2024). Leveraging functional genomic annotations and genome coverage to improve polygenic prediction of complex traits within and between ancestries. Nature Genetics, 56, 767–777. Zhu, Z., Zhang, F., Hu, H., Bakshi, A., Robinson, M.R., Powell, J.E., et al. (2016). Integration of summary data from GWAS and eQTL studies predicts complex trait gene targets. Nature Genetics, 48, 481–487. "],["references.html", "References", " References Abdellaoui, A., Hottenga, J.-J., Knijff, P. de, Nivard, M.G., Xiao, X., Scheet, P., et al. (2013). Population structure, migration, and diversifying selection in the Netherlands. European Journal of Human Genetics, 21, 1277–1285. Aschard, H., Vilhjálmsson, B.J., Joshi, A.D., Price, A.L., &amp; Kraft, P. (2015). Adjusting for heritable covariates can bias effect estimates in genome-wide association studies. The American Journal of Human Genetics, 96, 329–339. Bulik-Sullivan, B., Finucane, H.K., Anttila, V., Gusev, A., Day, F.R., Loh, P.-R., et al. (2015). An atlas of genetic correlations across human diseases and traits. Nature Genetics, 47, 1236–1241. Bulik-Sullivan, B.K., Loh, P.-R., Finucane, H.K., Ripke, S., Yang, J., Psychiatric Genomics Consortium, S.W.G. of the, et al. (2015). LD score regression distinguishes confounding from polygenicity in genome-wide association studies. Nature Genetics, 47, 291–295. Bycroft, C., Freeman, C., Petkova, D., Band, G., Elliott, L.T., Sharp, K., et al. (2018). The UK Biobank resource with deep phenotyping and genomic data. Nature, 562, 203–209. Chang, C.C., Chow, C.C., Tellier, L.C., Vattikuti, S., Purcell, S.M., &amp; Lee, J.J. (2015). Second-generation PLINK: Rising to the challenge of larger and richer datasets. Gigascience, 4, s13742–015. Chen, W., Wu, Y., Zheng, Z., Qi, T., Visscher, P.M., Zhu, Z., &amp; Yang, J. (2021). Improved analyses of GWAS summary statistics by reducing data heterogeneity and errors. Nature Communications, 12, 1–10. Choi, S.W., &amp; O’Reilly, P.F. (2019). PRSice-2: Polygenic risk score software for biobank-scale data. Gigascience, 8, giz082. Daetwyler, H.D., Villanueva, B., &amp; Woolliams, J.A. (2008). Accuracy of predicting the genetic risk of disease using a genome-wide approach. PloS One, 3, e3395. Darlay, R., Shah, R.L., Dodds, R.M., Nair, A.T., Pearson, E.R., Witham, M.D., et al. (2025). Exploring similarities and differences between methods that exploit patterns of local genetic correlation to identify shared causal loci through application to genome-wide association studies of multiple long term conditions. Genetic Epidemiology, 49, e70012. Das, S., Forer, L., Schönherr, S., Sidore, C., Locke, A.E., Kwong, A., et al. (2016). Next-generation genotype imputation service and methods. Nature Genetics, 48, 1284–1287. Day, F.R., Loh, P.-R., Scott, R.A., Ong, K.K., &amp; Perry, J.R. (2016). A robust example of collider bias in a genetic association study. The American Journal of Human Genetics, 98, 392–393. Ding, Y., Hou, K., Xu, Z., Pimplaskar, A., Petter, E., Boulier, K., et al. (2023). Polygenic scoring accuracy varies across the genetic ancestry continuum. Nature, 618, 774–781. Finucane, H.K., Bulik-Sullivan, B., Gusev, A., Trynka, G., Reshef, Y., Loh, P.-R., et al. (2015). Partitioning heritability by functional annotation using genome-wide association summary statistics. Nature Genetics, 47, 1228–1235. Fuat, A., Adlen, E., Monane, M., Coll, R., Groves, S., Little, E., et al. (2024). A polygenic risk score added to a QRISK 2 cardiovascular disease risk calculator demonstrated robust clinical acceptance and clinical utility in the primary care setting. European Journal of Preventive Cardiology, 31, 716–722. Gamazon, E.R., Wheeler, H.E., Shah, K.P., Mozaffari, S.V., Aquino-Michaels, K., Carroll, R.J., et al. (2015). A gene-based association method for mapping traits using reference transcriptome data. Nature Genetics, 47, 1091–1098. Gazal, S., Loh, P.-R., Finucane, H.K., Ganna, A., Schoech, A., Sunyaev, S., &amp; Price, A.L. (2018). Functional architecture of low-frequency variants highlights strength of negative selection across coding and non-coding annotations. Nature Genetics, 50, 1600–1607. Ge, T., Chen, C.-Y., Ni, Y., Feng, Y.-C.A., &amp; Smoller, J.W. (2019). Polygenic prediction via Bayesian regression and continuous shrinkage priors. Nature Communications, 10, 1776. Grinde, K.E., Browning, B.L., Reiner, A.P., Thornton, T.A., &amp; Browning, S.R. (2024). Adjusting for principal components can induce collider bias in genome-wide association studies. PLoS Genetics, 20, e1011242. Grotzinger, A.D., Fuente, J. de la, Privé, F., Nivard, M.G., &amp; Tucker-Drob, E.M. (2023). Pervasive downward bias in estimates of liability-scale heritability in genome-wide association study meta-analysis: A simple solution. Biological Psychiatry, 93, 29–36. Grotzinger, A.D., Rhemtulla, M., Vlaming, R. de, Ritchie, S.J., Mallard, T.T., Hill, W.D., et al. (2019). Genomic structural equation modelling provides insights into the multivariate genetic architecture of complex traits. Nature Human Behaviour, 3, 513–525. Gusev, A., Ko, A., Shi, H., Bhatia, G., Chung, W., Penninx, B.W., et al. (2016). Integrative approaches for large-scale transcriptome-wide association studies. Nature Genetics, 48, 245–252. Henches, L., Kim, J., Yang, Z., Rubinacci, S., Pires, G., Albiñana, C., et al. (2025). Polygenic risk score prediction accuracy convergence. Human Genetics and Genomics Advances, 6. Retrieved from https://doi.org/10.1016/j.xhgg.2025.100457 Hou, K., Ding, Y., Xu, Z., Wu, Y., Bhattacharya, A., Mester, R., et al. (2023). Causal effects on complex traits are similar for common variants across segments of different continental ancestries within admixed individuals. Nature Genetics, 55, 549–558. Hu, S., Ferreira, L.A., Shi, S., Hellenthal, G., Marchini, J., Lawson, D.J., &amp; Myers, S.R. (2025). Fine-scale population structure and widespread conservation of genetic effect sizes between human groups across traits. Nature Genetics, 1–11. Julienne, H., Laville, V., McCaw, Z.R., He, Z., Guillemot, V., Lasry, C., et al. (2021). Multitrait GWAS to connect disease variants and biological mechanisms. PLoS Genetics, 17, e1009713. Kanai, M., Elzur, R., Zhou, W., Wu, K.-H.H., Rasheed, H., Tsuo, K., et al. (2022). Meta-analysis fine-mapping is often miscalibrated at single-variant resolution. Cell Genomics, 2, 100210. Kelemen, M., Xu, Y., Jiang, T., Zhao, J.H., Anderson, C.A., Wallace, C., et al. (2025). Performance of deep-learning-based approaches to improve polygenic scores. Nature Communications, 16, 1–9. Kulm, S., Marderstein, A., Mezey, J., &amp; Elemento, O. (2020). A systematic framework for assessing the clinical impact of polygenic risk scores. medRxiv, 2020–2004. Levey, D.F., Stein, M.B., Wendt, F.R., Pathak, G.A., Zhou, H., Aslan, M., et al. (2021). Bi-ancestral depression GWAS in the Million Veteran Program and meta-analysis in&gt; 1.2 million individuals highlight new therapeutic directions. Nature Neuroscience, 24, 954–963. Li, A., Liu, S., Bakshi, A., Jiang, L., Chen, W., Zheng, Z., et al. (2023). mBAT-combo: A more powerful test to detect gene-trait associations from GWAS data. The American Journal of Human Genetics, 110, 30–43. Lloyd-Jones, L.R., Zeng, J., Sidorenko, J., Yengo, L., Moser, G., Kemper, K.E., et al. (2019). Improved polygenic prediction by Bayesian multiple regression on summary statistics. Nature Communications, 10, 5086. Loh, P.-R., Kichaev, G., Gazal, S., Schoech, A.P., &amp; Price, A.L. (2018). Mixed-model association for biobank-scale datasets. Nature Genetics, 50, 906–908. Mägi, R., Horikoshi, M., Sofer, T., Mahajan, A., Kitajima, H., Franceschini, N., et al. (2017). Trans-ethnic meta-regression of genome-wide association studies accounting for ancestry increases power for discovery and improves fine-mapping resolution. Human Molecular Genetics, 26, 3639–3650. Mak, T.S.H., Porsch, R.M., Choi, S.W., Zhou, X., &amp; Sham, P.C. (2017). Polygenic scores via penalized regression on summary statistics. Genetic Epidemiology, 41, 469–480. Manichaikul, A., Mychaleckyj, J.C., Rich, S.S., Daly, K., Sale, M., &amp; Chen, W.-M. (2010). Robust relationship inference in genome-wide association studies. Bioinformatics, 26, 2867–2873. Marchini, J., &amp; Howie, B. (2010). Genotype imputation for genome-wide association studies. Nature Reviews Genetics, 11, 499–511. Mbatchou, J., Barnard, L., Backman, J., Marcketta, A., Kosmicki, J.A., Ziyatdinov, A., et al. (2021). Computationally efficient whole-genome regression for quantitative and binary traits. Nature Genetics, 53, 1097–1103. Murphy, A.E., Schilder, B.M., &amp; Skene, N.G. (2021). MungeSumstats: A Bioconductor package for the standardization and quality control of many GWAS summary statistics. Bioinformatics, 37, 4593–4596. Okbay, A., Wu, Y., Wang, N., Jayashankar, H., Bennett, M., Nehzati, S.M., et al. (2022). Polygenic prediction of educational attainment within and between families from genome-wide association analyses in 3 million individuals. Nature Genetics, 54, 437–449. Pain, O., Glanville, K.P., Hagenaars, S.P., Selzam, S., Fürtjes, A.E., Gaspar, H.A., et al. (2021). Evaluation of polygenic prediction methodology within a reference-standardized framework. PLoS Genetics, 17, e1009021. Price, A.L., Patterson, N.J., Plenge, R.M., Weinblatt, M.E., Shadick, N.A., &amp; Reich, D. (2006). Principal components analysis corrects for stratification in genome-wide association studies. Nature Genetics, 38, 904–909. Price, A.L., Weale, M.E., Patterson, N., Myers, S.R., Need, A.C., Shianna, K.V., et al. (2008). Long-range LD can confound genome scans in admixed populations. The American Journal of Human Genetics, 83, 132–135. Pritchard, J.K., &amp; Przeworski, M. (2001). Linkage disequilibrium in humans: Models and data. The American Journal of Human Genetics, 69, 1–14. Privé, F. (2021). Optimal linkage disequilibrium splitting. Bioinformatics, 38, 255–256. Privé, F. (2022). Using the UK Biobank as a global reference of worldwide populations: application to measuring ancestry diversity from GWAS summary statistics. Bioinformatics, 38, 3477–3480. Privé, F., Albiñana, C., Arbel, J., Pasaniuc, B., &amp; Vilhjálmsson, B.J. (2023). Inferring disease architecture and predictive ability with LDpred2-auto. The American Journal of Human Genetics, 110, 2042–2055. Privé, F., Arbel, J., Aschard, H., &amp; Vilhjálmsson, B.J. (2022). Identifying and correcting for misspecifications in GWAS summary statistics and polygenic scores. Human Genetics and Genomics Advances, 3, 100136. Privé, F., Arbel, J., &amp; Vilhjálmsson, B.J. (2020). LDpred2: better, faster, stronger. Bioinformatics, 36, 5424–5431. Privé, F., Aschard, H., &amp; Blum, M.G. (2019). Efficient implementation of penalized regression for genetic risk prediction. Genetics, 212, 65–74. Privé, F., Aschard, H., Carmi, S., Folkersen, L., Hoggart, C., O’Reilly, P.F., &amp; Vilhjálmsson, B.J. (2022). Portability of 245 polygenic scores when derived from the UK Biobank and applied to 9 ancestry groups from the same cohort. The American Journal of Human Genetics, 109, 12–23. Privé, F., Aschard, H., Ziyatdinov, A., &amp; Blum, M.G.B. (2018). Efficient analysis of large-scale genome-wide data with two R packages: bigstatsr and bigsnpr. Bioinformatics, 34, 2781–2787. Privé, F., Luu, K., Blum, M.G., McGrath, J.J., &amp; Vilhjálmsson, B.J. (2020). Efficient toolkit implementing best practices for principal component analysis of population genetic data. Bioinformatics, 36, 4449–4457. Privé, F., Luu, K., Vilhjálmsson, B.J., &amp; Blum, M.G. (2020). Performing highly efficient genome scans for local adaptation with R package pcadapt version 4. Molecular Biology and Evolution, 37, 2153–2154. Privé, F., Vilhjálmsson, B.J., Aschard, H., &amp; Blum, M.G.B. (2019). Making the most of clumping and thresholding for polygenic scores. The American Journal of Human Genetics, 105, 1213–1221. Reed, E., Nunez, S., Kulp, D., Qian, J., Reilly, M.P., &amp; Foulkes, A.S. (2015). A guide to genome-wide association analysis and post-analytic interrogation. Statistics in Medicine, 34, 3769–3792. Ruth, K.S., Day, F.R., Tyrrell, J., Thompson, D.J., Wood, A.R., Mahajan, A., et al. (2020). Using human genetics to understand the disease impacts of testosterone in men and women. Nature Medicine, 26, 252–258. Speed, D., &amp; Balding, D.J. (2019). SumHer better estimates the SNP heritability of complex traits from summary statistics. Nature Genetics, 51, 277–284. Suzuki, K., Hatzikotoulas, K., Southam, L., Taylor, H.J., Yin, X., Lorenz, K.M., et al. (2023). Multi-ancestry genome-wide study in &gt; 2.5 million individuals reveals heterogeneity in mechanistic pathways of type 2 diabetes and complications. medRxiv. Retrieved from https://doi.org/10.1101/2023.03.31.23287839 Turley, P., Walters, R.K., Maghzian, O., Okbay, A., Lee, J.J., Fontana, M.A., et al. (2018). Multi-trait analysis of genome-wide association summary statistics using MTAG. Nature Genetics, 50, 229–237. Wallace, C. (2021). A more accurate method for colocalisation analysis allowing for multiple causal variants. PLoS Genetics, 17, e1009440. Wang, G., Sarkar, A., Carbonetto, P., &amp; Stephens, M. (2020). A simple new approach to variable selection in regression, with application to genetic fine mapping. Journal of the Royal Statistical Society Series B: Statistical Methodology, 82, 1273–1300. Wang, X., Walker, A., Revez, J.A., Ni, G., Adams, M.J., McIntosh, A.M., et al. (2023). Polygenic risk prediction: Why and when out-of-sample prediction R2 can exceed SNP-based heritability. The American Journal of Human Genetics, 110, 1207–1215. Wang, Y., Guo, J., Ni, G., Yang, J., Visscher, P.M., &amp; Yengo, L. (2020). Theoretical and empirical quantification of the accuracy of polygenic scores in ancestry divergent populations. Nature Communications, 11, 3865. Willer, C.J., Li, Y., &amp; Abecasis, G.R. (2010). METAL: Fast and efficient meta-analysis of genomewide association scans. Bioinformatics, 26, 2190–2191. Yang, J., Ferreira, T., Morris, A.P., Medland, S.E., ANthropometric Traits (GIANT) Consortium, G.I. of, Consortium, D.G.R.A.M. (DIAGRAM), et al. (2012). Conditional and joint multiple-SNP analysis of GWAS summary statistics identifies additional variants influencing complex traits. Nature Genetics, 44, 369–375. Yang, J., Weedon, M.N., Purcell, S., Lettre, G., Estrada, K., Willer, C.J., et al. (2011). Genomic inflation factors under polygenic inheritance. European Journal of Human Genetics, 19, 807–812. Yengo, L., Vedantam, S., Marouli, E., Sidorenko, J., Bartell, E., Sakaue, S., et al. (2022). A saturated map of common genetic variants associated with human height. Nature, 610, 704–712. Zabad, S., Haryan, C.A., Gravel, S., Misra, S., &amp; Li, Y. (2025). Towards whole-genome inference of polygenic scores with fast and memory-efficient algorithms. The American Journal of Human Genetics. Retrieved from https://doi.org/10.1016/j.ajhg.2025.05.002 Zhang, C., Zhang, Y., Zhang, Y., &amp; Zhao, H. (2023). Benchmarking of local genetic correlation estimation methods using summary statistics from genome-wide association studies. Briefings in Bioinformatics, 24, bbad407. Zheng, Z., Liu, S., Sidorenko, J., Wang, Y., Lin, T., Yengo, L., et al. (2024). Leveraging functional genomic annotations and genome coverage to improve polygenic prediction of complex traits within and between ancestries. Nature Genetics, 56, 767–777. Zhou, W., Kanai, M., Wu, K.-H.H., Rasheed, H., Tsuo, K., Hirbo, J.B., et al. (2022). Global Biobank Meta-analysis Initiative: Powering genetic discovery across human disease. Cell Genomics, 2, 100192. Zhu, Z., Zhang, F., Hu, H., Bakshi, A., Robinson, M.R., Powell, J.E., et al. (2016). Integration of summary data from GWAS and eQTL studies predicts complex trait gene targets. Nature Genetics, 48, 481–487. Zou, Y., Carbonetto, P., Wang, G., &amp; Stephens, M. (2022). Fine-mapping from summary data with the \"Sum of Single Effects\" model. PLoS Genetics, 18, e1010299. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
