---
output: html_document
editor_options: 
  chunk_output_type: console
---

# GWAS summary statistics

```{r setup, include=FALSE}
WORDS_TO_IGNORE <- c("rsid", "se", "eq", "sd", "sdlin", "sdlog", "MungeSumstats", "HDL", "hg", "liftOver", "rsID")
source("knitr-options.R")
source("spelling-check.R")
```

When genome-wide association studies (GWAS) were first introduced to explore genetic factors associated with diseases, people may not have realized how valuable the resulting GWAS summary statistics would be. 
Over the past decade, GWAS summary statistics have become essential for many genetic analyses. 

The summary statistics generated by GWAS do not include personal information, making GWAS summary statistics easy to share and use for further analysis. 
Collaboration among researchers in large consortia has facilitated meta-analyses of GWAS summary statistics from numerous study sites, resulting in unprecedentedly large GWAS sample sizes [@levey2021bi; @zhou2021global; @okbay2022polygenic; @yengo2022saturated; @suzuki2023multi].

## Format

This an example of GWAS summary statistics (a subset of it at least). 

:::: {.infobox .caution}
Fields included in GWAS summary statistics and their specific names in the header of the file can change very much from one file to the other; this is why tools like MungeSumstats have been developed [@murphy2021mungesumstats].
::::

```{r}
tgz <- runonce::download_file(
  "https://figshare.com/ndownloader/files/55262768", 
  dir = "tmp-data", fname = "FT_sumstats_small.tsv.gz")

writeLines(readLines(tgz, n = 6))
```

They often include

- Information to match variants (chromosome, physical position, alleles, rsid).

- $\hat{\beta}_j$ or $\hat{\gamma}_j$ &mdash; the GWAS effect size of variant $j$ (marginal effect), 

- $\text{se}(\hat{\gamma}_j)$ &mdash; its standard error,

- $z_j = \frac{\hat{\gamma}_j}{\text{se}(\hat{\gamma}_j)}$ &mdash; the Z-score of variant $j$,

- p-values derived from Z-scores

but they more rarely include

- $n_j$ &mdash; the GWAS sample size associated with variant $j$, 

- $f_j$ &mdash; the allele frequency of variant $j$,

- $\text{INFO}_j$ &mdash; the imputation INFO score (imputation quality) of variant $j$

:::: {.infobox .exo}
Read-in these GWAS summary statistics and derive the p-values again, using the other columns. Recall that the Z-score squared is $\chi^2$-distributed with one degree of freedom.
::::

<details>
<summary>Click to see solution</summary>
```{r}
(sumstats <- bigreadr::fread2(tgz))
chi2 <- with(sumstats, (beta / standard_error)^2)
pval <- pchisq(chi2, df = 1, lower.tail = FALSE)
plot(pval, sumstats$p_value, log = "xy"); abline(0, 1, col = "red")
```
</details>


## Quality control

However, **pooling many GWAS summary statistics in large meta-analyses has been a double-edged sword**. On the one hand, this has given much-needed power to many genetic analyses. On the other hand, the quality and standardization of GWAS summary statistics varies widely, and the more studies that are pooled together, the more issues arise when using these pooled summary data.

An overview of known issues in using GWAS summary statistics is presented in Figure \@ref(fig:overview-misspec). This includes weakening the construction of polygenic scores [@prive2021identifying], biasing heritability estimates [@gazal2018functional; @grotzinger2023pervasive], identifying spurious causal genes [@zou2022fine; @kanai2022meta], as well as undermining other genetic analyses [@chen2021improved; @julienne2021multitrait].

```{r overview-misspec, echo=FALSE, out.width="100%", fig.cap="Overview of possible errors and misspecifications in GWAS summary statistics, with possible harmful consequences, as well as possible remedies [@prive2021identifying]. $\\hat{\\gamma}$: GWAS effect sizes; SE: standard errors; $n^{eff}$: effective GWAS sample sizes; SD: standard deviations of genotypes; INFO: measure of imputation quality."}
knitr::include_graphics("https://github.com/privefl/thesis-docs/blob/master/figures/overview-misspec.jpg?raw=true")
```

---

The QC I recommend to perform consist in **comparing standard deviations** of genotypes estimated in 2 ways [@prive2020ldpred2; @prive2021identifying]:

<br>

1.  - When linear regression was used 
    \begin{equation}
    \text{sd}(G_j) \approx \dfrac{\text{sd}(y)}{\sqrt{n_j \cdot \text{se}(\hat{\gamma}_j)^2 + \hat{\gamma}_j^2}} (\#eq:sdlin)
    \end{equation}

    - When logistic regression was used (case-control phenotype)
    \begin{equation}\label{eq:approx-sd-log}
    \text{sd}(G_j) \approx \dfrac{2}{\sqrt{n_j^\text{eff} \cdot \text{se}(\hat{\gamma}_j)^2 + \hat{\gamma}_j^2}} (\#eq:sdlog)
    \end{equation}
<br>
2. \begin{equation}
\text{sd}(G_j) \approx \sqrt{2 \cdot f_j \cdot (1 - f_j) \cdot \text{INFO}_j} (\#eq:sdaf)
\end{equation}

---

With this QC, you can detect differences in per-variant GWAS sample sizes

```{r, echo=FALSE, out.width="72%"}
knitr::include_graphics("https://github.com/privefl/thesis-docs/blob/master/figures/simu-qc-plot.jpg?raw=true")
```

When $n_j$ are missing, you can use this to perform some QC (e.g. at 70% of max n) or to impute $n_j$. 

---

You can detect bias in total effective GWAS sample size

```{r, echo=FALSE, out.width="72%"}
knitr::include_graphics("https://github.com/privefl/thesis-docs/blob/master/figures/cad_quick_qc.png?raw=true")
```

$N_\text{eff} = \frac{4}{1 / N_\text{ca} + 1 / N_\text{co}}$

```{r, echo=FALSE, out.width="30%"}
knitr::include_graphics("https://github.com/privefl/thesis-docs/blob/master/figures/cad_neff_perstudy.png?raw=true")
```

For example, overestimating the sample size leads to underestimating the SNP heritability in many methods such as LD score regression [@grotzinger2023pervasive].

---

You can detect and QC low imputation INFO scores, & other issues

```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("https://github.com/privefl/thesis-docs/blob/master/figures/brca_onco_qc.png?raw=true")
```

---

Beware that multi-ancestry INFO scores are overestimated (e.g. in the UK Biobank)

```{r, echo=FALSE, out.width="65%"}
knitr::include_graphics("https://github.com/privefl/thesis-docs/blob/master/figures/overestim-info.png?raw=true")
```

---

:::: {.infobox .exo}
Use the following GWAS results for HDL and allele frequencies to estimate standard deviations and compare both estimates. How can you estimate `sd(y)`?
::::

```{r, message=FALSE}
library(bigsnpr)
obj.bigsnp <- snp_attach("tmp-data/GWAS_data_sorted_QC.rds")
NCORES <- nb_cores()
obj.svd <- readRDS("tmp-data/PCA_GWAS_data.rds")
PC <- predict(obj.svd)
covar <- cbind(as.matrix(obj.bigsnp$fam[c("sex", "age")]), PC[, 1:6])
G <- obj.bigsnp$genotypes
y <- obj.bigsnp$fam$hdl
ind.gwas <- which(!is.na(y) & complete.cases(covar))
gwas <- big_univLinReg(G, y[ind.gwas], ind.train = ind.gwas,
                       covar.train = covar[ind.gwas, ], ncores = NCORES)
N <- length(ind.gwas)
af <- big_colstats(G, ind.row = ind.gwas, ncores = NCORES)$sum / (2 * N)
```

<details>
<summary>Click to see solution</summary>

```{r}
sd_y <- with(gwas, quantile(sqrt(0.5 * (N * std.err^2 + estim^2)), 0.01))
c(sd(y[ind.gwas]), sd_y)
sd_ss <- sd_y / with(gwas, sqrt(N * std.err^2 + estim^2))
sd_af <- sqrt(2 * af * (1 - af))

library(ggplot2)
ggplot(dplyr::slice_sample(data.frame(sd_af, sd_ss), n = 100e3)) +
  geom_point(aes(sd_af, sd_ss), alpha = 0.5) +
  theme_bigstatsr(0.9) +
  geom_abline(linetype = 2, color = "red") +
  labs(x = "Standard deviations derived from the allele frequencies",
       y = "Standard deviations derived from the summary statistics")
```

```{r}
nb_na <- big_counts(G$copy(CODE_012), ind.row = ind.gwas)[4, ]
ggplot(dplyr::slice_sample(data.frame(sd_af, sd_ss, nb_na), n = 100e3)) +
  geom_point(aes(sd_af, sd_ss, color = nb_na), alpha = 0.5) +
  theme_bigstatsr(0.9) +
  scale_color_viridis_c(direction = -1, trans = "sqrt") +
  geom_abline(linetype = 2, color = "red") +
  labs(x = "Standard deviations derived from the allele frequencies",
       y = "Standard deviations derived from the summary statistics")
```

</details>

---

We will practice more QC on GWAS summary statistics in \@ref(exo-ldpred2).

I provide some example R script in [the LDpred2 tutorial](https://privefl.github.io/bigsnpr/articles/LDpred2.html) that implements this QC. You can also find some other scripts with examples how to prepare several GWAS summary statistics [here](https://github.com/privefl/paper-misspec/tree/main/code).

I am currently working on implementing a method that performs a complementary QC, as well as the imputation of GWAS summary statistics (i.e. GWAS results for more variants).


## Ancestry inference {#ssancestry}

In \@ref(ancestry), we've seen how to infer ancestry for individual-level data using reference data provided in @prive2021using; we can also use this to infer the ancestry composition of a GWAS dataset using only allele frequencies from the GWAS summary statistics. The tutorial accompanying @prive2021using is [here](https://privefl.github.io/bigsnpr/articles/ancestry.html).

:::: {.infobox .exo}
Reuse some of the code from \@ref(ancestry) and allele frequencies (that you should compute) to get the ancestry composition of this dataset using function `snp_ancestry_summary()`.
::::

<details>
<summary>Click to see solution</summary>

```{r}
library(bigsnpr)
obj.bed <- bed("tmp-data/GWAS_data_sorted_QC.bed")
NCORES <- nb_cores()
```


```{r}
all_freq <- bigreadr::fread2(
  runonce::download_file(
    "https://figshare.com/ndownloader/files/38019027",  # subset for the tutorial (46 MB)
    # "https://figshare.com/ndownloader/files/31620968",  # for real analyses (849 MB)
    dir = "tmp-data", fname = "ref_freqs.csv.gz"))

projection <- bigreadr::fread2(
  runonce::download_file(
    "https://figshare.com/ndownloader/files/38019024",  # subset for the tutorial (44 MB)
    # "https://figshare.com/ndownloader/files/31620953",  # for real analyses (847 MB)
    dir = "tmp-data", fname = "projection.csv.gz"))

# coefficients to correct for overfitting of PCA
correction <- c(1, 1, 1, 1.008, 1.021, 1.034, 1.052, 1.074, 1.099,
                1.123, 1.15, 1.195, 1.256, 1.321, 1.382, 1.443)
```

```{r, warning=FALSE, message=FALSE}
# match variants between the two datasets
library(dplyr)
matched <- obj.bed$map %>%
  transmute(chr = chromosome, pos = physical.pos, a1 = allele1, a0 = allele2) %>%
  mutate(beta = 1) %>%
  snp_match(all_freq[1:5]) %>%
  print()
```

:::: {.infobox .caution}
When matching between variants from two datasets, sometimes physical positions are in two different genome builds, usually it can be hg19 and hg38 (for newer datasets). You will need to either convert one of the two sets of positions with `snp_modifyBuild()` (that uses liftOver, not available for Windows) or by matching using rsIDs instead of positions (by using `join_by_pos = FALSE`).
::::

```{r, warning=FALSE, message=FALSE}
# further subsetting on missing values
counts <- bed_counts(obj.bed, ind.col = matched$`_NUM_ID_.ss`, ncores = NCORES)
ind <- which((counts[4, ] / nrow(obj.bed)) < 0.05)
matched2 <- matched[ind, ]
```

```{r, warning=FALSE, message=FALSE}
# compute allele frequencies
af <- bed_MAF(obj.bed, ind.col = matched2$`_NUM_ID_.ss`)$af
af2 <- ifelse(matched2$beta > 0, af, 1 - af)

# get ancestry composition
res <- snp_ancestry_summary(
  freq = af2,
  info_freq_ref = all_freq[matched2$`_NUM_ID_`, -(1:5)],
  projection = projection[matched2$`_NUM_ID_`, -(1:5)],
  correction = correction
)
res
```

```{r}
group <- colnames(all_freq)[-(1:5)]
group[group %in% c("Scandinavia", "United Kingdom", "Ireland")]   <- "Europe (North West)"
group[group %in% c("Europe (South East)", "Europe (North East)")] <- "Europe (East)"
grp_fct <- factor(group, unique(group))

final_res <- tapply(res, grp_fct, sum)
round(100 * final_res, 1)
```

</details>
